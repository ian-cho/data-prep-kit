{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Data Prep Kit <p>Data Prep Kit is a community-driven project that simplifies unstructured data preparation for LLM application development. It addresses the growing challenge of preparing diverse data (language, code, vision, multimodal) for fine-tuning, instruction-tuning, and RAG applications. The modules in the kit have been tested in producing pre-training datasets for the Granite open source LLM models.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>The kit provides a growing set of modules/transforms targeting laptop-scale to datacenter-scale processing.</li> <li>The data modalities supported today are: Natural Language and Code.</li> <li>The modules are built on common frameworks for Python, Ray and Spark runtimes for scaling up data processing.</li> <li>The kit provides a framework for developing custom transforms for processing parquet files. </li> <li>The kit uses Kubeflow Pipelines-based workflow automation.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>The latest version of the Data Prep Kit is available on PyPi for Python 3.10, 3.11 or 3.12. It can be installed using: </p> <pre><code>pip install  'data-prep-toolkit-transforms[all]'\n</code></pre> <p>This will install all available transforms. </p> <p>For guidance on creating the virtual environment for installing the data prep kit, click here.</p>"},{"location":"#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"#fastest-way-to-experience-data-prep-kit","title":"Fastest way to experience Data Prep Kit","text":"<p>With no setup necessary, let's use a Google Colab friendly notebook to try Data Prep Kit. This is a simple transform to extract content from PDF files: examples/notebooks/Run_your_first_transform_colab.ipynb  | . (Here are some tips for running Data Prep Kit transforms on Google Colab. For this simple example, these tips are either already taken care of, or are not needed.)  The same notebook can be downloaded and run on the local machine, without cloning the repo or any other setup. </p>"},{"location":"#examples","title":"Examples","text":"<p>Now that you have run a single transform, the next step is to explore how to put these transforms  together to run a data prep pipeline for end to end real enterprise use cases like fine-tuning a model or building a RAG application. </p> <p>We have a complete set of data processing recipes for such use cases. </p> <p>We also have a developer tutorial for contributing a new transform to the kit. </p> <p>For advanced users, here is more information for adding your own transform,  running transforms from the command line, scaling and automation and more.  Also, repository structure and use are discussed here.</p>"},{"location":"#windows-users","title":"Windows users","text":"<p>Please click here for guidance on how to run transforms in Windows.</p>"},{"location":"#using-huggingface-data-files","title":"Using HuggingFace data files","text":"<p>All the transforms in the kit include small sample data files for testing, but advanced users who want to download real data files from HuggingFace and use them in testing, can refer to this. </p>"},{"location":"#current-list-of-transforms","title":"Current list of transforms","text":"<p>The matrix below shows the the combination of modules and supported runtimes. All the modules can be accessed here and can be combined to form data processing pipelines, as shown in the examples folder. </p> Modules Python-only Ray Spark KFP on Ray Data Ingestion Code (from zip) to Parquet PDF to Parquet HTML to Parquet Web to Parquet Universal (Code &amp; Language) Exact dedup filter Fuzzy dedup filter Unique ID annotation Filter on annotations Profiler Resize Hate, Abuse, Profanity (HAP) Tokenizer Language-only Language identification Document quality Document chunking for RAG Text encoder PII Annotator/Redactor Similarity Code-only Programming language annotation Code quality annotation Malware annotation Header cleanser Semantic file ordering License Select Annotation Code profiler"},{"location":"#contributing","title":"Contributing","text":"<p>Contributors are welcome to add new modules to expand to other data modalities as well as add runtime support for existing modules! Please read this for details.</p>"},{"location":"#get-help-and-support","title":"Get help and support","text":"<p>Please feel free to connect with us using the discussion section.</p>"},{"location":"#resources","title":"Resources","text":"<p>Papers, talks, presentations and tutorials.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use Data Prep Kit in your research, please cite our paper:</p> <pre><code>@misc{wood2024dataprepkitgettingdataready,\n      title={Data-Prep-Kit: getting your data ready for LLM application development}, \n      author={David Wood and Boris Lublinsky and Alexy Roytman and Shivdeep Singh \n      and Constantin Adam and Abdulhamid Adebayo and Sungeun An and Yuan Chi Chang \n      and Xuan-Hong Dang and Nirmit Desai and Michele Dolfi and Hajar Emami-Gohari \n      and Revital Eres and Takuya Goto and Dhiraj Joshi and Yan Koyfman \n      and Mohammad Nassar and Hima Patel and Paramesvaran Selvam and Yousaf Shah  \n      and Saptha Surendran and Daiki Tsuzuku and Petros Zerfos and Shahrokh Daijavad},\n      year={2024},\n      eprint={2409.18164},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2409.18164}, \n}\n</code></pre>"},{"location":"ADVANCED/","title":"Advanced","text":"Data Prep Kit for Advanced Users <p>Below we discuss the following:  * Adding your own transform to the repository * Running transforms using the CLI * Scaling transform execution * Using HuggingFace data</p> <p></p>"},{"location":"ADVANCED/#add-your-own-transform","title":"Add your own transform","text":"<p>At the core of the framework, is a data processing library, that provides a systematic way to implement the data processing modules. The library is python-based and enables the application of \"transforms\" to a one or more input data files to produce one or more output data files. We use the popular parquet format to store the data (code or language).  Every parquet file follows a set schema. A user can use one or more transforms (or modules)  as discussed in the main readme.</p> <p>A transform can follow one of the two patterns: annotator or filter.</p> <ul> <li> <p>Annotator An annotator transform adds information during the processing by adding one more columns to the parquet files. The annotator design also allows a user to verify the results of the processing before the actual filtering of the data.</p> </li> <li> <p>Filter A filter transform processes the data and outputs the transformed data, e.g., exact deduplication. A general purpose SQL-based filter transform enables a powerful mechanism for identifying columns and rows of interest for downstream processing.</p> </li> </ul> <p>For a new module to be added, a user can pick the right design based on the  processing to be applied. More details here.</p> <p>One can leverage Python-based processing logic and the Data Processing Library  to easily build and contribute new transforms. We have provided an example transform that  can serve as a template to add new simple transforms.  Follow the step-by-step tutorial to help you add your own new transform. </p> <p>For a deeper understanding of the library's architecture, its transforms, and available runtimes, we encourage the reader to consult the comprehensive overview document alongside dedicated sections on transforms and runtimes.</p> <p>Additionally, check out our video tutorial for a visual, example-driven guide on adding custom modules.</p> <p></p>"},{"location":"ADVANCED/#running-transforms-at-the-command-line","title":"Running Transforms at the Command Line","text":"<p>You can run transforms via the command line or from within a docker image. * This document shows how to   run a transform using the command line interface and a virtual environment. * You can follow this document to run transforms inside the docker image. </p> <p></p>"},{"location":"ADVANCED/#scaling-from-laptop-to-cluster-","title":"Scaling from laptop to cluster \ud83d\udcbb -&gt; \ud83d\udda5\ufe0f\u2601\ufe0f","text":"<p>Data-prep-kit provides the flexibility to transition your projects from  proof-of-concept (PoC) stage to full-scale production mode,  offering all the necessary tools to run your data transformations at high volume.  In this section, we enable you how to run your transforms at scale and how to automate them. </p>"},{"location":"ADVANCED/#scaling-of-transforms","title":"Scaling of Transforms","text":"<p>To enable processing of large data volumes leveraging multi-mode clusters, Ray  or Spark wrappers are provided, to readily scale out the Python implementations.</p> <p>A generalized workflow is shown here.</p>"},{"location":"ADVANCED/#kfp-automation","title":"KFP Automation","text":"<p>The toolkit also supports transform execution automation based on  Kubeflow pipelines (KFP), tested on a locally deployed Kind cluster and external OpenShift clusters. There is an  automation to create a Kind cluster and deploy all required components on it. The KFP implementation is based on the KubeRay Operator for creating and managing the Ray cluster and KubeRay API server to interact with the KubeRay operator. An additional framework along with several kfp components is used to simplify the pipeline implementation.</p> <p>A simple transform pipeline tutorial explains the pipeline creation and execution.  In addition, if you want to combine several transformers in a single pipeline, you can look at multi-steps pipeline </p> <p>When you finish working with the cluster, and want to clean up or destroy it. See the  clean up the cluster</p> <p></p>"},{"location":"ADVANCED/#using-huggingface-data","title":"Using HuggingFace Data","text":"<p>If you wish to download and use parquet data files from HuggingFace  while testing any of the toolkit transforms, use HuggingFace  download APIs  that provide caching and optimize the download process. Here is an example of the code needed to download a sample file, first install huggingface_hub</p> <p><code>bash  pip install --upgrade huggingface_hub</code> Then use the following to download a specific file,  <pre><code>from huggingface_hub import hf_hub_download\nimport pandas as pd\n\nREPO_ID = \"HuggingFaceFW/fineweb\"\nFILENAME = \"data/CC-MAIN-2013-20/000_00000.parquet\"\n\nhf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")\n</code></pre></p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or  advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic  address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a  professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p> <p>For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq</p>"},{"location":"CONTRIBUTING/","title":"How to Contribute","text":""},{"location":"CONTRIBUTING/#contributing-in-general","title":"Contributing In General","text":"<p>Our project welcomes external contributions. If you have an itch, please feel free to scratch it.</p> <p>To contribute code or documentation, please submit a pull request. You can get started with open issues with the label - good first issue.  Before embarking on a more ambitious contribution, please quickly get in touch with us via raising an issue.</p> <p>Note: We appreciate your effort, and want to avoid a situation where a contribution requires extensive rework (by you or by us), sits in backlog for a long time, or cannot be accepted at all!</p>"},{"location":"CONTRIBUTING/#proposing-new-features","title":"Proposing new features","text":"<p>If you would like to implement a new feature, please raise an issue,  before sending a pull request so the feature can be discussed. This is to avoid you wasting your valuable time working on a feature that the project developers are not interested in accepting into the code base.</p>"},{"location":"CONTRIBUTING/#fixing-bugs","title":"Fixing bugs","text":"<p>If you would like to fix a bug, please raise an issue, before sending a pull request so it can be tracked.</p>"},{"location":"CONTRIBUTING/#merge-approval","title":"Merge approval","text":"<p>The project maintainers use LGTM (Looks Good To Me) in comments on the code review to indicate acceptance. A change requires LGTMs from two of the maintainers of each component affected.</p> <p>For a list of the maintainers, see the MAINTAINERS.md page.</p>"},{"location":"CONTRIBUTING/#legal","title":"Legal","text":"<p>Each source file must include a license header for the Apache Software License 2.0. Using the SPDX format is the simplest approach. e.g.</p> <pre><code>/*\nCopyright &lt;holder&gt; All Rights Reserved.\n\nSPDX-License-Identifier: Apache-2.0\n*/\n</code></pre> <p>We have tried to make it as easy as possible to make contributions. This applies to how we handle the legal aspects of contribution. We use the same approach - the Developer's Certificate of Origin 1.1 (DCO) - that the Linux\u00ae Kernel community uses to manage code contributions.</p> <p>We simply ask that when submitting a patch for review, the developer must include a sign-off statement in the commit message.</p> <p>Here is an example Signed-off-by line, which indicates that the submitter accepts the DCO:</p> <pre><code>Signed-off-by: John Doe &lt;john.doe@example.com&gt;\n</code></pre> <p>To include Signed-off-by message automatically, set the git config as, <pre><code>git config user.name \"FirstName LastName\"\ngit config user.email \"YourEmail@example.com\"\n</code></pre></p> <p>and include flag <code>-s | --sign-off</code> when you commit a change to your local git repository, for example</p> <pre><code>git commit -s -m \"your commit message\"\n</code></pre>"},{"location":"CONTRIBUTING/#transform-setup-and-testing","title":"Transform Setup and Testing","text":"<p>Please note the many useful options of the make command, as shown by using <code>make help</code>, that will take care of manual steps that would have been needed for tasks such as building, publishing, setting up or testing transforms in most directories.</p>"},{"location":"CONTRIBUTING/#coding-style-guidelines","title":"Coding style guidelines","text":"<p>Coding style as enforced by <code>pre-commit</code>.</p>"},{"location":"MAINTAINERS/","title":"MAINTAINERS","text":"<p>Maroun Touma - touma@us.ibm.com</p> <p>Shivdeep Singh - shivdeep.singh@ibm.com</p> <p>Revital Eres - eres@il.ibm.com</p>"},{"location":"RELEASE/","title":"Release Management","text":""},{"location":"RELEASE/#overview","title":"Overview","text":"<p>Releases are created from the main repository branch using the version numbers, including an intermediate version suffix,  defined in <code>.make.versions</code>. The following points are important:</p> <ol> <li>In general, a common version number is used for all published pypi wheels and docker images.</li> <li><code>.make.versions</code> contains the version to be used when publishing the next release. </li> <li>Whenever <code>.make.versions</code> is changed, <code>make set-versions</code> should be run from the top of the repo.</li> <li>Corollary: <code>make set-versions</code> should ONLY be used from the top of the repo when <code>.make.versions</code> changes.</li> <li>The main branch always has the version suffix set to .dev\\&lt;N&gt;, which allows intermediate publishing from the main branch using version X.Y.Z.dev\\&lt;N&gt;.</li> <li>The <code>scripts/release-branch.sh</code> script automates creation of a new release branch and tag and version numbers in <code>.make.versions</code> </li> <li>Building and publishing is done manually, or soon via a git action, in the branch created by <code>scripts/release-branch.sh</code>. </li> <li>Wheels can only be published once to pypi for a given version.</li> <li>Transform and kfp images may be republished to the docker registry.</li> <li>Releases done via the <code>release-branch.sh</code> script will have their micro version number set to 0 (e.g., 1.2.0)</li> <li>Intermediate releases that bump the micro version may be done by individual transforms. This can mean that version X.Y.Z of a transform is equivalent to the X.Y+1.0 release.  The latter created when running the <code>release-branch.sh</code> script.</li> </ol>"},{"location":"RELEASE/#cutting-the-release","title":"Cutting the release","text":"<p>Creating the release involves</p> <ol> <li>Editing the <code>release-notes.md</code> to list major/minor changes and commit to the main branch.</li> <li>Creating a release branch and updating the main branch versions (using <code>release-branch.sh</code>).</li> <li>Creating a github release and tag from the release branch using the github web UI.</li> <li>Building and publishing pypi library wheels and docker registry image.</li> </ol> <p>Each is discussed below.</p>"},{"location":"RELEASE/#editing-release-notesmd","title":"Editing release-notes.md","text":"<p>Make a dummy release on github (see below) to get a listing of all commits. Use this to come up with the items. Commit this to the main branch so it is ready for including in the release branch.</p>"},{"location":"RELEASE/#creating-release-branch","title":"Creating release branch","text":"<p>The <code>scripts/release-branch.sh</code> is currently run manually to create the branch and tags as follows:</p> <ol> <li>Creates the <code>releases/vX.Y.Z</code> from the main branch where <code>X.Y.Z</code> are defined in .make.versions</li> <li>Creates the <code>pending-releases/vX.Y.Z</code> branch for PR'ing back into the <code>releases/vX.Y.Z</code> branch. </li> <li>In the new <code>pending-releases/vX.Y.Z</code> branch <ol> <li>Nulls out the version suffix in the new branch's <code>.make.version</code> file. </li> <li>Applies the unsuffixed versions to the artifacts published from the repo using <code>make set-versions</code>..</li> <li>Commits and pushes branch </li> </ol> </li> <li>Creates the <code>pending-version-change/vX.Y.Z</code> branch for PR'ing back into the main branch.<ul> <li>Note: this branch is named with the new release version (i.e. vX.Y.Z), however   the version in this branch is actually X.Y+1.0.dev0.</li> </ul> </li> <li>In the <code>pending-version-change/vX.Y.Z</code> branch<ol> <li>Increments the minor version (i.e. Z+1) and resets the suffix to <code>dev0</code> in <code>.make.versions</code>.</li> <li>Commits and pushes branch </li> </ol> </li> </ol> <p>To double-check the version that will be published from the release, <pre><code>git checkout pending-releases/vX.Y.Z \nmake show-version\n</code></pre> This will print for example, 1.2.3. </p> <p>To run the script from the top of the repo:</p> <pre><code>scripts/release-branch.sh\n</code></pre> <p>After running the script, you should</p> <ol> <li>Create a pull request from branch <code>pending-releases/vX.Y.Z</code> into the <code>releases/vX.Y.Z</code> branch, and merge.</li> <li>Use the github web UI to create a git release and tag of the <code>releases/vX.Y.Z</code> branch</li> <li>Create a pull request from branch <code>pending-version-change/vX.Y.Z</code> into the main branch, and merge. </li> </ol>"},{"location":"RELEASE/#creating-the-github-release","title":"Creating the Github Release","text":"<p>After running the <code>release-branch.sh</code> script, to create tag <code>vX.Y.Z</code> and branch <code>releases/vX.Y.Z</code> and PRing/merging <code>vX.Y.Z</code> into <code>releases/vX.Y.Z</code>. 1. Go to the releases page.  1. Select <code>Draft a new release</code> 1. Select target branch <code>releases/vX.Y.Z</code> 1. Select <code>Choose a tag</code>, type in vX.Y.Z, click <code>Create tag</code> 1. Press <code>Generate release notes</code>  1. Add a title (e.g., Release X.Y.Z)  1. Add any additional relese notes. 1. Press <code>Publish release</code></p>"},{"location":"RELEASE/#building-and-publishing-wheels-and-images","title":"Building and Publishing Wheels and Images","text":"<p>After creating the release and tag on github: </p> <ol> <li>Switch to a release branch (e.g. releases/v1.2.3). </li> <li>Be sure you're at the top of the repository (<code>.../data-prep-kit</code>)</li> <li>Optionally, <code>make show-version</code> to see the version that will be published</li> <li>Running the following, either manually or in a git action<ol> <li><code>make build</code></li> <li><code>make publish</code>   (See credential requirements below)</li> </ol> </li> </ol> <p>For docker registry publishing, the following environment variables/credentials are needed:</p> <ul> <li>DPK_DOCKER_REGISTRY_USER - user used with the registry defined in DOCKER_HOST in <code>.make.defaults</code></li> <li>DPK_DOCKER_REGISTRY_KEY - key/password for docker registry user.</li> </ul> <p>To publish to pypi, the credentials in <code>~/.pypirc</code> file (let us know if there is a way to do this with environment variables). See pypi for details.</p>"},{"location":"release-notes/","title":"Data Prep Kit Release notes","text":""},{"location":"release-notes/#release-100-1242025","title":"Release 1.0.0 - 1/24/2025","text":""},{"location":"release-notes/#general","title":"General","text":"<ol> <li>Refactored all language transforms and implemented simplified APIs for the refactored transforms</li> <li>Added notebook examples for each of the transforms </li> <li>Streamlined documentation and added tutorial for developers who want to build new transforms </li> <li>Other minor enhancements and bug fixes were done for transforms, workflow pipelines, and CI/CD makefiles</li> </ol>"},{"location":"release-notes/#transforms","title":"Transforms","text":"<ol> <li>Added new similarity transform (for detecting confidentiality, copyright, and/or plagiarism in documents)</li> </ol>"},{"location":"release-notes/#release-023-12152024","title":"Release 0.2.3 - 12/15/2024","text":""},{"location":"release-notes/#general_1","title":"General","text":"<p>New algorithm for Fuzzy dedup transform Sample notebooks for some of the language transforms Integrate Semantic profiler and report generation for code profiler transform</p>"},{"location":"release-notes/#data-prep-toolkit-libraries-python-ray-spark","title":"data-prep-toolkit libraries (python, ray, spark)","text":"<ol> <li>Increase ray agent limit to 10,000 (default was 100) </li> </ol>"},{"location":"release-notes/#transforms_1","title":"Transforms","text":"<ol> <li>Fuzzy dedup new algorithm for Python, Ray and Spark</li> </ol>"},{"location":"release-notes/#release-022-11252024","title":"Release 0.2.2 - 11/25/2024","text":""},{"location":"release-notes/#general_2","title":"General","text":"<ol> <li>Update RAG example to use granite model </li> <li>Updated transforms with Docling 2</li> <li>Added single package for dpk with extra for [spark] and [ray]</li> <li>Added single package for transforms with extra for [all] or [individual-transform-name]</li> </ol>"},{"location":"release-notes/#data-prep-toolkit-libraries-python-ray-spark_1","title":"data-prep-toolkit libraries (python, ray, spark)","text":"<ol> <li>Fix metadata logging even when actors crash </li> <li>Add multilock for ray workers downloads/cleanup</li> <li>Multiple updates to spark runtime</li> <li>Added support for python 3.12</li> <li>refactoring of data access code</li> </ol>"},{"location":"release-notes/#kfp-workloads","title":"KFP Workloads","text":"<ol> <li>Modify superpipeline params type Str/json</li> <li>Set kuberay apiserver version </li> <li>Add Super pipeline for code transforms</li> </ol>"},{"location":"release-notes/#transforms_2","title":"Transforms","text":"<ol> <li>Enhance pdf2parquet with docling2 support for extracting HTML, DOCS, etc.</li> <li>Added web2parquet transform</li> <li>Added HAP transform</li> </ol>"},{"location":"release-notes/#http-connector-023","title":"HTTP Connector 0.2.3","text":"<ol> <li>Enhanced parameter/configuration allows the user to customize crawler settings </li> <li>implement subdomain focus feature in data-prep-connector </li> </ol>"},{"location":"release-notes/#release-022-http-connector-module-10232024","title":"Release 0.2.2- HTTP Connector Module - 10/23/2024","text":""},{"location":"release-notes/#general_3","title":"General","text":"<ol> <li>Bug fixes across the repo</li> <li>Minor enhancements and experimentation with single packaging techniques using [extra]</li> <li>Decoupled the release process for each of the component so we can be more responsive to the needs of our stakeholders</li> <li>The minor digit for the release for all components is incremented and the patch digit is reset to 0 for all new releases of the data-prep-toolkit</li> <li>The patch digit for the release of any one component can be increased independently from other component patch number</li> </ol>"},{"location":"release-notes/#data-prep-toolkit-connector","title":"data-prep-toolkit-Connector","text":"<ol> <li>Released first version of the data-prep-toolkit-connector for crawling web sites and downloading HTML and PDF files for ingestion by the pipeline</li> </ol>"},{"location":"release-notes/#release-021-9242024","title":"Release 0.2.1 - 9/24/2024","text":""},{"location":"release-notes/#general_4","title":"General","text":"<ol> <li>Bug fixes across the repo</li> <li>Added AI Alliance RAG demo, tutorials and notebooks and tips for running on google colab</li> <li>Added new transforms and single package for transforms published to pypi</li> <li>Improved CI/CD with targeted workflow triggered on specific changes to specific modules</li> <li>New enhancements for cutting a release</li> </ol>"},{"location":"release-notes/#data-prep-toolkit-libraries-python-ray-spark_2","title":"data-prep-toolkit libraries (python, ray, spark)","text":"<ol> <li>Restructure the repository to distinguish/separate runtime libraries</li> <li>Split data-processing-lib/ray into python and ray</li> <li>Spark runtime</li> <li>Updated pyarrow version</li> <li>Define required transform() method as abstract to AbstractTableTransform</li> <li>Enables configuration of makefile to use src or pypi for data-prep-kit library dependencies </li> </ol>"},{"location":"release-notes/#kfp-workloads_1","title":"KFP Workloads","text":"<ol> <li>Add a configurable timeout before destroying the deployed Ray cluster.</li> </ol>"},{"location":"release-notes/#transforms_3","title":"Transforms","text":"<ol> <li>Added 7 new transdforms including: language identification, profiler, repo level ordering, doc quality, pdf2parquet, HTML2Parquet and PII Transform</li> <li>Added ededup python implementation and incremental ededup </li> <li>Added fuzzy floating point comparison</li> </ol>"},{"location":"release-notes/#release-020-6272024","title":"Release 0.2.0 - 6/27/2024","text":""},{"location":"release-notes/#general_5","title":"General","text":"<ol> <li>Many bug fixes across the repo, plus the following specifics.</li> <li>Enhanced CI/CD and makefile improvements  include definition of top-level targets (clean, set-verions, build, publish, test)</li> <li>Automation of release process branch/tag management</li> <li>Documentation improvements </li> </ol>"},{"location":"release-notes/#data-prep-toolkit-libraries-python-ray-spark_3","title":"data-prep-toolkit libraries (python, ray, spark)","text":"<ol> <li>Split libraries into 3 runtime-specific implementations</li> <li>Fix missing final count of processed and add percentages</li> <li>Improved fault tolerance in python and ray runtimes </li> <li>Report global DataAccess retry metric  </li> <li>Support for binary data transforms</li> <li>Updated to Ray version to 2.24</li> <li>Updated to PyArrow version 16.1.0</li> </ol>"},{"location":"release-notes/#kfp-workloads_2","title":"KFP Workloads","text":"<ol> <li>Add KFP V2 support </li> <li>Create a distinct (timestamped) execution.log file for each retry</li> <li>Support for multiple inputs/outputs</li> </ol>"},{"location":"release-notes/#transforms_4","title":"Transforms","text":"<ol> <li>Added language/lang_id - detects language in documents</li> <li>Added universal/profiler - counts works/tokens in documents</li> <li>Converted ingest2parquet tool to transform named code2parquet</li> <li>Split transforms, as appropriate, into python, ray and/or spark.</li> <li>Added spark implementations of filter, doc_id and noop transforms.</li> <li>Switch from using requirements.txt to pyproject.toml file for each transform runtime</li> <li>Repository restructured to move kfp workflow definitions to associated transform project directory</li> </ol>"},{"location":"release-notes/#release-011-5242024","title":"Release 0.1.1 - 5/24/2024","text":""},{"location":"release-notes/#release-010-5152024","title":"Release 0.1.0 - 5/15/2024","text":""},{"location":"release-notes/#release-010-5082024","title":"Release 0.1.0 - 5/08/2024","text":""},{"location":"resources/","title":"New Features &amp; Enhancements","text":"<ul> <li>We are working hard with your feedback and DPK 1.0.0 alpha has been released with simplified APIs for language transforms. Check out this page for example implementation of some of these transforms.</li> <li>HAP and PII recipe notebooks contributed by our partners in GSI team are now merged.</li> </ul>"},{"location":"resources/#data-prep-kit-resources","title":"Data Prep Kit Resources","text":""},{"location":"resources/#papers","title":"\ud83d\udcc4 Papers","text":"<ol> <li>Data-Prep-Kit: getting your data ready for LLM application development</li> <li>Granite Code Models: A Family of Open Foundation Models for Code Intelligence</li> <li>Scaling Granite Code Models to 128K Context</li> </ol>"},{"location":"resources/#external-events-and-showcase","title":"\ud83c\udfa4 External Events and Showcase","text":"<ol> <li> <p>Workshop at the AI for Connectivity Hackathon: \u201cPreparing Data for LLM Applications with Docling &amp; Data Prep Kit\u201d - Jan 25, 2025</p> </li> <li> <p>Talk on DPK at IBM TechXchange Agents day - Jan 23, 2025 - Slides</p> </li> <li> <p>DPK tutorial at CODS-COMAD 2024 - Dec 18, 2024</p> </li> <li> <p>\u201cGenerative AI Model Data Pre-Training on Kubernetes: A Use Case Study\u201d was accepted for KubeCon EU 2025 - Dec 2024</p> </li> <li> <p>DPK has been added to AI Alliance's \u201cLiving Guide to Applying AI\u201d - Dec 2024</p> </li> <li> <p>Workshop on Preparing Data for LLM Applications Using Data Prep Kit -Dec 2024 - Video </p> </li> <li> <p>DPK tutorial and hands on session at IIIT Delhi - Nov 22, 2024</p> </li> <li> <p>Talk and Hands on session at MIT Bangalore - Nov 8, 2024</p> </li> <li> <p>PyData NYC 2024 - 90 mins Tutorial - Nov 6, 2024</p> </li> <li> <p>\"Data Prep Kit: A Comprehensive Cloud-Native Toolkit for Scalable Data Preparation in GenAI App\" - Oct 28-29, 2024 - Video | Slides</p> </li> <li> <p>Tech Educator summit IBM CSR Event - Oct 16, 2024</p> </li> <li> <p>Data Science Dojo Meetup - Oct 9, 2024 - Video</p> </li> <li> <p>Open Source RAG Pipeline workshop with Data Prep Kit at TechEquity's AI Summit in Silicon Valley - Oct 2024</p> </li> <li> <p>\"RAG with Data Prep Kit\" Workshop @ Mountain View, CA, USA ** - info - Sep 21, 2024</p> </li> <li> <p>IBM TechXchange Las Vegas</p> </li> <li> <p>Unstructured Data Meetup - SF, NYC, Silicon Valley</p> </li> <li> <p>Data Exchange Podcast with Ben Lorica - Sep 2024</p> </li> <li> <p>Open Source AI Demo Night - Aug 8, 2024</p> </li> <li> <p>\"Building Successful LLM Apps: The Power of high quality data\" - Video  |   Slides - Aug 2024</p> </li> <li> <p>\"Hands on session for fine tuning LLMs\" - Video - Aug 2024</p> </li> <li>\"Build your own data preparation module using data-prep-kit\" - Video - Aug 2024</li> </ol>"},{"location":"resources/#example-code","title":"Example Code","text":"<p>Find example code in readme section of each tranform and some sample jupyter notebooks for getting started here</p>"},{"location":"resources/#blogs-tutorials","title":"Blogs / Tutorials","text":"<ul> <li>Blog on Ziliz \"Introducing IBM Data Prep Kit for Streamlined LLM Workflows\" by Yesha Shastri, Freelance Technical Writer in AI/ML</li> <li>IBM Developer Blog </li> <li>Introductory Blog on DPK</li> <li>DPK Header Cleanser Module Blog by external contributor </li> </ul>"},{"location":"resources/#relevant-online-communities","title":"Relevant online communities","text":"<ul> <li>Data Prep Kit Discord Channel</li> <li>DPK is now listed in Github Awesome-LLM under LLM Data section</li> <li>DPK is now up for access via IBM Skills Build Download</li> <li>DPK added to the Application Hub of \u201cAI Sustainability Catalog\u201d</li> </ul>"},{"location":"resources/#we-want-your-feedback","title":"We Want Your Feedback!","text":"<p>Feel free to contribute to discussions or create a new one to share your feedback</p>"},{"location":"data-connector-lib/","title":"DPK Connector","text":"<p>DPK Connector is a scalable and compliant web crawler developed for data acquisition towards LLM development. It is built on Scrapy. For more details read the documentation.</p>"},{"location":"data-connector-lib/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>make venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-connector-lib/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library <pre><code>make test build publish\n</code></pre></p> <p>To up the version number, edit the Makefile to change VERSION and rerun the above. This will require committing both the <code>Makefile</code> and the autotmatically updated <code>pyproject.toml</code> file.</p>"},{"location":"data-connector-lib/#how-to-use","title":"How to use","text":"<p>See the overview.</p>"},{"location":"data-connector-lib/doc/overview/","title":"DPK Connector Overview","text":"<p>The Data Prep Kit Connector (DPK Connector) is a Python library for scalable and compliant web crawling.</p> <p>Features: - Robots.txt compliant: The Connector follows allow/disallow lists and some extended directives such as <code>Crawl-delay</code> in robots.txt of websites. - Sitemap support: The Connector automatically parses sitemap urls from input and tries to find them from robots.txt. - User agent and headers customization: You can use your own user agent string and request headers. - Domain and path focus: You can limit domains and paths accessed by the library. - Mime type filters: You can restrict mime types which can be downloaded. - Parallel processing: Requests to websites are processed in parallel.</p>"},{"location":"data-connector-lib/doc/overview/#how-to-install","title":"How to install","text":""},{"location":"data-connector-lib/doc/overview/#from-pypi","title":"From PyPI","text":"<pre><code>pip install data-prep-connector\n</code></pre>"},{"location":"data-connector-lib/doc/overview/#from-github","title":"From Github","text":"<pre><code>pip install git+https://github.com/IBM/data-prep-kit.git@dev#subdirectory=data-connector-lib\n</code></pre>"},{"location":"data-connector-lib/doc/overview/#example-usage","title":"Example usage","text":"<pre><code>from dpk_connector import crawl, shutdown\n\n\ndef main():\n    \"\"\"\n    An example of running a crawl.\n    \"\"\"\n\n    def on_downloaded(url: str, body: bytes, headers: dict) -&gt; None:\n        \"\"\"\n        Callback function called when a page has been downloaded.\n        You have access to the request URL, response body and headers.\n        \"\"\"\n        print(f\"url: {url}, headers: {headers}, body: {body[:64]}\")\n\n    user_agent = \"Mozilla/5.0 (X11; Linux i686; rv:125.0) Gecko/20100101 Firefox/125.0\"\n\n    # Start crawling\n    crawl(\n        [\"https://crawler-test.com/\"],\n        on_downloaded,\n        user_agent=user_agent,\n        depth_limit=0,\n    )  # blocking call\n\n    # Shutdown all crawls\n    shutdown()\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"data-processing-lib/","title":"Data Processing Library","text":"<p>This provides a python framework for developing transforms on data stored in files - currently parquet files are supported - and running them in a ray cluster. Data files may be stored in the local file system or  COS/S3. For more details see the documentation.</p>"},{"location":"data-processing-lib/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>make venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-processing-lib/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library  <pre><code>make test build publish\n</code></pre></p> <p>To up the version number, edit the Makefile to change VERSION and rerun the above.  This will require committing both the <code>Makefile</code> and the autotmatically updated <code>pyproject.toml</code> file.</p>"},{"location":"data-processing-lib/doc/architecture/","title":"Data Processing Architecture","text":"<p>In this section we cover the high-level architecture, some of the core components.  </p> <p>Transform implementation and examples are provided in the tutorial.</p>"},{"location":"data-processing-lib/doc/architecture/#architecture","title":"Architecture","text":"<p>The architecture is a \"standard\" implementation of Embarrassingly parallel to process many input files in parallel using a distribute network of RayWorkers.</p> <p></p> <p>The architecture includes the following core components: </p> <ul> <li> <p>RayLauncher accepts and validates   CLI parameters to establish the Ray Orchestrator with the proper configuration.  It uses the following components, all of which can/do define CLI configuration parameters.:</p> <ul> <li>Transform Orchestrator Configuration is responsible   for defining and validating infrastructure parameters   (e.g., number of workers, memory and cpu, local or remote cluster, etc.). This class has very simple state  (several dictionaries) and is fully pickleable. As a result framework uses its instance as a  parameter in remote functions/actors invocation.</li> <li>DataAccessFactory - provides the   configuration for the type of DataAccess to use when reading/writing the input/output data for   the transforms.  Similar to Transform Orchestrator Configuration, this is a pickleable   instance that is passed between Launcher, Orchestrator and Workers.</li> <li>TransformConfiguration - defines specifics   of the transform implementation including transform implementation class, its short name, any transform-   specific CLI parameters, and an optional TransformRuntime class, discussed below. </li> </ul> <p>After all parameters are validated, the ray cluster is started and the DataAccessFactory, TransformOrchestratorConfiguraiton and TransformConfiguration are given to the Ray Orchestrator, via Ray remote() method invocation. The Launcher waits for the Ray Orchestrator to complete.</p> </li> <li> <p>documents with Ray Orchestrator is responsible for overall management of   the data processing job. It creates the actors, determines the set of input data and distributes the    references to the data files to be processed by the workers. More specifically, it performs the following:</p> </li> <li> <p>Uses the DataAccess instance created by the DataAccessFactory to determine the set of the files    to be processed.  </p> </li> <li>uses the TransformConfiguration to create the TransformRuntime instance </li> <li>Uses the TransformRuntime to optionally apply additional configuration (ray object storage, etc) for the configuration   and operation of the Transform.</li> <li>uses the TransformOrchestratorConfiguration to determine the set of RayWorkers to create   to execute transformers in parallel, providing the following to each worker:<ul> <li>Ray worker configuration</li> <li>DataAccessFactory </li> <li>Transform class and its TransformConfiguration containing the CLI parameters and any TransformRuntime additions.</li> </ul> </li> <li>in a load-balanced, round-robin fashion, distributes the names of the input files to the workers for them to transform/process.</li> </ul> <p>Additionally, to provide monitoring of long-running transforms, the orchestrator is instrumented with    custom metrics, that are exported to localhost:8080 (this is the endpoint that    Prometheus would be configured to scrape).   Once all data is processed, the orchestrator will collect execution statistics (from the statistics actor)    and build and save it in the form of execution metadata (<code>metadata.json</code>). Finally, it will return the execution    result to the Launcher.</p> <ul> <li> <p>Ray worker is responsible for  reading files (as PyArrow Tables) assigned by the orchestrator, applying the transform to the input table and writing out the  resulting table(s).  Metadata produced by each table transformation is aggregated into Transform Statistics (below).</p> </li> <li> <p>Transform Statistics is a general  purpose data collector actor aggregating the numeric metadata from different places of  the framework (especially metadata produced by the transform). These statistics are reported as metadata (<code>metadata.json</code>) by the orchestrator upon completion.</p> </li> </ul>"},{"location":"data-processing-lib/doc/architecture/#core-components","title":"Core Components","text":"<p>Some of the core components used by the architecture are definfed here:</p> <ul> <li>CLIProvider - provides a general purpose   mechanism for defining, validating and sharing CLI parameters.    It is used by the DataAccessFactor and Transform Configuration (below).</li> <li>Data Access is an abstraction layer for different data access supported by the framework. The main components   of this layer are:</li> <li>Data Access is the basic interface for the data access, and enables the identification of    input files to process, associated output files, checkpointing and general file reading/writing.     Currently, the framework implements several concrete implementations of the Data Access, including     local data support and     s3. Additional Data Access implementations can be added as required.</li> <li>Data Access Factory is an implementation of the      factory design pattern for creation     of the data access instances. Data Access Factory, as a CLIProvider,  enables the definition of CLI      parameters that configure the instance of Data Access to be created. Data Access factory has very simple state      (several dictionaries) and is fully pickleable. The framework uses Data Access Factory instance as a      parameter in remote functions/actors invocations.</li> </ul>"},{"location":"data-processing-lib/doc/architecture/#transforms","title":"Transforms","text":"<p>A brief discussion of the Transform components are provided here. For a more complete discussion, see the tutorials.</p> <ul> <li>Transform - defines the methods required of any transform implementation - <code>transform()</code> and <code>flush()</code> - and provides the bulk of any transform implementation convert one Table to 0 or more new Tables.   In general, this is not tied to the above Ray infrastructure  and so can usually be used independent of Ray. </li> <li>TransformConfiguration - this is the bootstrap   class provided to the Launcher that enables the instantiation of the Transform and the TransformRuntime within   the architecture.  It is a CLIProvider, which allows it to define transform-specific CLI configuration   that is made available to the Transform's initializer.</li> </ul>"},{"location":"data-processing-lib/doc/data-access-factory/","title":"Data Access Factory","text":""},{"location":"data-processing-lib/doc/data-access-factory/#introduction","title":"Introduction","text":"<p>Data Access Factory(DAF) provides a mechanism to create  DataAccess  implementations that support the processing of input data files and the expected destination of the processed files. The <code>DataAccessFactory</code> is most often configured using command line arguments to specify the type of <code>DataAccess</code> instance to create (see <code>--data_*</code> options here. Currently,  it supports DataAccessLocal and  DataAccessS3 implementations.</p> <p>You can use DAF and the resulting DataAccess implementation in your transform logic to read and write extra file(s), for example, write log or metadata files.</p> <p>This document explains how to initialize and use DAF to write a file using a <code>DataAccess</code> instance. </p>"},{"location":"data-processing-lib/doc/data-access-factory/#data-access","title":"Data Access","text":"<p>Each Data Access implementation supports the notion of processing a set of input files to produce a set of output files, generally in a 1:1 mapping,  although this is not strictly required. With this in mind, the following function is provided:  * Input file identification by      * input folder      * sub-directory selection (aka data sets))     * file extension     * files extensions to checkpoint     * maximum count     * random sampling * Output file identification (for a given input) * Checkpointing  - determines the set of input files that need processing  (i.e. which do not have corresponding output files). In the case of parquet files, where inputs and outputs are parquet this comparison is fairly simple. In the case of binary files it is a little bit more involved as input and output files may have different extensions. in this case you need to specify both <code>files extensions</code> and <code>files extensions to checkpoint</code> * Reading and writing of files.</p> <p>Each transform runtime uses a DataAccessFactory to create a DataAccess instance which is then used to identify and process the target input data. Transforms may use this the runtime instance or can use their own DataAccessFactory. This might be needed if reading or writing other files to/from other locations.</p>"},{"location":"data-processing-lib/doc/data-access-factory/#creating-daf-instance","title":"Creating DAF instance","text":"<p><pre><code>from data_processing.data_access import DataAccessFactory\ndaf = DataAccessFactory(\"myprefix_\", False)\n</code></pre> The first parameter <code>cli_arg_prefix</code> is prefix used to look for parameter names  starting with prefix <code>myprefix_</code>. Generally the prefix used is specific to the transform.</p>"},{"location":"data-processing-lib/doc/data-access-factory/#preparing-and-setting-parameters","title":"Preparing and setting parameters","text":"<p><pre><code>from argparse import Namespace\n\ns3_cred = {\n    \"access_key\": \"XXXX\",\n    \"secret_key\": \"XXX\",\n    \"url\": \"https://s3.XXX\",\n}\n\ns3_conf={\n    'input_folder': '&lt;COS Location of input&gt;', \n    'output_folder': 'cos-optimal-llm-pile/somekey'\n}\n\nargs = Namespace(\n    myprefix_s3_cred=s3_cred,\n    myprefix_s3_config=s3_conf,\n)\nassert daf.apply_input_params(args)\n</code></pre> <code>apply_input_params</code> will extract and use parameters from <code>args</code> with  prefix <code>myprefix_</code>(which is <code>myprefix_s3_cred</code> and <code>myprefix_s3_config</code> in this example).</p> <p>The above is equivalent to passing the following on the command line to a runtime launcher <pre><code>... --myprefix_s3_cred '{ \"access_key\": \"XXXX\", \"secret_key\": \"XXX\", \"url\": \"https:/s3.XXX\" }'\\\n    --myprefix_s3_config '{ \"input_folder\": \"&lt;COS Location of input&gt;\", \"cos-optimal-llm-pile/somekey\" }'\n</code></pre></p>"},{"location":"data-processing-lib/doc/data-access-factory/#create-dataaccess-and-write-file","title":"Create DataAccess and write file","text":"<pre><code>data_access = daf.create_data_access()\ndata_access.save_file(f\"data/report.log\", \"success\")\n</code></pre> <p>Call to <code>create_data_access</code> will create the <code>DataAccess</code> instance (<code>DataAccessS3</code> in this case) . <code>save_file</code> will write a new file at <code>data/report.log</code> with content <code>success</code>.</p> <p>When writing a transform, the <code>DataAccessFactory</code> is generally created in the transform's configuration class and passed to the transform's initializer by the runtime.  See this section on accessing external resources for details.</p>"},{"location":"data-processing-lib/doc/launcher-options/","title":"Runtime Command Line Options","text":"<p>A number of command line options are available when launching a transform. * Transform options defined by the specific transform * Runtime/launcher independent options, primarily for identifying data sources and destinations.  * Runtime-specific options for controlling aspects of the individual runtime.</p> <p>The runtime options are discussed below (see the specific transform or using -help to determine transform options.)</p>"},{"location":"data-processing-lib/doc/launcher-options/#runtime-independent-launcher-cli-arguments","title":"Runtime-independent Launcher CLI Arguments","text":"<p>The following are the set of command line launcher options available to all runtimes. <pre><code>  --data_s3_cred DATA_S3_CRED\n                        AST string of options for s3 credentials. Only required for S3 data access.\n                        access_key: access key help text\n                        secret_key: secret key help text\n                        url: optional s3 url\n                        region: optional s3 region\n                        Example: { 'access_key': 'access', 'secret_key': 'secret', \n                        'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud', \n                        'region': 'us-east-1' }\n  --data_s3_config DATA_S3_CONFIG\n                        AST string containing input/output paths.\n                        input_folder: Path to input folder of files to be processed\n                        output_folder: Path to output folder of processed files\n                        Example: { 'input_folder': 's3-path/your-input-bucket', \n                        'output_folder': 's3-path/your-output-bucket' }\n  --data_local_config DATA_LOCAL_CONFIG\n                        ast string containing input/output folders using local fs.\n                        input_folder: Path to input folder of files to be processed\n                        output_folder: Path to output folder of processed files\n                        Example: { 'input_folder': './input', 'output_folder': '/tmp/output' }\n  --data_max_files DATA_MAX_FILES\n                        Max amount of files to process\n  --data_checkpointing DATA_CHECKPOINTING\n                        checkpointing flag\n  --data_data_sets DATA_DATA_SETS\n                        List of sub-directories of input directory to use for input. For example, ['dir1', 'dir2']\n  --data_files_to_use DATA_FILES_TO_USE\n                        list of file extensions to choose for input.\n  --data_num_samples DATA_NUM_SAMPLES\n                        number of random input files to process\n</code></pre></p>"},{"location":"data-processing-lib/doc/launcher-options/#python-launcher-cli-arguments","title":"Python Launcher CLI Arguments","text":"<p>The following are the set of command line launcher options available on for the python runtime. <pre><code>  --runtime_num_processors RUNTIME_NUM_PROCESSORS\n                        size of multiprocessing pool\n  --runtime_pipeline_id RUNTIME_PIPELINE_ID\n                        pipeline id\n  --runtime_job_id RUNTIME_JOB_ID\n                        job id\n  --runtime_code_location RUNTIME_CODE_LOCATION\n                        AST string containing code location\n                        github: Github repository URL.\n                        commit_hash: github commit hash\n                        path: Path within the repository\n                        Example: { 'github': 'https://github.com/somerepo', 'commit_hash': '1324', \n                        'path': 'transforms/universal/code' }\n</code></pre></p>"},{"location":"data-processing-lib/doc/launcher-options/#ray-launcher-cli-arguments","title":"Ray Launcher CLI Arguments","text":"<p>The following are the set of command line launcher options available on for the Ray runtime. <pre><code>  --runtime_num_workers RUNTIME_NUM_WORKERS\n                        number of workers\n  --runtime_worker_options RUNTIME_WORKER_OPTIONS\n                        AST string defining worker resource requirements.\n                        num_cpus: Required number of CPUs.\n                        num_gpus: Required number of GPUs\n                        resources: The complete list can be found at\n                                   https://docs.ray.io/en/latest/ray-core/api/doc/ray.remote_function.RemoteFunction.options.html#ray.remote_function.RemoteFunction.options\n                                   and contains accelerator_type, memory, name, num_cpus, num_gpus, object_store_memory, placement_group,\n                                   placement_group_bundle_index, placement_group_capture_child_tasks, resources, runtime_env,\n                                   scheduling_strategy, _metadata, concurrency_groups, lifetime, max_concurrency, max_restarts,\n                                   max_task_retries, max_pending_calls, namespace, get_if_exists\n                        Example: { 'num_cpus': '8', 'num_gpus': '1', \n                        'resources': '{\"special_hardware\": 1, \"custom_label\": 1}' }\n  --runtime_creation_delay RUNTIME_CREATION_DELAY\n                        delay between actor' creation\n  --runtime_pipeline_id RUNTIME_PIPELINE_ID\n                        pipeline id\n  --runtime_job_id RUNTIME_JOB_ID\n                        job id\n  --runtime_code_location RUNTIME_CODE_LOCATION\n                        AST string containing code location\n                        github: Github repository URL.\n                        commit_hash: github commit hash\n                        path: Path within the repository\n                        Example: { 'github': 'https://github.com/somerepo', 'commit_hash': '1324', \n                        'path': 'transforms/universal/code' }\n</code></pre></p>"},{"location":"data-processing-lib/doc/launcher-options/#spark-launcher-cli-arguments","title":"Spark Launcher CLI Arguments","text":"<p>The following are the set of command line launcher options available on for the Spark runtime. <pre><code>  --runtime_pipeline_id RUNTIME_PIPELINE_ID\n                        pipeline id\n  --runtime_job_id RUNTIME_JOB_ID\n                        job id\n  --runtime_code_location RUNTIME_CODE_LOCATION\n                        AST string containing code location\n                        github: Github repository URL.\n                        commit_hash: github commit hash\n                        path: Path within the repository\n                        Example: { 'github': 'https://github.com/somerepo', 'commit_hash': '1324', \n                        'path': 'transforms/universal/code' }\n</code></pre></p>"},{"location":"data-processing-lib/doc/overview/","title":"Data Processing Overview","text":"<p>The Data Processing Framework is python-based and enables the  application of \"transforms\" to  a one or more input data files  to produce one or more output data files. Various runtimes are available to execute the transforms using a common shared methodology and mechanism to configure input and output across either local or S3-base storage.</p> <p>The framework allows simple 1:1 transformation of (parquet) files, but also enables more complex transformations requiring coordination among transforming nodes. This might include operations such as de-duplication, merging, and splitting. The framework uses a plug-in model for the primary functions.  The core transformation-specific classes/interfaces are as follows:</p> <ul> <li>AbstractBinaryTransform -  a simple, easily-implemented interface allowing the definition transforms of arbitrary data as a byte array. Additionally table transform interface is provided allowing definition of transforms operating on  pyarrow tables.</li> <li>TransformConfiguration - defines the transform short name, its implementation class,  and command line configuration parameters.</li> </ul> <p>In support of running a transform over a set of input data in a runtime, the following class/interfaces are provided:</p> <ul> <li>AbstractTransformLauncher - is the central runtime interfacee expected to be implemented by each runtime (python ray, spark, etc.) to apply a transform to a set of data.   It is configured with a <code>TransformRuntimeConfiguration</code> and a <code>DataAccessFactory</code> instance (see below).</li> <li>DataAccessFactory - is used to configure the input and output data files to be processed and creates the <code>DataAccess</code> instance (see below) according to the CLI parameters.</li> <li>TransformRuntimeConfiguration - captures   the <code>TransformConfiguration</code> and runtime-specific configuration.</li> <li>DataAccess - is   the interface defining data i/o methods and selection.  Implementations for local   and S3 storage are provided.</li> </ul> <p></p> <p>To learn more consider the following:</p> <ul> <li>Transforms</li> <li>Transform Exceptions</li> <li>Transform Runtimes</li> <li>Transform Tutorial Example</li> <li>Data Access Factory</li> <li>Testing Transforms</li> <li>Utilities</li> <li>Architecture Deep Dive</li> <li>Transform project root readme</li> </ul>"},{"location":"data-processing-lib/doc/python-runtime/","title":"Python runtime","text":""},{"location":"data-processing-lib/doc/python-runtime/#python-runtime","title":"Python Runtime","text":"<p>The python runtime provides a simple mechanism to run a transform on a set of input data to produce a set of output data, all within a single python execution environment. We currently support two  options of Python execution: * Sequential execution - all files are processed sequentially * Usage of the Python multiprocessing pool.  In this case execution start user-defined number of processors which allows to parallelize data processing</p> <p><code>Note</code> some of transformers, for example, exact dedup do not support multi processing Python runtime, as they rely on a shared classes, which are not supported by this runtime</p> <p>To support multiprocessing pool based runtime, Python execution introduced an additional  parameter: * <code>runtime_num_processors</code> defines the number of processors to use for execution. If this number is greater then 0, multiprocessing pool runtime is used with number of processor equal to  <code>num_processors</code>. Default number of processors is 0.</p> <p>Usage of this parameter allows user to choose the type of Python execution runtime and configure parallelism in the case of multiprocessing pool.</p> <p>A <code>PythonTransformLauncher</code> class is provided that enables the running of the transform.  For example,</p> <p><pre><code>launcher = PythonTransformLauncher(YourTransformConfiguration())\nlauncher.launch()\n</code></pre> The <code>YourTransformConfiguration</code> class configures your transform. More details can be found in the transform tutorial.</p>"},{"location":"data-processing-lib/doc/ray-runtime/","title":"Ray Runtime","text":"<p>The Ray runtime provides the ability to run in either a local or Kubernetes cluster, and includes the following set of components:</p> <ul> <li>RayTransformLauncher - this is a  class generally used to implement <code>main()</code> that makes use of a <code>TransformConfiguration</code> to  start the Ray runtime and execute the transform over the specified set of input files. The RayTransformLauncher is created using a <code>RayTransformConfiguration</code> instance.</li> <li>RayTransformConfiguration - this  class extends transform's base TransformConfiguration implementation to add an optional  <code>TranformRuntime</code> (see next) class to be used by the transform implementation.</li> <li>TransformRuntime -  this provides the ability for the transform implementor to create additional Ray resources  and include them in the configuration used to create a transform (see, for example, </li> <li>fuzzy dedup Many transforms will not need additional resources and can use the DefaultRayTransformRuntime. <code>TransformRuntime</code> also provide the ability to supplement the statics collected by Statistics (see below).</li> </ul> <p>Roughly speaking the following steps are completed to establish transforms in the RayWorkers</p> <ol> <li>Launcher parses the CLI parameters using an ArgumentParser configured with its own CLI parameters  along with those of the Transform Configuration, </li> <li>Launcher passes the Transform Configuration and CLI parameters to the RayOrchestrator</li> <li>RayOrchestrator creates the Transform Runtime using the Transform Configuration and its CLI parameter values</li> <li>Transform Runtime creates transform initialization/configuration including the CLI parameters, and any Ray components need by the transform.</li> <li>RayWorker is started with configuration from the Transform Runtime.</li> <li>RayWorker creates the Transform using the configuration provided by the Transform Runtime.</li> <li>Statistics is used to collect the statistics submitted by the individual transform, that  is used for building execution metadata.</li> </ol> <p></p>"},{"location":"data-processing-lib/doc/ray-runtime/#ray-transform-launcher","title":"Ray Transform Launcher","text":"<p>The RayTransformLauncher uses the Transform Configuration and provides a single method, <code>launch()</code>, that kicks off the Ray environment and transform execution coordinated  by orchestrator. For example, <pre><code>launcher = RayTransformLauncher(YourTransformConfiguration())\nlauncher.launch()\n</code></pre> Note that the launcher defines some additional CLI parameters that are used to control the operation of the  orchestrator and workers and  data access.  Things such as data access configuration, number of workers, worker resources, etc. Discussion of these options is beyond the scope of this document  (see Launcher Options for a list of available options.)</p>"},{"location":"data-processing-lib/doc/ray-runtime/#transform-configuration","title":"Transform Configuration","text":"<p>In general, a transform should be able to run in both the python and Ray runtimes. As such we first define the python-only transform configuration, which will then be used by the Ray-runtime-specific transform configuration.  The python transform configuration implements TransformConfiguration and defines with transform-specific name, and implementation  and class. In addition, it is responsible for providing transform-specific methods to define and capture optional command line arguments. <pre><code>class YourTransformConfiguration(TransformConfiguration):\n\n    def __init__(self):\n        super().__init__(name=\"YourTransform\", transform_class=YourTransform)\n        self.params = {}\n\n    def add_input_params(self, parser: ArgumentParser) -&gt; None:\n        ...\n    def apply_input_params(self, args: Namespace) -&gt; bool:\n        ...\n</code></pre> Next we define the Ray-runtime specific transform configuration as an extension of the RayTransformConfiguration and uses the <code>YourTransformConfiguration</code> above. <pre><code>class YourTransformConfiguration(RayTransformConfiguration):\n    def __init__(self):\n        super().__init__(YourTransformConfiguration(),\n                         runtime_class=YourTransformRuntime)\n</code></pre> This class provides the ability to create the instance of <code>YourTransformRuntime</code> class (see below) as needed by the Ray runtime.  Note, that not all transforms will require a <code>runtime_class</code> and can omit this parameter to default to an acceptable runtime class. Details are covered in the advanced transform tutorial.</p>"},{"location":"data-processing-lib/doc/ray-runtime/#transform-runtime","title":"Transform Runtime","text":"<p>The  DefaultRayTransformRuntime class is provided and will be  sufficient for many use cases, especially 1:1 table transformation. However, some transforms will require use of the Ray environment, for example, to create additional workers, establish a shared memory object, etc. Of course, these transforms will generally not run outside of a Ray environment. </p> <pre><code>class DefaultRayTransformRuntime:\n\n    def __init__(self, params: dict[str, Any]):\n        ...\n\n    def get_transform_config(\n        self, data_access_factory: DataAccessFactory, statistics: ActorHandle, files: list[str]\n    ) -&gt; dict[str, Any]:\n        ...\n\n    def compute_execution_stats(self, stats: dict[str, Any]) -&gt; dict[str, Any]:\n        ...\n</code></pre> <p>The RayOrchestrator initializes the instance with the CLI parameters provided by the Transform Configurations <code>get_input_params()</code> method.</p> <p>The <code>get_transform_config()</code> method is used by the RayOrchestrator to create the parameters used to initialize the Transform in the RayWorker.  This is where additional Ray components would be added to the environment  and references added to them, as needed, in the returned dictionary of configuration data that will initialize the transform. For those transforms that don't need this support, the default implementation simpy returns the CLI parameters used to initialize the runtime instance.</p> <p>The <code>computed_execution_stats()</code> provides an opportunity to augment the statistics collected and aggregated by the TransformStatistics actor. It is called by the RayOrchestrator after all files have been processed.</p>"},{"location":"data-processing-lib/doc/spark-runtime/","title":"Spark Framework","text":"<p>The Spark runtime implementation is roughly based on the ideas from  here, here and here.  Spark itself is basically used for execution parallelization, but all data access is based on the framework's data access, thus preserving all the implemented features. At  the start of the execution, the list of files to process is obtained (using data access framework) and then split between Spark workers for reading actual data, its transformation and writing it back. The implementation is based on Spark RDD (For comparison of the three Apache Spark APIs:  RDDs, DataFrames, and Datasets see this  Databricks blog post) As defined by Databricks: <pre><code>RDD was the primary user-facing API in Spark since its inception. At the core, an RDD is an \nimmutable distributed collection of elements of your data, partitioned across nodes in your \ncluster that can be operated in parallel with a low-level API that offers transformations \nand actions.\n</code></pre> This APIs fits perfectly into what we are implementing. It allows us to fully leverage our  existing DataAccess APIs thus preserving all of the investments into flexible, reliable data  access. Additionally RDDs flexible low-level control allows us to work on partition level,  thus limiting the amount of initialization and set up. Note that in our approach transform's processing is based on either binary or parquet data,  not Spark DataFrames or DataSet. We are not currently supporting supporting these Spark APIs,  as they are not well mapped into what we are implementing.</p> <p>In our implementation we are using  pyspark.SparkContext.parallelize for running multiple transforms in parallel. We allow 2 options for specifying the number of partitions, determining  how many partitions the RDD should be divided into. See here for the explanation of this parameter: * If you specify a positive value of the parameter, Spark will attempt to evenly   distribute the data from seq into that many partitions. For example, if you have   a collection of 100 elements and you specify numSlices as 4, Spark will try   to create 4 partitions with approximately 25 elements in each partition.  * If you don\u2019t specify this parameter, Spark will use a default value, which is   typically determined based on the cluster configuration or the available resources   (number of workers).</p>"},{"location":"data-processing-lib/doc/spark-runtime/#transforms","title":"Transforms","text":"<ul> <li>SparkTransformRuntimeConfiguration   allows to configure transform to use PySpark. In addition to its base class   TransformRuntimeConfiguration features,   this class includes <code>get_bcast_params()</code> method to get very large configuration settings. Before starting the   transform execution, the Spark runtime will broadcast these settings to all the workers.</li> </ul>"},{"location":"data-processing-lib/doc/spark-runtime/#runtime","title":"Runtime","text":"<p>Spark runtime extends the base framework with the following set of components: * SparkTransformExecutionConfiguration   allows to configure Spark execution * SparkTransformFileProcessor extends   AbstractTransformFileProcessor to work on   PySpark * SparkTransformLauncher allows   to launch PySpark runtime and execute a transform * orchestrate function orchestrates Spark   based execution</p>"},{"location":"data-processing-lib/doc/testing-e2e-transform/","title":"Testing End-to-End Transform operation","text":"<p>WIP - Points to discuss</p> <ol> <li>Reading input files and writing output files. </li> <li>Testing of the transform runtime and use of ray components in the transform</li> </ol>"},{"location":"data-processing-lib/doc/transform-exceptions/","title":"Exceptions","text":"<p>A transform may find that it needs to signal error conditions. For example, if a referenced model could not be loaded or a given input data (e.g., pyarrow Table) does not have the expected format (.e.g, columns). In general, it should identify such conditions by raising an exception.  With this in mind, there are two types of exceptions:</p> <ol> <li>Those that would not allow any data to be processed (e.g. model loading problem).</li> <li>Those that would not allow a specific datum to be processed (e.g. missing column).</li> </ol> <p>In the first situation the transform should throw an  unrecoverable exception, which will cause the runtime to terminate processing of all data.  Note: any exception thrown from <code>init</code> method of transform will cause runtime to  terminate processing</p> <p>In the second situation (identified in the <code>transform()</code> or <code>flush()</code> methods), the transform should throw an exception from the associated method. This will cause only the error-causing datum to be ignored and not written out, but allow continued processing of tables by the transform. In both cases, the runtime will log the exception as an error.</p>"},{"location":"data-processing-lib/doc/transform-external-resources/","title":"Support for external resources","text":"<p>Often when implementing a transform, the transform will require loading its own resources  (e.g. models, configuration, etc.) to complete its job.  For example, the Blocklist transform loads a list of domains to block.  Resources can be loaded from either S3 or local storage or a  custom location defined by the transform (i.e. hugging face, etc). In addition to actually loading the resource(s), the transform needs to define the configuration that  defines the location of the domain list. </p> <p>In the next sections we cover the following:</p> <ol> <li>How to define the transform-specific resource location(s) as command line arguments</li> <li>How to load the transform-specific resources, either or both of:<ol> <li>During transform initialization - this is useful for testing outside of ray, and optionally</li> <li>During transform configuration in the Ray runtime.  This may not be feasible if a resource       is not picklable.</li> </ol> </li> </ol>"},{"location":"data-processing-lib/doc/transform-external-resources/#defining-transform-specific-resource-locations","title":"Defining Transform-specific Resource Locations","text":"<p>Each transform has a configuration class that defines the command line options with which the transform can be configured.  In the example below, the  DataAccessFactory  is used in the configuration to add transform-specific arguments that allow a <code>DataAccessFactory</code> to be initialized specifically for the transform.  The initialized <code>DataAcessFactory</code> is then made available to the transform's initializer to enable it to read from transform-specific location.  Note that you may choose not to use the DataAccessFactory and might have your own mechanism for loading a  resource (for example, to load a hugging face model).  In this case you will define CLI arguments that allow you to configure where the resources is located. The implementation using DataAccessFactory looks as follows (the code here is from block listing):</p> <p><pre><code>class BlockListTransformConfiguration(DefaultTransformConfiguration):\n    ...\n    def add_input_params(self, parser: argparse.ArgumentParser) -&gt; None:\n        \"\"\"\n        Add Transform-specific arguments to the given parser.\n        This will be included in a dictionary used to initialize the BlockListTransform.\n        By convention a common prefix should be used for all mutator-specific CLI args\n        (e.g, noop_, pii_, etc.)\n        \"\"\"\n\n        ...\n\n       # The DataAccess created by the DataAccessFactory below will use this CLI arg value \n       parser.add_argument(\n            f\"--{blocked_domain_list_path_key}\",\n            type=str,\n            required=False,\n            default=blocked_domain_list_path_default,\n            help=\"S3/COS URL or local folder (file or directory) that points to the list of block listed domains.\"\n        )  \n        # Create the DataAccessFactor to use CLI args with the given blocklist prefix.\n        self.daf = DataAccessFactory(f\"{arg_prefix}_\")\n        # Add the DataAccessFactory parameters to the transform's configuration parameters.\n        self.daf.add_input_params(parser)\n</code></pre> We are creating the <code>DataAccessFactory</code> using a transform-specific prefix to define the transform-specific command line options to configure the  transform's factory instance. In this case, all the transform's DataAccessFactory parameters are prepended with  <code>blocklist_</code>, (<code>arg_prefix</code>).  For example <code>blocklist_s3_cred</code>.</p> <p>After configuring the command line argument parser above,  The BlocklistConfiguration <code>apply_input_params()</code> is implemented to capture all  <code>blocklist_</code> prefixed parameters and apply the arguments  to the DataAccessFactory. In addition, it adds the factory to the parameters that will be made available to the transform. In this way, the transform initializer will receive the DataAccessFactory created and initialized by the configuration instance.</p> <p><pre><code>    def apply_input_params(self, args: argparse.Namespace) -&gt; bool:\n        # Capture the args that are specific to this transform\n        ...\n\n        # Add the DataAccessFactory to the transform's configuration parameters.\n        self.params[block_data_factory_key] = self.daf\n        # mark this parameter to be removed\n        self.remove_from_metadata.append(block_data_factory_key)\n        # Validate and populate the transform's DataAccessFactory \n        return self.daf.apply_input_params(args)\n</code></pre> Note here, that as daf can contain secrets we do not want him to show up in the execution metadata, we add its key to the <code>self.remove_from_metadata</code> array. All the keys contained in this array will be removed from metadata. This  can also be very usefull for any keys cantaining sensitive information, for example, secrets, passwords, etc.</p> <p>The above code can be run in a non-ray main() as follows:  <pre><code>if __name__ == \"__main__\":\n    parser = ArgumentParser()\n    bltc = BlockListTransformConfiguration()\n    bltc.add_input_params(parser)\n    args = parser.parse()\n    config = bltc.apply_input_params(args) \n    transform = BlockListTransform(config)\n    ...\n</code></pre></p>"},{"location":"data-processing-lib/doc/transform-external-resources/#when-and-where-to-load-the-additional-resources","title":"When and Where to Load the Additional Resources","text":"<p>With a DataAccessFactory established, it can be used in either the transform's Runtime class when running in Ray, or in the transform's initializer to load the resource(s). These two approaches have the following considerations:</p> <ul> <li>Loading in transform itself: <ul> <li>Advantages<ul> <li>enables debugging without the need for a remote debugger to attach to the Ray worker.</li> <li>simplifies local testing, especially if a transform itself can be tested locally.</li> <li>can load any resource regardless of its picklability (irrelevant for data, but relevant for models).</li> </ul> </li> <li>Disadvantages<ul> <li>can create additional load on external resources, for example S3 or external web site.</li> </ul> </li> </ul> </li> <li>Loading in the Ray runtime, storing it plasma (Ray object storage), delivering it to the  transform via pointer:<ul> <li>Advantages<ul> <li>minimises load on external resources, for example S3 or external web site</li> </ul> </li> <li>Disadvantages<ul> <li>can be problematic if the resource/model is not picklable</li> <li>makes it slightly more complex for testing as loading is done in a process separate from the launcher</li> </ul> </li> </ul> </li> </ul> <p>With the above in mind, we recommend at least loading the resource(s) in the transform's initializer. This will ease debugging.  If load is an issue and the resource is picklable, then ALSO implement loading in the transform's Runtime. Next we show how to load resources using both approaches.</p>"},{"location":"data-processing-lib/doc/transform-external-resources/#loading-in-the-transform-initializer","title":"Loading in the Transform Initializer","text":"<p>If you decide to implement resource loading in the transform itself, you can do this in the init method of the transform class.  Let's look at the implementation, based on block listing) example. The code below demonstrates loading of data.</p> <p><pre><code>    # Get the DataAccessFactory we created above in the configuration\n    daf = config.get(block_data_factory_key)\n    if daf is None:\n        raise RuntimeError(f\"Missing configuration value for key {block_data_factory_key}\")\n    data_access = daf.create_data_access()\n    url = config.get(blocked_domain_list_path_key)\n    if url is None:\n        raise RuntimeError(f\"Missing configuration value for key {blocked_domain_list_path_key}\")\n    domain_list = get_domain_list(url, data_access)\n</code></pre> Note that alternatively,  if you are downloading data/models from the same source as the data itself, you can use <code>data_access</code> key to get the DataAccess object used to read/write the data.</p>"},{"location":"data-processing-lib/doc/transform-external-resources/#loading-in-the-transform-runtime","title":"Loading in the Transform Runtime","text":"<p>If you decide to implement resource loading in the transform runtime,  you must implement a custom transform runtime class, in particular, to  implement the <code>get_transform_config()</code> method that produces the configuration for the transform an in this example, load the domain list and store a Ray reference in the configuration.  Let's look at the implementation, based on the block listing) transform.  First define the initializer() which must accept a dictionary of parameters as generally will be defined by the configuration and its CLI parameters.</p> <p><pre><code>class BlockListRuntime(DefaultTableTransformRuntime):\n\n    def __init__(self, params: dict[str, Any]):\n        super().__init__(params)\n</code></pre> Once this is done, you need to at least implement <code>get_transform_config</code> method, which is called by the Ray orchestrator to establish the transform configuration parameters passed to the Ray Worker that then creates the transform instance using the provided  configuation parameters.  In short, these are the parameters that will be used to configure your transform in the Ray worker.</p> <p><pre><code>    def get_transform_config(\n        self, data_access_factory: DataAccessFactory, statistics: ActorHandle, files: list[str]\n    ) -&gt; dict[str, Any]:\n        # create the list of blocked domains by reading the files at the conf_url location\n        url = self.params.get(blocked_domain_list_path_key, None)\n        if url is None:\n            raise RuntimeError(f\"Missing configuration key {blocked_domain_list_path_key}\")\n        blocklist_data_access_factory = self.params.get(block_data_factory_key, None)\n        if blocklist_data_access_factory is None:\n            raise RuntimeError(f\"Missing configuration key {block_data_factory_key}\")\n        # Load domain list \n        domain_list = get_domain_list(url, blocklist_data_access_factory.create_data_access())\n        # Store it in Ray object storage\n        domain_refs = ray.put(domain_list)\n        # Put the reference in the configuration that the transform initializer will use.\n        return {domain_refs_key: domain_refs} | self.params\n</code></pre> In the implementation above, we do something very similiar to what was done in the transform initializer, except that here we store the loaded resource (i.e. the domain list) in Ray global memory and place the key for the stored object in the configuration.  This way the transform initializer can first look for this key and if found, avoid loading the domain list itself.</p> <p>Alternatively, if you are downloading resources from the same source as  the data itself, you can use input parameter <code>data_access_factory</code> to create data access  and download everything that you need using it.</p> <p>Finally, and as mentioned above, the transform's initializer looks to see if the key to the domain list is present and uses it instead of loading the domain list itself.</p> <pre><code>        runtime_provided_domain_refs = config.get(domain_refs_key, None)\n        if runtime_provided_domain_refs is None:\n            # this is useful during local debugging and testing without Ray\n            url = config.get(blocked_domain_list_path_key, None)\n            if url is None:\n                raise RuntimeError(f\"Missing configuration value for key {annotation_column_name_key}\")\n            daf = config.get(block_data_factory_key, None)\n            if url is None:\n                raise RuntimeError(f\"Missing configuration value for key {block_data_factory_key}\")\n            data_access = daf.create_data_access()\n            domain_list = get_domain_list(url, data_access)\n        else:\n            # This is recommended for production approach. In this case domain list is build by the\n            # runtime once, loaded to the object store and can be accessed by actors without additional reads\n            try:\n                domain_list = ray.get(runtime_provided_domain_refs)\n            except Exception as e:\n                logger.info(f\"Exception loading list of block listed domains from ray object storage {e}\")\n                raise RuntimeError(f\"exception loading from object storage for key {runtime_provided_domain_refs}\")\n</code></pre>"},{"location":"data-processing-lib/doc/transform-external-resources/#conclusion","title":"Conclusion","text":"<p>Generally, although both resource loading approaches can be used, we recommend  always implementing loading in the transform initializer, and if desired, loading in the transform  runtime if feasible (picklable, etc) or desirable for other reasons such as network bandwidth.</p>"},{"location":"data-processing-lib/doc/transform-runtimes/","title":"Transform Runtimes","text":"<p>Runtimes provide a mechanism to run a transform over a set of input files to produce a set of output files.  Each runtime is started using a launcher. The available runtimes are as follows: </p> <ul> <li>Python runtime - provides single process running of a transform.</li> <li>Ray runtime - provides running transforms across multiple Ray workers to   achieve highly scalable processing.</li> <li>Spark runtime - provides running spark-based transforms in a spark cluster.</li> </ul>"},{"location":"data-processing-lib/doc/transform-s3-testing/","title":"Testing transforms with S3","text":"<p>For testing transforms with S3 we are using Minio, which can be installed on Linux, macOS and Windows. Here we are assuming Mac usage, refer to documentation above for other platforms.</p>"},{"location":"data-processing-lib/doc/transform-s3-testing/#installing-minio","title":"Installing Minio","text":"<p>The simplest way to install Minio on Mac is using Homebrew. Use the following command:</p> <pre><code>brew install minio/stable/minio\n</code></pre> <p>In addition to the Minio server install the latest stable MinIO cli using</p> <p><pre><code>brew install minio/stable/mc\n</code></pre> Now you can start Minio server using the following command:</p> <pre><code>minio server start\n</code></pre> <p>When it starts you can connect to the server UI using the following address: <code>http://localhost:9000</code> The default user name/password is <code>minioadmin|minioadmin</code></p>"},{"location":"data-processing-lib/doc/transform-s3-testing/#populating-minio-with-testing-data","title":"Populating Minio with testing data","text":"<p>Populating Minio server with test data can be done using <code>mc</code>. First configure mc to work with the local Minio server:</p> <pre><code>mc alias set local http://127.0.0.1:9000 minioadmin minioadmin\n</code></pre> <p>This set an alias <code>local</code> to 'mc' connected to the local Minio server instance. Now we can use our mc instance to populate server using a set of commands provided by <code>mc</code>.</p> <p>First test the connection to the newly added MinIO deployment using the <code>mc admin info</code> command:</p> <pre><code>mc admin info local\n</code></pre> <p>To copy the data to Minio, you first need to create a bucket:</p> <pre><code>mc mb local/test\n</code></pre> <p>Once the bucket is created, you can copy files (assuming you are in the transforms directory), using:</p> <pre><code>mc cp --recursive tools/ingest2parquet/test-data/input/ local/test/ingest2parquet/input\nmc cp --recursive code/code_quality/test-data/input/ local/test/code_quality/input\nmc cp --recursive code/proglang_select/test-data/input/ local/test/proglang_select/input\nmc cp --recursive code/proglang_select/test-data/languages/ local/test/proglang_select/languages\nmc cp --recursive code/malware/test-data/input/ local/test/malware/input\n\nmc cp --recursive language/doc_quality/test-data/input/ local/test/doc_quality/input\nmc cp --recursive language/lang_id/ray/test-data/input/ local/test/lang_id/input\n\nmc cp --recursive universal/blocklist/test-data/input/ local/test/blocklist/input\nmc cp --recursive universal/blocklist/test-data/domains/ local/test/blocklist/domains\nmc cp --recursive universal/doc_id/test-data/input/ local/test/doc_id/input\nmc cp --recursive universal/ededup/test-data/input/ local/test/ededup/input\nmc cp --recursive universal/fdedup/test-data/input/ local/test/fdedup/input\nmc cp --recursive universal/filter/test-data/input/ local/test/filter/input\nmc cp --recursive universal/noop/test-data/input/ local/test/noop/input\nmc cp --recursive universal/resize/test-data/input/ local/test/resize/input\nmc cp --recursive universal/tokenization/test-data/ds01/input/ local/test/tokenization/ds01/input\nmc cp --recursive universal/tokenization/test-data/ds02/input/ local/test/tokenization/ds02/input\n</code></pre> <p>Note, that once the data is copied, Minio is storing it on the local file system, so you do not need to copy it again after cluster restart</p>"},{"location":"data-processing-lib/doc/transform-s3-testing/#creating-access-and-secret-key-for-minio-access","title":"Creating access and secret key for Minio access","text":"<p>The last thing is to add Minio access and secret keys for accessing it. The following command:</p> <pre><code>mc admin user svcacct add --access-key \"localminioaccesskey\" --secret-key \"localminiosecretkey\" local minioadmin\n</code></pre> <p>creates both access and secret key for usage by the applications</p>"},{"location":"data-processing-lib/doc/transform-standalone-testing/","title":"Testing Transforms","text":"<p>A test framework is provided as part of the library in the <code>data_processing_test</code> package. Transform testing makes use of an <code>AbstractTransformTest</code> super-class that defines  the generic tests that should be common to all transforms. Initially this means testing the transformation of one or more in-memory  input tables to a one or more output tables and one or more metadata dictionaries.</p> <p>The <code>AbstractTransformTest</code> class defines the <code>test_*(...)</code> methods and makes use  of pytest fixtures to define test cases (i.e. inputs) applied to the test method(s). Each <code>test_*(...)</code> method (currently only one) has an associated abstract <code>get_test_*_fixtures()</code> method that must be implemented by the specific transform implementation test to define the various sets of inputs tested. This approach allows the definition of new generic transform tests that existing transform implementation tests will easily leverage.</p> <p>The first (currently only test) is a the <code>test_transform()</code> method that takes the following inputs:</p> <ul> <li>the transform implementation being tested, properly configured with the configuration dictionary for the associated test data.</li> <li>a list of N (1 or more) input tables to be processed with the transform's <code>transform(Table)</code> method.</li> <li>The expected list of accumulated tables across the N calls to  <code>transform(Table)</code> and the single finalizing call to the transform's <code>flush()</code> method. In the case where the <code>transform()</code> returns an empty list, no associated expected Table  should be included in this list. </li> <li>The expected list of accumulated metadata dictionaries across the N calls to <code>transform(Table)</code>   and the single finalizing call to the transform's <code>flush()</code> method.  This list should be of length N+1 for the N calls to <code>transform(Table)</code> plus the finalizing call to <code>flush()</code>.</li> </ul> <p>As an example, consider the <code>NOOPTransformTest</code> developed as ane example of the testing framework.</p> <p><pre><code>from typing import Tuple\n\nimport pyarrow as pa\nfrom data_processing.test_support import AbstractTransformTest\nfrom noop_transform import NOOPTransform\n\n# Define the test input and expected outputs\ntable = pa.Table.from_pydict({\"name\": pa.array([\"Tom\"]), \"age\": pa.array([23])})\nexpected_table = table  # We're a noop after all.\nexpected_metadata_list = [{\"nfiles\": 1, \"nrows\": 1}, {}]  # transform() result  # flush() result\n\n\nclass TestNOOPTransform(AbstractTransformTest):\n\n  # Define the method that provides the test fixtures to the test from the super class.\n  def get_test_transform_fixtures(self) -&gt; list[Tuple]:\n    fixtures = [\n      (NOOPTransform({\"sleep\": 0}), [table], [expected_table], expected_metadata_list),\n      (NOOPTransform({\"sleep\": 1}), [table], [expected_table], expected_metadata_list),\n    ]\n    return fixtures\n</code></pre> In the above, we use the <code>NOOPTransform</code> to process the single input <code>table</code>, to produce the expected table <code>expected_table</code> and list of metadata in <code>expected_metadata_list</code>.  The <code>NOOPTransform</code> has no configuration that effects the transformation of input to output. However, in general this will not be the case and a transform may have different configurations and associated test data.  For example, a transform might be configurable to use different models and perhaps as a result have different results. </p> <p>Once the test class is defined you may run the test from your IDE or from the command line:  <pre><code>% cd .../data-prep-kit/transforms/universal/noop\n% make venv\n% source venv/bin/activate\n(venv)% export PYTHONPATH=.\n(venv)% pytest test/test_noop.py \n================================================================================ test session starts ================================================================================\nplatform darwin -- Python 3.10.11, pytest-8.0.2, pluggy-1.4.0\nrootdir: /Users/dawood/git/data-prep-kit/transforms/universal/noop\nplugins: cov-4.1.0\ncollected 2 items                                                                                                                                                                \n\ntest/test_noop.py ..                                                                                                                                                          [100%]\n\n================================================================================= 2 passed in 0.83s =================================================================================\n(venv) % \n</code></pre> Finally, the tests should be runnable with the <code>Makefile</code>  as follows: <pre><code>$ make test\n...\n</code></pre> Note that the venv was created and activated for you as part of running the test.</p>"},{"location":"data-processing-lib/doc/transform-testing/","title":"Transform Testing","text":"<p>Once a transform has been built, testing can be enabled with the provided testing framework:</p> <ul> <li>Standalone Transform Testing - shows how to test a transform independent of the runtime.</li> <li>End-to-End Testing - shows how to test the transform running in a runtime.</li> <li>Testing Transforms with S3 -shows how to set up <code>minio</code> to test transforms with the S3 interfaces.</li> </ul>"},{"location":"data-processing-lib/doc/transform-tutorial-examples/","title":"Tutorial Example","text":"<p>We use a tutorial to show how to develop a new transform. Before getting started, you may want to consult the  transform project root readme documentation.</p> <ul> <li>Tutorial -  Here we will take a simple example to show the basics of creating a simple transform that takes a single input Table, and produces a single Table.</li> <li>External resources transform - shows how to load additional resources (models, configuration, etc) for a transform.</li> </ul>"},{"location":"data-processing-lib/doc/transformer-utilities/","title":"Transform Utilities","text":"<p>A class TransformUtils provides several methods that simplify  transformer's implementation. Currently, it includes the following methods:</p> <ul> <li><code>deep_get_size</code> is the method to get the complete size of the Python object based on   https://www.askpython.com/python/built-in-methods/variables-memory-size-in-python   It supports Python structures: list, tuple and set</li> <li><code>normalize_string</code> normalizes string, converting it to lowercase and removing spaces, punctuation and CR</li> <li><code>str_to_hash</code>convert string to 259 bit hash</li> <li><code>str_to_int</code> getting an integer representing string by calculating string's hash</li> <li><code>validate_columns</code> check whether required columns exist in the table</li> <li><code>add_column</code> adds column to the table avoiding duplicates. If the column with the given name already exists it will  be removed before it is added</li> <li><code>validate_path</code> cleans up s3 path - Removes white spaces from the input/output paths   removes schema prefix (s3://, http:// https://), if exists   adds the \"/\" character at the end, if it doesn't exist   removes URL encoding</li> </ul> <p>It also contains two variables:</p> <ul> <li><code>RANDOM_SEED</code> number that is used for methods that require seed</li> <li><code>LOCAL_TO_DISK</code> rough local size to size on disk/S3</li> </ul> <p>This class should be extended with additional methods, generally useful across multiple transformers and documentation  should be added here </p>"},{"location":"data-processing-lib/doc/transforms/","title":"Transforms","text":"<p>Transform is a basic integration unit of DPK that can be executed in any of the supported by the DPK  runtimes (Python, Ray and Spark). All transforms  are derived from the  AbstractTransform class. Theis class provides no functionality and is used as just a marker that a given class implements transform. There are currently two types of transforms defined in DPK:</p> <ul> <li>AbstractBinaryTransform which is a base  class for all data transforms. Data transforms convert a file of data producing zero or more data files  and metadata. A specific class of the binary transform is  AbstractTableTransform that consumes and produces data files containing pyarrow tables</li> <li>AbstractFolderTransform which is a base class consuming a folder (that can contain an arbitrary set of files, that need to be processed together) and proces zero or more data files and metadata.</li> </ul> <p>In the discussion that follows, we'll focus on the transformation of pyarrow Tables using the <code>AbstractTableTransform</code> class (see below), supported by Ray Spark and Python runtimes.</p>"},{"location":"data-processing-lib/doc/transforms/#abstracttabletransform-class","title":"AbstractTableTransform class","text":"<p>AbstractTableTransform  is expected to be extended when implementing a transform of pyarrow Tables. In general, when possible a transform should be independent of the runtime in which it runs, and the mechanism used to define its configuration (e.g., the <code>TransformConfiguration</code> class below, or other mechanism). That said, some transforms may require facilities provided by the runtime (shared memory, distribution, etc.), but as a starting point, think of the transform as an independent operator.</p> <p>The following methods are defined:</p> <ul> <li><code>__init__(self, config:dict)</code> - an initializer through which the transform can be created  with implementation-specific configuration.  For example, the location of a model, maximum number of rows in a table, column(s) to use, etc.   Error checking of configuration should be done here.</li> <li><code>transform(self, table:pyarrow.Table) -&gt; tuple(list[pyarrow.Table], dict)</code> - this method is responsible for the actual transformation of a given table to zero or more output tables, and optional  metadata regarding the transformation applied.  Zero tables might be returned when merging tables across calls to <code>transform()</code> and more than 1 table might be returned when splitting tables by size or other criteria.</li> <li>output tables list - the RayWork handles the various number of returned tables as follows: <ul> <li>0 - no file will be written out and the input file name will not be used in the output directory.</li> <li>1 - one parquet file will be written to the output directory with </li> <li>N - N parquet files are written to the output with <code>_&lt;index&gt;</code> appended to the base file name</li> </ul> </li> <li>dict - is a dictionary of transform-specific data keyed to numeric values.  A statistics component will          accumulate/add dictionaries across all calls to transform across all calls to all transforms running          in a given runtime (see below). As an example, a          transform might wish to track the number of instances of PII entities detected and might return           this as <code>{ \"entities\" : 1234 }</code>.</li> <li><code>flush() -&gt; tuple(list[pyarrow.Table], dict)</code> - this is provided for transforms that make use of buffering (e.g. to resize the tables) across calls  to <code>transform()</code> and need to be flushed of all buffered data at the end of processing of input tables. The return values are handled the same waa as the return values for <code>transform()</code>.  Since most transforms will likely not need this feature, a default implementation is provided to return an empty list and empty dictionary.</li> </ul>"},{"location":"data-processing-lib/doc/transforms/#transformconfiguration-class","title":"TransformConfiguration class","text":"<p>The TransformConfiguration serves as an interface and must be implemented by the any <code>AbstractTableTransform</code> implementation to enable running within and runtime or from a command line to capture  transform configuration.  It provides the following configuration:</p> <ul> <li>the transform class to be used,</li> <li>command line arguments used to initialize the Transform Runtime and generally, the Transform.</li> <li>Transform Runtime class to use</li> <li>transform short name </li> </ul> <p>It is expected that transforms are initialized with a fixed name, the class of its corresponding <code>AbstractTableTransform</code> implementation and optionally the configuration keys that should not be exposed as metadata for a run. To support command line configuration, the <code>TransformConfiguration</code> extends the CLIArgumentProvider class. The set of methods of interest are</p> <ul> <li><code>__init__(self, name:str, transform_class:type[AbstractTableTransform], list[str]:remove_from_metadata )</code> - sets the required fields</li> <li><code>add_input_params(self, parser:ArgumentParser)</code> - adds transform-specific command line options that will be made available in the dictionary provided to the transform's initializer.</li> <li><code>apply_input_params(self, args: argparse.Namespace)</code> - verifies  and captures the relevant transform parameters.</li> <li><code>get_input_params(self ) -&gt; dict[str,Anny]</code> - returns the dictionary of configuration values that should be used to initialize the transform.</li> </ul>"},{"location":"data-processing-lib/python/","title":"Data Processing Library","text":"<p>This provides a python framework for developing transforms on data stored in files - currently parquet files are supported - and running them in a ray cluster. Data files may be stored in the local file system or  COS/S3. For more details see the documentation.</p>"},{"location":"data-processing-lib/python/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>make venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-processing-lib/python/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library  <pre><code>make test build publish\n</code></pre></p> <p>To up the version number, edit the Makefile to change VERSION and rerun the above.  This will require committing both the <code>Makefile</code> and the autotmatically updated <code>pyproject.toml</code> file.</p>"},{"location":"data-processing-lib/ray/","title":"Data Processing Library","text":"<p>This provides a python framework for developing transforms on data stored in files - currently parquet files are supported - and running them in a ray cluster. Data files may be stored in the local file system or  COS/S3. For more details see the documentation.</p>"},{"location":"data-processing-lib/ray/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>cd data-processing-lib/ray/\nmake venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-processing-lib/ray/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library  <pre><code>make test build publish\n</code></pre> To up the version number, edit the Makefile to change VERSION and rerun the above.  This will require committing both the <code>Makefile</code> and the autotmatically updated <code>pyproject.toml</code> file.</p>"},{"location":"data-processing-lib/spark/","title":"Spark Data Processing Library","text":"<p>This provides a python framework for developing transforms on data stored in files - currently any binary and parquet files are supported - and running them in a Spark cluster. Data files may be stored in the local file system or COS/S3. For more details see the documentation.</p>"},{"location":"data-processing-lib/spark/#virtual-environment","title":"Virtual Environment","text":"<p>The project uses <code>pyproject.toml</code> and a Makefile for operations. To do development you should establish the virtual environment <pre><code>make venv\n</code></pre> and then either activate <pre><code>source venv/bin/activate\n</code></pre> or set up your IDE to use the venv directory when developing in this project</p>"},{"location":"data-processing-lib/spark/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>To test, build and publish the library  <pre><code>make test build publish\n</code></pre> To up the version number, edit the Makefile to change VERSION and rerun the above.  This will require committing both the <code>Makefile</code> and the automatically updated <code>pyproject.toml</code> file.</p>"},{"location":"doc/data-processing/","title":"Data Processing Using Multiple Transforms","text":"<p>The transformation framework is designed to operate on arbitrary arrays of bytes with a specialization for columnar data, generally contained in parquet files and read as pyarrow tables.</p> <p>In general, transforms can be written to process any type of binary data, to be interpreted by the transform.</p> <p>The table-specific transforms are written to process a table to, for example:</p> <ul> <li>Remove duplicates or non supported documents, for example, exact and fuzzy dedup, or language selection</li> <li>Transform files, for example coalesce or split files or add/remove some of the columns</li> <li>Annotate the tables to add additional data such as document quality score, language, etc.</li> <li>Derive statistics on data in one or more columns </li> <li>Filter the table to remove or edit rows and/or columns, for example to remove rows from blocked domains.</li> </ul> <p>The table is generally expected to have something like the following minimal set of columns :</p> <ul> <li>URL source of the document (can be use for domain block listing)</li> <li>Document id</li> <li>Contents of the actual document to be used for LLM training</li> </ul> <p>The following might be an example sequence of transforms (applied to tables, though the model is equivalent for binary data transformations)</p> <p></p> <p>The ordering of the transforms can change depending on the requirements on the training data. </p> <p>Each transform is generally run on a set of input files to produce a set of output files. The transforms are generally sequenced/stitched together,  each accepting the completed set of output files from a previous transform in the sequence. </p> <p></p> <p>Transforms are generally executed in one of the available  \"runtimes\" with scalability provided by the Ray and Spark runtimes.</p>"},{"location":"doc/google-colab/","title":"Running on Google Colab","text":"<p>Google Colab is free hosted Jupyter environment.</p> <p>Here are some tips for running data prep kit application code on Google Colab.</p>"},{"location":"doc/google-colab/#how-to-determine-if-we-are-running-on-colab","title":"How to Determine if We are Running on Colab?","text":"<p>Use one of these code snippets</p> <pre><code>import os\n\nif os.getenv(\"COLAB_RELEASE_TAG\"):\n   print(\"Running in Colab\")\n   RUNNING_IN_COLAB = True\nelse:\n   print(\"NOT in Colab\")\n   RUNNING_IN_COLAB = False\n</code></pre>"},{"location":"doc/google-colab/#installing-dependencies","title":"Installing Dependencies","text":"<p>We need to install data prep kit and libraries in Colab</p> <pre><code>if RUNNING_IN_COLAB:\n    ! pip install  --default-timeout=100  data-prep-toolkit-transforms-ray==0.2.1.dev3\n</code></pre>"},{"location":"doc/google-colab/#downloading-data-files-on-colab","title":"Downloading Data Files on Colab","text":"<pre><code>if RUNNING_IN_COLAB:\n    !mkdir -p 'input'\n    !wget -O 'input/1.pdf'  'remote_file_url'\n</code></pre>"},{"location":"doc/google-colab/#ray-runtime-settings","title":"Ray Runtime Settings","text":"<p>These are some recommended settings for running RAY based notebooks.  You can use these as starting points and tweak for your application</p> <pre><code>if RUNNING_IN_COLAB:\n  RAY_RUNTIME_WORKERS = 2\n  RAY_NUM_CPUS =  0.3\n  RAY_MEMORY_GB = 2  # GB\n</code></pre> <p>It is recommended to set cpu per worker (RAY_NUM_CPUS) to a low number.  Otherwise Ray jobs seem to hang and will not complete.</p>"},{"location":"doc/google-colab/#fuzzy-dedupe-settings","title":"Fuzzy Dedupe Settings","text":"<p>Start with the following settings before launching fuzzy dedupe job.</p> <p>Here is the infrastructure section for fuzzy dedupe.  Again we recommend to keep CPU share low.</p> <pre><code>    # infrastructure\n    \"fdedup_bucket_cpu\": 0.3,\n    \"fdedup_doc_cpu\": 0.3,\n    \"fdedup_mhash_cpu\": 0.3,\n    \"fdedup_num_doc_actors\": 1,\n    \"fdedup_num_bucket_actors\": 1,\n    \"fdedup_num_minhash_actors\": 1,\n    \"fdedup_num_preprocessors\": 1,\n</code></pre> <p>Here is full code for completeness</p> <pre><code>local_conf = {\n    \"input_folder\": input_folder,\n    \"output_folder\": output_folder,\n}\nworker_options = {\"num_cpus\" : RAY_NUM_CPUS}\n\nparams = {\n    # where to run\n    \"run_locally\": True,\n    # Data access. Only required parameters are specified\n    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n    # Orchestration parameters\n    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n    \"runtime_num_workers\": RAY_RUNTIME_WORKERS,\n    # columns used\n    \"fdedup_doc_column\": \"contents\",\n    \"fdedup_id_column\": \"int_id_column\",\n    \"fdedup_cluster_column\": \"hash_column\",\n    # infrastructure\n    \"fdedup_bucket_cpu\": 0.3,\n    \"fdedup_doc_cpu\": 0.3,\n    \"fdedup_mhash_cpu\": 0.3,\n    \"fdedup_num_doc_actors\": 1,\n    \"fdedup_num_bucket_actors\": 1,\n    \"fdedup_num_minhash_actors\": 1,\n    \"fdedup_num_preprocessors\": 1,\n    # fuzzy parameters\n    \"fdedup_num_permutations\": 64,\n    \"fdedup_threshold\": 0.7,\n    \"fdedup_shingles_size\": 5,\n    \"fdedup_delimiters\": \" \"\n}\n\n# Pass commandline params\nsys.argv = ParamsUtils.dict_to_req(d=params)\n\nlauncher = RayTransformLauncher(FdedupRayTransformConfiguration())\nreturn_code = launcher.launch()\n</code></pre>"},{"location":"doc/mac/","title":"Mac","text":""},{"location":"doc/mac/#execution-on-laptops-with-apple-silicon-cpu","title":"\u2757 Execution on laptops with Apple Silicon (CPU)","text":"<p>Starting with certain models introduced in late 2020, Apple began the transition from Intel processors to Apple silicon in Mac computers. These CPUs have ARM architecture and are incompatible with Intel processors. </p>"},{"location":"doc/mac/#transforms","title":"Transforms","text":"<p>Developing transforms for either the python or Ray runtimes, without KubeFlow pipelines (KFP), should have no issues on Apple silicon Macs, or other platforms for that matter. Therefore, to the extent the supported versions of python are used, transforms can be developed that will run on Apple silicon Macs. </p>"},{"location":"doc/mac/#virtualization-considerations","title":"Virtualization Considerations","text":"<p>Desktops such as Docker Desktop, Podman desktop and Rancher desktop use different virtualization and emulation techniques, (qemu, Apple Virtualization framework) to allow the execution of containers based on images compiled for Intel silicon. However, emulation significantly impacts performance, and there are additional restrictions, such as Virtual Machine RAM size.</p> <p>On the other hand, executing a Kind Kubernetes cluster with KubeFlow pipelines (KFP) and local data storage (Minio) requires a significant amount of memory. For this initial Data Prep Kit release, we do not recommend local (Kind) execution on Mac computers with Apple silicon. Instead, we suggest using a real Kubernetes cluster or a Linux virtual machine with an Intel CPU.</p>"},{"location":"doc/mac/#memory-considerations","title":"Memory Considerations","text":"<p>To verify that running transforms through KFP does not leak memory and also get an idea on the required Podman VM memory size configuration, a few tests were devised and run, as summarized below:</p>"},{"location":"doc/mac/#memory-and-endurance-considerations","title":"Memory and Endurance Considerations","text":"<p>A test was devised with a set of 1483 files on a Mac with 32GB memory and 4CPU cores. Traceback library was used to check for memory leak.  10 iterations were run and the memory usage was observed, which peaked around 4 GB. There were no obvious signs of a memory leak. </p> <p>Another set of tests was done with the 1483 files on a podman VM with different memory configurations. The results are shown below. It seems that it needed around 4GB of available memory to run successfully for all 1483 files.</p> CPU Cores Total Memory Memory Used by Ray Transform Files Processed Successfully 4 8GB 4.2GB NOOP 1483 4 6GB 3GB NOOP 910 4 4GB 2GB NOOP 504 <p>Note: the current release does not support building cross-platform images, therefore, please do not build images  on the Apple silicon. </p>"},{"location":"doc/repo/","title":"Repository Structure and Use","text":"<p>Here we discuss the structure, use and approach to code management in the repo.</p>"},{"location":"doc/repo/#setup","title":"Setup","text":"<p>There are various entry points that you can choose based on the use case. Each entry point has its pre-requirements and setup steps. The common part of are:</p>"},{"location":"doc/repo/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or 3.11  -Docker/Podman</li> </ul> <p>Two important development tools will also be installed using the steps below: - pre-commit - twine </p>"},{"location":"doc/repo/#installation-steps","title":"Installation Steps","text":"<p><pre><code>pip install pre-commit\npip install twine\n...\ngit clone git@github.com:IBM/data-prep-kit.git\ncd data-prep-kit\npre-commit install\n</code></pre> Please note that there are further installation steps  for running the transforms in general, as documented  here  and on a local Kind cluster or on an existing Kubernetes  cluster, as documented here.</p>"},{"location":"doc/repo/#repository-structure","title":"Repository structure","text":"<ul> <li> <p>data_processing_lib - provides the core transform framework and library  supporting data transformations in 3 runtimes</p> <ul> <li>python </li> <li>ray</li> <li>spark</li> </ul> </li> <li> <p>transform</p> <ul> <li>universal<ul> <li>noop </li> <li>...</li> </ul> </li> <li>code<ul> <li>code_quality</li> <li>...</li> </ul> </li> <li>language<ul> <li>...</li> </ul> </li> </ul> </li> <li>kfp - Kubeflow pipeline support<ul> <li>kfp_support_lib - Data Preparation Kit Library. KFP support</li> <li>kfp_ray_components - Kubflow pipeline components used in the pipelines</li> </ul> </li> <li>scripts</li> </ul>"},{"location":"doc/repo/#build-and-makefiles","title":"Build and Makefiles","text":"<p>Makefiles are used for operations performed across all projects in the directory tree. There are two types of users envisioned to use the make files.  </p> <ul> <li>adminstrators - perform git actions and release management </li> <li>developers - work with core libraries and transforms</li> </ul> <p>Each directory has access to a <code>make help</code> target that will show all available targets.</p>"},{"location":"doc/repo/#administrators","title":"Administrators","text":"<p>Generally, administrators will issue make commands from the top of the repository to, for example publish a new release.  The top level make file provides a set of targets that  are executed recursively, which as a result are expected to be implementd by sub-directories.  These and their semantics are expected to be implemented, as appropriate, in the sub-directories are as follows:</p> <ul> <li>clean - Restore the directory to as close to initial repository clone state as possible. </li> <li>build - Build all components contained in a given sub-directory. This might include pypi distributions, images, etc.</li> <li>test -  Test all components contained in a given sub-directory. </li> <li>publish - Publish any components in sub-directory.  This might include things published to pypi or the docker registry.</li> <li>set-versions - apply the DPK_VERSION to all published components. </li> </ul> <p>Sub-directories are free to define these as empty/no-op targets, but generally are required to define them unless a parent directory does not recurse into the directory.</p>"},{"location":"doc/repo/#build-and-deploy-a-dev-release-for-integration-testing-recommended-step-for-all-transforms-prior-to-merging-the-corresponding-pr","title":"Build and deploy a dev release for integration testing (Recommended step for all transforms prior to merging the corresponding PR)","text":"<ol> <li>Create your fork from the main repo or sync an existing fork with main repo</li> <li>Clone the fork     <pre><code>git clone git@github.com:&lt;USER&gt;/data-prep-kit.git data-prep-kit-dev\ncd data-prep-kit-dev\n</code></pre></li> <li>Create a new local branch from dev     <pre><code>git checkout dev\ngit checkout -b \"testing-$(date '+%Y-%m-%d')\"\n</code></pre></li> <li>Merge changes from remote branch (if more than one PR, repeat below for each PR). In the example below, replace '' and '' with the git url and branch from each PR (e.g, PR1, PR2, ...)     <code>shell      git remote add &lt;remote_name_PR1&gt; &lt;fork_url&gt;       git fetch &lt;remote_name_PR1&gt; &lt;branch_name&gt;      git merge &lt;remote_name_PR1&gt;/&lt;branch_name&gt;</code> <li>Change to the transforms folder, clean any previous build, build a new wheel and publish the wheel as a dev branch to pypi. Follow instructions to setup your environment to be able to publish:     <pre><code>cd transforms\nrm -fr build dist data_prep_toolkit_transforms.egg-info\nmake build-pkg-dist\npip install twine\nmake publish-dist\n</code></pre></li> <li> <p>Note- 'make publish-dist' will fail if a previous build with the same tag is already present on pypi. In this case, add a 'build tag' and publish again. The 'build tag' is a number that immediately follows the distribution package version seperated by a dash <code>({distribution}-{version}(-{build tag})?-{python tag}-{abi tag}-{platform tag}.whl)</code></p> <p><pre><code>mv dist/data_prep_toolkit_transforms-1.0.1.dev1-py3-none-any.whl dist/data_prep_toolkit_transforms-1.0.1.dev1-1-py3-none-any.whl\n</code></pre> <pre><code>make publish-dist\n</code></pre> Note-  'make publish-dist' will fail if the choosen 'build tag' already exists. In this case, consult the pypi site to identify the latest build tag previously used and increment by 1</p> </li> <li> <p>When testing the new wheel in a notebook or a venv, make sure to use the --no-cache option: <code>pip install --no-cache data-prep-toolkit-transforms-1.0.1.dev1</code></p> </li>"},{"location":"doc/repo/#developers","title":"Developers","text":"<p>Generally, developers will be working in a python project directory (e.g., data-processing-lib/python, transforms/universal/filter, etc.)  and can issue the administrator's make targets (e g., build, test, etc) or others that might be defined locally (e.g., venv, test-image, test-src in transform projects). Key targets are as follows:</p> <ul> <li>venv -  creates the virtual environment from either a pyproject.toml or requirements.txt file.</li> </ul> <p>If working with an IDE, one generally makes the venv, then configures the IDE to  reference the venv, src and test directories.</p> <p>Transform projects generally include these transform project-specific targets for convenience, which are triggered with the the <code>test</code> target.</p> <ul> <li>test-src - test python tests in the test directory</li> <li>test-image - build and test the docker image for the transform</li> </ul> <p>Please also consult transform project conventions for  additional considerations when developing transforms.</p>"},{"location":"doc/repo/#transforms-and-kfp","title":"Transforms and KFP","text":"<p>The kfp_ray directories in the transform projects provide  <code>workflow-</code> targets and are dedicated to handling the  Kubeflow Pipelines  workflows for the specified transforms.</p> <p>```</p>"},{"location":"doc/quick-start/contribute-your-own-transform/","title":"Developer Tutorial","text":"Data Prep Kit- Developer Tutorial  <p>In this tutorial, we take the developer through the steps for contributing a new transform to the DPK. We will cover: 1. How to clone the repo and set up the file structure for the transform 1. Write the code by implementing the transform specific functionality  1. Use the framework capabilities to accelerate development, testing and deployment</p> <p>For this tutorial, we will follow a suggested flow. Developers are welcome to explore on their own to achieve the same results. Except for the transform name and module name, developers have a lot of freedom in how they choose their class name, file names and file structure. That said, following the convention proposed in this document would make it easier for the community to chime in to help with debugging and maintaining the code base.</p> <p>The new transform we will build  as part of this tutorial is meant to annotate each document in the data set with a digest value that is calculated using a SHA256, SHA512 or MD5 hash function. The objective is to show how we can use a user defined function to build a transform and how developers can specify the configuration parameters for the transform and how we integrate the transform with the python and/or ray orchestrators.</p> <ul> <li> <p>For the purpose of this tutorial, our transform will be called digest and the python named module is dpk_digest. Developers have some freedom in how they name their modules to the extent that the chosen name does not conflict with an existing transform name.</p> </li> <li> <p>The user defined function for this tutorial is a simple annotator that uses the python hashlib package:   <code>hash = hashlib.new(self.algorithm)       digest=hash.update(elt.encode('utf-8'))</code></p> </li> <li>The transform defines a single configuration parameter digest_algortihm that specifies the name of the digest algorithm to use and selected from a predefined list that includes ['SHA256', 'SHA512', or 'MD5']</li> </ul>"},{"location":"doc/quick-start/contribute-your-own-transform/#list-of-steps-to-follow-in-this-part-of-the-tutorial","title":"List of Steps to follow in this part of the tutorial","text":"<ol> <li>Create a folder structure - clone git repo and create file structure for new transform</li> <li>Implement AbstractTableTransform - core functionality for annotating documents</li> <li>Implement TransformConfiguration - configure and validate transform parameters</li> <li>Implement PythonTransformRuntimeConfiguration - wire the transform to the runtime so it is correctly invoked</li> <li>Implement RayTransformRuntimeConfiguration - extend the transform to scale up using ray</li> <li>Develop Unit Test - get test data and write Unit Test</li> <li>Integrate with CI/CD - automate testing, integration and packaging</li> <li>Create notebook - jupyter notebook showing how the transform can be invoked</li> <li>Create Readme file - Readme file explaining how the transform is used</li> <li>(Optional)Setup for KFP Pipeline - Create artifacts for integrating with KFP Workflow</li> </ol>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-1-create-a-folder-structure","title":"Step 1: Create a folder structure","text":"<p>fork and clone the repo locally</p> <p>This can be done from the github web browser or using the gh CLI as below:</p> <p>Example using CLI (https://github.com/cli/cli)</p> <pre><code>gh auth login --skip-ssh-key\ngh repo fork git@github.com:IBM/data-prep-kit.git --default-branch-only \n</code></pre> <p>ASSUMPTION: We assume that the developer had already installed git cli and setup her/his public key as part of the developer\u2019s profile.  https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account</p> <p>Create a placeholder for the new transform</p> <p>The DPK transforms are currently organized in three categories for Code (Transforms that are used specifically for programming languages),  Language(Transforms that are used specifically for natural languages) and Universal (Transforms that are used for both language and code). It is safe to assume that our transform can be used for calculating the hash for natural languages text and programming languages alike and we will add it to the universal subfolder. We will also create the python module and a skeleton of the code including a notebook and readme.md file. A typical implementation would have the following file structure.</p> <pre><code>data-prep-kit\n\u2502\n\u2514\u2500\u2500\u2500transforms\n\u2502   |\n\u2502   \u2514\u2500\u2500\u2500universal\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500\u2500digest\n\u2502            |\n\u2502            \u2514\u2500\u2500\u2500dpk_digest\n\u2502            |      \u2502\n\u2502            |      \u2502 __init__.py\n\u2502            |      \u2502 transform.py\n\u2502            |      | runtime.py\n\u2502            \u2502      |\n\u2502            |      \u2514\u2500\u2500\u2500ray\n\u2502            |           \u2502 __init__.py\n\u2502            |           | runtime.py\n\u2502            |           \u2502 \n\u2502            \u2514\u2500\u2500\u2500test\n\u2502            \u2502    |\n\u2502            \u2502    |test_digest_ray.py\n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500test-data\n\u2502            \u2502     |\n\u2502            |     \u2514\u2500\u2500\u2500input\n\u2502            |     |     \u2502\n\u2502            |     |     \u2502 testfile.parquet\n\u2502            |     |     \n\u2502            |     \u2514\u2500\u2500\u2500expected\n\u2502            |          \u2502\n\u2502            |          \u2502 testfile.parquet\n\u2502            |          \u2502 metadata.json\n\u2502            | \n\u2502            | requirements.txt\n\u2502            | Dockerfile.python\n\u2502            | Dockerfile.ray\n\u2502            | digest.ipynb\n\u2502            | README.md\n\u2502            | Makefile\n</code></pre> <p>although our transform does not require additional packages, we need to create an empty requirements.txt file</p> <pre><code>cd data-prep-kit/transforms/universal/digest\ntouch requirements.txt\n</code></pre>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-2-implement-abstracttabletransform","title":"Step 2: Implement AbstractTableTransform","text":"<p>dpk_digest/transform.py </p> <p>This file implements the key logic for the transform. It receives a pyarrow table with a list of documents in the data set and appends a new column with a digest value. We will describe the contents of the file in 2 sections:</p> <ul> <li>The first portion of the file includes the language for the license used to distribute and use the code and a set of import statements for the library modules that will be needed for invoking this transform.</li> </ul> <pre><code># (C) Copyright IBM Corp. 2024.\n# Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#  http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \u201cAS IS\u201d BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\n\nfrom typing import Any\n\nimport pyarrow as pa\nimport hashlib\nfrom data_processing.transform import AbstractTableTransform\nfrom data_processing.utils import TransformUtils\n</code></pre> <p>AbstractTableTransform defines a template for the APIs that will be invoked by the runtime to trigger the transform.</p> <p>TransformUtils provides a number of shortcuts commonly used by the transforms for data conversions, table manipulations, etc.</p> <ul> <li>The remaining section of the file implements the Transform method that will be called by the framework when new data is available for annotation. </li> </ul> <pre><code>class DigestTransform(AbstractTableTransform):\n    def __init__(self, config: dict[str, Any]):\n        super().__init__(config)\n\n        ## If the algorithm is not specified, use sha256\n        self.algorithm = config.get('digest_algorithm', \"sha256\")\n\n    def transform(self, \n                  table: pa.Table, \n                  file_name: str = None) -&gt; tuple[list[pa.Table], dict[str, Any]]:\n\n        ## calculate digest for the content column\n        tf_digest = []\n        for elt in table['contents'].to_pylist():\n            h = hashlib.new(self.algorithm)\n            h.update(elt.encode('utf-8'))\n            tf_digest.append(h.hexdigest())\n\n        ## digest as a new column to the existing table\n        table = TransformUtils.add_column(table=table, \n                                           name='digest', \n                                           content=tf_digest)\n\n        metadata = {\"nrows\": len(table)}\n        return [table], metadata\n</code></pre> <p>__init__() receives a dictionary that represents the different configuration parameters specified by the user. In our case, the only parameter used is the string value representing the name of digest. If the user does not specify a digest, we will use default value for \"sha256\".</p> <p>transform() The transform method implements the callback that the runtime uses when it identifies new data to be processed by this transform. It receives a pyarrow table, calculates the digest for each row in the table and appends the digest as a new column to the same table.</p> <p>dpk_digest/__init__.py</p> <pre><code># (C) Copyright IBM Corp. 2024.\n# Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#  http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \u201cAS IS\u201d BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n###############################################################################\nfrom .transform import *\n</code></pre>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-3-implement-transformconfiguration","title":"Step 3: Implement TransformConfiguration","text":"<p>dpk_digest/runtime.py  This file implements 3 classes, the first being TransformConfiguration. It defines two user defined methods that must be implemented by the developer for each transform:</p> <ul> <li>The add_input_params() method is called by the framework to define the set of command line parameters exposed by the runtime to configure the transforms.  The runtime processes the command line parameters and makes them available to the transform instance initializer.</li> <li>The apply_input_params() method is called by the framework to validate the values associated with the configuration parameter.</li> </ul> <pre><code># (C) Copyright IBM Corp. 2024.\n# Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#  http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \u201cAS IS\u201d BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\nimport sys\nfrom dpk_digest.transform import DigestTransform\nfrom data_processing.transform import TransformConfiguration\nfrom data_processing.utils import ParamsUtils, CLIArgumentProvider, get_logger\nfrom argparse import ArgumentParser, Namespace\nfrom data_processing.runtime.pure_python import PythonTransformLauncher\nfrom data_processing.runtime.pure_python.runtime_configuration import (\n    PythonTransformRuntimeConfiguration,)\n\nlogger = get_logger(__name__)\n\nclass DigestConfiguration(TransformConfiguration):\n    def __init__(self):\n        ## Identify the transform implementation that this configuration relates to\n        super().__init__(name='digest', transform_class=DigestTransform,)\n\n    def add_input_params(self, parser: ArgumentParser) -&gt; None:\n        ## Define how varius configuration parameters should be parsed\n        parser.add_argument(\n            \"--digest_algorithm\",\n            type=str,\n            help=\"Specify the hash algorithm to use for calculating the digest value.\",\n        )\n\n    def apply_input_params(self, args: Namespace) -&gt; bool:\n        ## Validate each of the configuration parameters received from the user\n        captured = CLIArgumentProvider.capture_parameters(args, 'digest', True)\n        self.params = self.params | captured\n        if captured.get('digest_algorithm') not in ['sha256','SHA512', 'MD5']:\n            logger.error(f\"Parameter digest_algorithm cannot be other than ['sha256','SHA512', 'MD5']. You specified {args.digest_algorithm}\")\n            return False\n        return True\n</code></pre>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-4-implement-pythontransformruntimeconfiguration","title":"Step 4: Implement PythonTransformRuntimeConfiguration","text":"<p>dpk_digest/runtime.py (continued)  The other two classes in this file include:</p> <ul> <li>DigestRuntime: Implements PythonTransformRuntimeConfiguration and wires the transform into the python orchestrator and allows the framework to instantiate, configure and invoke the transfrom.</li> </ul> <pre><code>class DigestRuntime(PythonTransformRuntimeConfiguration):\n\n    def __init__(self):\n        super().__init__(transform_config=DigestConfiguration())\n\nif __name__ == \"__main__\":\n    launcher = PythonTransformLauncher(DigestRuntime())\n    launcher.launch()\n</code></pre> <ul> <li>Digest: Implements a wrapper that simplifies how the transform is invoked and hides some of the complexity that is inherited by the runtime orchestrator.</li> </ul> <pre><code>class Digest:\n    def __init__(self, **kwargs):\n        self.params = {}\n        for key in kwargs:\n            self.params[key] = kwargs[key]\n        # if input_folder and output_folder are specified, then assume it is represent data_local_config\n        try:\n            local_conf = {k: self.params[k] for k in (\"input_folder\", \"output_folder\")}\n            self.params[\"data_local_config\"] = ParamsUtils.convert_to_ast(local_conf)\n            del self.params[\"input_folder\"]\n            del self.params[\"output_folder\"]\n        except:\n            pass\n\n    def transform(self):\n        sys.argv = ParamsUtils.dict_to_req(d=(self.params))\n        launcher = PythonTransformLauncher(DigestRuntime())\n        return_code = launcher.launch()\n        return return_code\n</code></pre>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-5-implement-raytransformruntimeconfiguration","title":"Step 5: Implement RayTransformRuntimeConfiguration","text":"<p>dpk_digest/ray/runtime.py This file implements the necessary API for integrating the digest transform with the ray orchestrator.</p> <pre><code># (C) Copyright IBM Corp. 2024.\n# Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#  http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \u201cAS IS\u201d BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\nimport sys\n\nfrom data_processing.utils import ParamsUtils, get_logger\nfrom data_processing_ray.runtime.ray import RayTransformLauncher\nfrom data_processing_ray.runtime.ray.runtime_configuration import RayTransformRuntimeConfiguration\nfrom dpk_digest.runtime import DigestConfiguration\n\nlogger = get_logger(__name__)\n\nclass DigestRayRuntime(RayTransformRuntimeConfiguration):\n\n    def __init__(self):\n        super().__init__(transform_config=DigestConfiguration())\n\nif __name__ == \"__main__\":\n    launcher = RayTransformLauncher(DigestRayRuntime())\n    launcher.launch()\n</code></pre> <ul> <li>Similarly, we will implement the Digest api for the ray submodule to define a simplified method for the API.</li> </ul> <pre><code>class Digest:\n    def __init__(self, **kwargs):\n        self.params = {}\n        for key in kwargs:\n            self.params[key] = kwargs[key]\n        try:\n            local_conf = {k: self.params[k] for k in (\"input_folder\", \"output_folder\")}\n            self.params[\"data_local_config\"] = ParamsUtils.convert_to_ast(local_conf)\n            del self.params[\"input_folder\"], self.params[\"output_folder\"]\n        except:\n            pass\n        try:\n            worker_options = {k: self.params[k] for k in (\"num_cpus\", \"memory\")}\n            self.params[\"runtime_worker_options\"] = ParamsUtils.convert_to_ast(worker_options)\n            del self.params[\"num_cpus\"], self.params[\"memory\"]\n        except:\n            pass\n\n    def transform(self):\n        sys.argv = ParamsUtils.dict_to_req(d=(self.params))\n        launcher = RayTransformLauncher(DigestRayRuntime())\n        return_code = launcher.launch()\n        return return_code\n</code></pre>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-6-develop-unit-test","title":"Step 6: Develop Unit Test","text":"<ul> <li>For our testing, we will need some initial data as input to our transform. We will copy it from another transform test folder.</li> </ul> <pre><code>cd data-prep-kit/transforms/universal/digest\nmkdir -p test-data/input\ncp ../../language/doc_chunk/test-data/expected/*.parquet test-data/input\n</code></pre> <ul> <li>Create a virtual environment and run the transform against the input data to produce the expected output data. This will be used by the CI/CD to verify that the logic of the transform always produces the same output for a given input.</li> </ul> <p><pre><code>cd data-prep-kit/transforms/universal/digest\npython -m venv venv &amp;&amp; source venv/bin/activate\npip intall data-prep-toolkit\npip install -r requirements.txt\npython -m dpk_digest.runtime --digest_algorithm sha256 --data_local_config \"{ 'input_folder' : 'test-data/input', 'output_folder' : 'expected\u2019}\u201d \n</code></pre> If the test code runs properly, we should see 2 new files created in the test-data/expected folder: <pre><code>test-data/expected/test1.parquet\ntest-data/expected/metadata.json\n</code></pre></p> <ul> <li>Developers have some freedom in designing their unit tests. In this section we show how developers can use the test fixture defined in the framework to rapidly create unit tests.</li> </ul> <p>test/test_digest.py</p> <pre><code># (C) Copyright IBM Corp. 2024.\n# Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#  http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \u201cAS IS\u201d BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\n\nimport os\n\nfrom data_processing.test_support.launch.transform_test import (\n    AbstractTransformLauncherTest,\n)\nfrom data_processing.runtime.pure_python import PythonTransformLauncher\nfrom dpk_digest.runtime import DigestRuntime\n\n\nclass TestDigestTransform(AbstractTransformLauncherTest):\n\n    def get_test_transform_fixtures(self) -&gt; list[tuple]:\n        basedir = os.path.abspath(os.path.join(os.path.dirname(__file__), \n                                               \"../test-data\"))\n        fixtures = []\n        transform_config = {\n            \"run_locally\": True,\n            \"digest_algorithm\": \"sha256\", \n        }\n        launcher = TransformLauncher(DigestRuntime())\n        fixtures.append((launcher, transform_config, \n                         basedir + \"/input\", \n                         basedir + \"/expected\"))\n        return fixtures\n</code></pre>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-7-integrate-with-cicd","title":"Step 7: Integrate with CI/CD","text":"<ul> <li>The repo implements a rich set of functionality for setting up the environment, running unit tests, publishing the transforms to pypi, building the transforms as part of a docker image and running it with Kubeflow. For the purpose of this section, we will explore only a portion of the capabilities that support this initial phase of the implementation.</li> </ul> <p>We will first copy the Makefile template from the parent folder:</p> <pre><code>cd data-prep-kit/transforms/universal/digest\ncp ../../Makefile.transform.template Makefile\ncp ../../Dockerfile.python.template Dockerfile.python\n</code></pre> <ul> <li>The Makefile has a number of predefined targets that will be useful for testing and publishing the transform. To get a list of available targets, run the following command from the digest folder:</li> </ul> <p><pre><code>make\n</code></pre> Below is a small list of available targets that may be useful at this stage.  <pre><code>Target               Description\n------               -----------\nclean                Clean up the virtual environment.\nvenv                 Create the virtual environment using requirements.txt\ntest-src             Create the virtual environment using requirements.txt and run the unit tests\nimage                Build the docker image for the transform\ntest-image           Build and test the docker image for the transform \npublish              Publish the docker image to quay.io container registry\n</code></pre></p> <ul> <li> <p>Create virtual environment with all preloaded dependencies: <pre><code>make clean &amp;&amp; make venv\n</code></pre></p> </li> <li> <p>Run unit tests and verify the proper operations of the code:</p> </li> </ul> <pre><code>make test-src\n</code></pre> <ul> <li>Edit transforms/pyproject.toml and add the requirements.txt for the module and the name of the module and its package location:</li> </ul> <pre><code>digest = { file = [\"universal/digest/requirements.txt\"]}\n...\n[tool.setuptools.package-dir]\n...\ndpk_digest = \"universal/digest/dpk_digest\"\n...\n</code></pre>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-8-create-notebook","title":"Step 8: Create Notebook","text":"<p>The notebook should show how to run the notebook from the current folder. Guidance on how to set up jupyter lab can be found here. This is a simple notebook for our digest transform. </p>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-9-create-readme-file","title":"Step 9: Create Readme file","text":"<p>The README file for the transform should have, at a minimum, the following sections: Summary, Contributors, Configuration and command line options, an Example of how to run from command line and link to a notebook. If applicable, it should have more sections on Troubleshooting, Transforming data using the transform image and sections on Ray and/or Spark versions of the transform.  This is a minimal README file for our digest transform. </p>"},{"location":"doc/quick-start/contribute-your-own-transform/#step-10-set-up-kfp-pipeline","title":"Step 10: Set up KFP Pipeline","text":"<p>It might be desirable to build a KubeFlow Pipeline (KFP) chaining multiple transforms together. In this section, we will cover the steps a developer needs to take so that the operation team can create a pipeline tailored to their specific use case. We will only cover the artifact that the developer needs to produce to enable the integration of the digest transform in a KFP pipeline.</p> <p>kfP-ray/Makefile</p> <ul> <li>Create a folder to host KFP related artifacts</li> </ul> <pre><code>cd data-prep-kit/transforms/universal/digest\nmkdir -p kfp_ray\ncp ../../Makefile.kfp.template kfp_ray/Makefile\n</code></pre> <p>kfP-ray/digest_wf.py</p> <ul> <li> <p>Create a KFP definition file. This file will be used to produce the kfp workflow yaml definition file. The full content of this file in available here. We only highlight some of the key elements.</p> </li> <li> <p>This file defines the reference to the docker image for the transform and entry point:</p> <ul> <li>task_image = \"quay.io/dataprep1/data-prep-kit/digest-ray:latest\"</li> <li>EXEC_SCRIPT_NAME: str = \"-m dpk_digest.ray.runtime\"</li> </ul> </li> </ul> <pre><code># (C) Copyright IBM Corp. 2024.\n# Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d);\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#  http://www.apache.org/licenses/LICENSE-2.0\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \u201cAS IS\u201d BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n################################################################################\nimport os\n\nimport kfp.compiler as compiler\nimport kfp.components as comp\nimport kfp.dsl as dsl\nfrom workflow_support.compile_utils import ONE_HOUR_SEC, ONE_WEEK_SEC, ComponentUtils\n\n\ntask_image = \"quay.io/dataprep1/data-prep-kit/digest-ray:latest\"\n\n# the name of the job script\nEXEC_SCRIPT_NAME: str = \"-m dpk_digest.ray.runtime\"\n# components\nbase_kfp_image = \"quay.io/dataprep1/data-prep-kit/kfp-data-processing:0.2.3\"\n\n# path to kfp component specifications files\ncomponent_spec_path = \"../../../../kfp/kfp_ray_components/\"\n</code></pre> <ul> <li> <p>It defines the list of configuration parameters that are required by the framework return as a dictionary structure:</p> <ul> <li>\"digest_algorithm\": digest_algorithm,</li> </ul> </li> </ul> <pre><code>def compute_exec_params_func(\n    ...\n    ...\n    digest_algorithm: str,\n) -&gt; dict:\n    return {\n        ...\n        ...\n        \"digest_algorithm\": digest_algorithm,\n    }\n</code></pre> <ul> <li> <p>It assigns a name to this workflow task:</p> <ul> <li>TASK_NAME: str = \"digest\"</li> </ul> </li> </ul> <pre><code># Task name is part of the pipeline name, the ray cluster name and the job name in DMF.\nTASK_NAME: str = \"digest\"\n</code></pre> <ul> <li>Pipeline definition method and default values:</li> </ul> <pre><code> @dsl.pipeline(\n    name=TASK_NAME + \"-ray-pipeline\",\n    description=\"Pipeline for digest\",\n    )\ndef digest(\n    ###\n    ...\n    ###\n):\n</code></pre> <ul> <li>It defines the main entry point for compiling the yaml file required for running kfp.</li> </ul> <pre><code>if __name__ == \"__main__\":\n    # Compiling the pipeline\n    compiler.Compiler().compile(digest, __file__.replace(\".py\", \".yaml\"))\n</code></pre>"},{"location":"doc/quick-start/contribute-your-own-transform/#contributors","title":"Contributors","text":"<ul> <li>Maroun Touma (touma@us.ibm.com)</li> <li>Shahrokh Daijavad (shahrokh@us.ibm.com)</li> </ul>"},{"location":"doc/quick-start/quick-start/","title":"Quick Start for Data Prep Kit","text":"<p>Here we provided short examples of various uses of the Data Prep Kit. Most users who want to jump right in can use standard pip install to deploy the data-prep-kit and the python or ray transforms to their virtual python environment. </p> <p>When setting up a virtual environment it is recommended to use python3.11 as in the example below using conda. </p>"},{"location":"doc/quick-start/quick-start/#create-a-virtual-environment","title":"Create a Virtual Environment","text":"<p>setup a virtual environment using conda</p> <pre><code>conda create -n data-prep-kit-1 -y python=3.11\n</code></pre> <p>Linux system only: Install the gcc/g++ that is required while building fastext: If you are using a linux system, install gcc using the below commands, as it will be required to compile and install fasttext currently used by some of the transforms.</p> <pre><code>conda install gcc_linux-64\nconda install gxx_linux-64\n</code></pre> <p>activate the new conda environment</p> <pre><code>conda activate data-prep-kit-1\n</code></pre> <p>make sure env is switched to data-prep-kit-1 and Check python version.</p> <pre><code>python --version\nThe command above should say: 3.11\n</code></pre> <p>install data prep toolkit</p> <p><pre><code>pip3 install 'data-prep-toolkit-transforms[ray,all]'\n</code></pre> the command above install the complete library with all the tansforms. In certain situations, it may be desirable to install a specific transform with or without the ray runtime. In that case, the command can specify the name of the transform in the [extra] value such as:</p> <p>To install the lang_id transform, use the following command:</p> <pre><code>pip3 install 'data-prep-toolkit-transforms[lang_id]' \n</code></pre> <p>to install the lang_id transform with the ray runtime, use the following command:</p> <pre><code>pip3 install 'data-prep-toolkit-transforms[ray,lang_id]' \n</code></pre>"},{"location":"doc/quick-start/quick-start/#setting-up-jupyter-lab-for-local-experimentation-with-transform-notebooks","title":"Setting up Jupyter lab for local experimentation with transform notebooks","text":"<pre><code>pip install jupyterlab ipykernel ipywidgets\npython -m ipykernel install --user --name=data-prep-kit --display-name \"dataprepkit\"\n</code></pre>"},{"location":"doc/quick-start/quick-start/#running-transforms","title":"Running transforms","text":""},{"location":"doc/quick-start/quick-start/#notebooks","title":"Notebooks","text":"<pre><code>* There is a [simple notebook](https://github.com/ian-cho/data-prep-kit/blob/dev/examples/notebooks/Run_your_first_transform_colab.ipynb) for running a single transform that can be run from either Google Colab or the local environment by downloading the file.  \n* In most indidividual transform folders, we have included one (Python), two (Python and Ray), or three (Python, Ray and Spark) notebooks for running that transform. In order to run all these notebooks in the local environment, we clone the repo as: \n```bash\ngit clone git@github.com:IBM/data-prep-kit.git \n```\nThen we go to an indvidual transformer folder, where we find the corresponding notebooks. As an example:\n\n```bash\ncd data-prep-kit/transforms/universal/fdedup\nmake venv\nsource venv/bin/activate \npip install jupyterlab\njupyter lab\n```\nYou can now run the [Python version](https://github.com/ian-cho/data-prep-kit/blob/dev/transforms/universal/fdedup/fdedup_python.ipynb), [Ray version](https://github.com/ian-cho/data-prep-kit/blob/dev/transforms/universal/fdedup/fdedup_ray.ipynb) or [Spark version](https://github.com/ian-cho/data-prep-kit/blob/dev/transforms/universal/fdedup/fdedup_spark.ipynb) of the three notebooks for this transform.\n</code></pre>"},{"location":"doc/quick-start/quick-start/#command-line","title":"Command line","text":"<ul> <li>Using the CLI - install and run a transform from the command line.</li> <li>Using a docker image - runs a transform in a docker transform image </li> <li>Using a project's virtual environment - runs a transform on from its project directory</li> </ul>"},{"location":"doc/quick-start/quick-start/#running-transforms-on-windows","title":"Running transforms on Windows","text":"<p>The following transforms have been shown to work on Windows with the patched release described below:</p> <pre><code>pdf2parquet\ndoc_chunk\ndoc_quality\nhtml2parqet\npii_redactor\ntext_encoder\ndoc_id\nededup\nfilter\nhap\nprofiler\nresize\ntokenization\n</code></pre> <p>A working windows environment can be setup using Anaconda Navigator with: * Python 3.12.7 * Windows-11-10.0.22631-SP0 * 3.12.7 | packaged by Anaconda, Inc. | (main, Oct 4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)]</p> <p>To install the transforms on windows, use the following commands:</p> <pre><code>pip intall data-prep-toolkit==0.2.3.post1\npip install data-prep-toolkit-transforms[all]\npip install fasttext-wheel\n</code></pre> <p>To install the ray transforms on windows using the ray runtime, use the following commands:</p> <pre><code>pip intall data-prep-toolkit[ray]==0.2.3.post1\npip install data-prep-toolkit-transforms[all]\npip install fasttext-wheel\n</code></pre> <p>To install a single transform, for example pdf2parquet, use the following commands: <pre><code>pip intall data-prep-toolkit==0.2.3.post1\npip install --user data-prep-toolkit-transforms[pdf2parquet]\n</code></pre></p> <p>If you want to try Notebooks that run many of the transforms in sequence, use the Anaconda Navigator to install and launch an instance of Jupyter Notebook and use the following sample Notebook for the python runtime and another sample Notebook for the ray runtime. </p>"},{"location":"doc/quick-start/quick-start/#creating-transforms","title":"Creating transforms","text":"<ul> <li>Tutorial - shows how to use the library to add a new transform.</li> </ul>"},{"location":"doc/quick-start/run-transform-cli/","title":"Running a Transform from the Command Line","text":"<p>Here we address a simple use case of applying a single transform to a  set of parquet files.  We'll use the <code>pdf2parquet</code> transform as an example, but in general, this process will work for any of the transforms contained in Data Prep Kit. Additionally, what follows uses the  python runtime but the examples below should also work for the ray or spark  runtimes.</p>"},{"location":"doc/quick-start/run-transform-cli/#install-data-prep-kit-from-pypi","title":"Install data prep kit from PyPi","text":"<p>The latest version of the Data Prep Kit is available on PyPi for Python 3.10, 3.11 or 3.12. It can be installed using: </p> <pre><code>pip install  'data-prep-toolkit-transforms[ray,all]'\n</code></pre> <p>The above installs all available transforms and both the python and Ray runtimes. </p> <p>NOTE: As of this writing, on linux systems there is an  issue  installing <code>fasttext</code> for the <code>lang_id</code> transform.  A workaround is to install using conda. Alternatively, you may choose to install only the transform(s) of interest (see below).</p> <p>When installing select transforms, users can specify the name of the transform in the pip command, rather than [all]. For example, use the following command to install only the pdf2parquet transform: <pre><code>pip install 'data-prep-toolkit-transforms[pdf2parquet]'\n</code></pre> As an alternative, installing in a conda environment can be found here.</p>"},{"location":"doc/quick-start/run-transform-cli/#run-a-transform-at-the-command-line","title":"Run a transform at the command line","text":"<p>Here we run the <code>pdf2parquet</code> transform on its input data to  import pdf content into rows of a parquet file. First, we load some data for the transform to run on using the following python code: <pre><code>import urllib.request\nimport shutil\nshutil.os.makedirs(\"input\", exist_ok=True)\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/language/pdf2parquet/test-data/input/archive1.zip\", \"input/archive1.zip\")\nurllib.request.urlretrieve(\"https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/language/pdf2parquet/test-data/input/redp5110-ch1.pdf\", \"input/redp5110-ch1.pdf\")\n</code></pre> <pre><code>% ls input\narchive1.zip        redp5110-ch1.pdf\n</code></pre></p> <p>Next we run <code>pdf2parquet</code> on the data in the <code>input</code> folder. <pre><code>python -m dpk_pdf2parquet.transform_python \\\n    --data_local_config \"{ 'input_folder': 'input', 'output_folder': 'output'}\" \\\n    --data_files_to_use \"['.pdf', '.zip']\" \n</code></pre> Parquet files are generated in the designated <code>output</code> folder: <pre><code>% ls output\narchive1.parquet        metadata.json           redp5110-ch1.parquet\n</code></pre> All transforms are runnable from the command line in the manner above.</p>"},{"location":"doc/quick-start/run-transform-image/","title":"Running a Transform Image","text":"<p>Here we address a simple use case of applying a single transform to a  set of parquet files. We'll use one of the pre-built docker images from the repository to process the data. Additionally, what follows uses the  python runtime image, but the examples below should also work for the ray or spark runtime images (<code>-ray</code> or <code>-spark</code> image name suffix instead of <code>-python</code>).</p>"},{"location":"doc/quick-start/run-transform-image/#getting-an-image","title":"Getting an image","text":"<p>You may build the transform locally in the repository, for example, <pre><code>cd transforms/universal/noop\nmake image\ndocker images | grep noop\n</code></pre> produces <pre><code>quay.io/dataprep1/data-prep-kit/noop-python                     latest   aac55fa3d82d  5 seconds ago  355 MB\n...\n</code></pre> Or, you can use the pre-built images (latest, or 0.2.1 or later tags)  on quay.io found at https://quay.io/user/dataprep1.</p>"},{"location":"doc/quick-start/run-transform-image/#local-data-python-runtime","title":"Local Data - Python Runtime","text":"<p>To use an image to process local data we will mount the host input and output directories into the image.  Any mount point can be used, but we will use <code>/input</code> and <code>/output</code>.</p> <p>To process data in the <code>/home/me/input</code> directory and write it to the <code>/home/me/output</code> directory, we mount these directories into the image at the above mount points. So for example, using the locally built <code>noop</code> transform:</p> <p><pre><code>docker run  --rm \n    -v /home/me/input:/input \\\n    -v /home/me/output:/output \\\n    noop-python:latest  \\\n    python -m dpk_noop.runtime \\\n    --data_local_config \"{ \\\n        'input_folder'  : '/input', \\\n        'output_folder' : '/output' \\\n        }\"\n</code></pre> To run the quay.io located transform instead, substitute  <code>quay.io/dataprep1/data-prep-kit/noop-python:latest</code> for <code>noop-python:latest</code>, as follows: <pre><code>docker run  --rm \n    -v /home/me/input:/input \\\n    -v /home/me/output:/output \\\n    quay.io/dataprep1/data-prep-kit/noop-python:latest  \\\n    python -m dpk_noop.runtime \\\n    --data_local_config \"{ \\\n        'input_folder'  : '/input', \\\n        'output_folder' : '/output' \\\n        }\"\n</code></pre></p>"},{"location":"doc/quick-start/run-transform-image/#local-data-ray-runtime","title":"Local Data - Ray Runtime","text":"<p>To use the ray runtime, we must  1. Switch to using the ray-based image <code>noop-ray:latest</code> 2. Use the ray runtime python main defined in <code>dpk_noop/ray/runtime.py</code></p> <p>For example, using the quay.io image <pre><code>docker run  --rm \n    -v /home/me/input:/input \\\n    -v /home/me/output:/output \\\n    quay.io/dataprep1/data-prep-kit/noop-ray:latest     \\\n    python -m dpk_noop.ray.runtime \\\n    --data_local_config \"{ \\\n        'input_folder'  : '/input', \\\n        'output_folder' : '/output' \\\n        }\"\n</code></pre> This is functionally equivalent to the python-runtime, but additional configuration can be provided (see the  launcher args) for details.</p>"},{"location":"doc/quick-start/run-transform-image/#s3-located-data-python-runtime","title":"S3-located Data - Python Runtime","text":"<p>When processing data located in S3 buckets, one can use the same image and specify different <code>--data_s3_*</code> configuration as follows: </p> <pre><code>docker run  --rm \n    noop-python:latest  \\\n    python -m dpk_noop.runtime \\\n    --data_s3_cred \"{ \\\n        'access_key'  : '...', \\\n        'secret_key' : '...', \\\n        'url' : '...', \\\n        }\"  \\\n    --data_s3_config \"{ \\\n        'input_folder'  : '...', \\\n        'output_folder' : '...', \\\n        }\"  \n</code></pre>"},{"location":"doc/quick-start/run-transform-venv/","title":"Running a Transform in a Virtual Environment","text":"<p>Here we address a simple use case of applying a single transform to a  set of parquet files.  We'll use the noop transform as an example, but in general, this process will work for any of the transforms contained in the repository. Additionally, what follows uses the  python runtime (e.g., transforms/universal/noop/dpk_noop directory),  but the examples below should also work for the ray (transforms/universal/noop/dpk_noop/ray directory) or spark  (transforms/universal/noop/dpk_noop/spark directory) runtimes.</p>"},{"location":"doc/quick-start/run-transform-venv/#creating-the-virtual-environment","title":"Creating the Virtual Environment","text":"<p>Each transform project contains a Makefile that will assist in building the virtual environment in a directory named <code>venv</code>. To create the virtual environment for the <code>noop</code> transform:</p> <p><pre><code>cd transforms/universal/noop\nmake venv \n</code></pre> Note, if needed, you can override the default <code>python</code> command used  in <code>make venv</code> above, with for example:</p> <pre><code>make PYTHON=python3.10 venv\n</code></pre>"},{"location":"doc/quick-start/run-transform-venv/#local-data","title":"Local Data","text":"<p>To process data in the <code>/home/me/input</code> directory and write it to the <code>/home/me/output</code> directory, activate the virtual environment and then call the transform referencing these directories. So for example, using the <code>noop</code> transform  to read parquet files from <code>/home/me/input</code>:</p> <pre><code>cd transforms/universal/noop\nsource venv/bin/activate\npython -m dpk_noop.runtime \\\n    --data_local_config \"{ \\\n        'input_folder'  : '/home/me/input', \\\n        'output_folder' : '/home/me/output' \\\n        }\"\ndeactivate\n</code></pre>"},{"location":"doc/quick-start/run-transform-venv/#s3-located-data","title":"S3-located Data","text":"<p>When processing data located in S3 buckets, one can use the same  approach and specify different <code>--data_s3_*</code> configuration as follows: </p> <pre><code>cd transforms/universal/noop\nsource venv/bin/activate\npython -m dpk_noop.runtime \\\n    --data_s3_cred \"{ \\\n        'access_key'  : '...', \\\n        'secret_key' : '...', \\\n        'url' : '...', \\\n        }\"  \\\n    --data_s3_config \"{ \\\n        'input_folder'  : '...', \\\n        'output_folder' : '...', \\\n        }\"  \n</code></pre>"},{"location":"examples/","title":"Index","text":"About Data Prep Kit Recipes <p>Welcome to cooking with Data Prep Kit. Here we share some of our most asked, searched and shared Data Prep Kit recipes for processing unstructured and structured data for plethora of use cases like RAG, fine-tuning etc. along with examples of KFP workflows.</p> <ul> <li>Data Files</li> <li>Introductory Recipe Notebook to get started</li> <li>Recipes for Processing Code and Language Data for Finetuning LLMs</li> <li>Recipe for building RAG system using pdf data</li> <li>Recipe for building RAG system using html data</li> <li>Recipe for curating customer service data for HAP</li> <li>Recipe for curating customer service data for PII</li> <li>KFP Pipeline Walkthrough</li> </ul>"},{"location":"examples/agentic/","title":"Agentic Data Agent Experiments","text":""},{"location":"examples/agentic/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Project Overview</li> <li>Installation Guide</li> <li>Usage</li> </ol>"},{"location":"examples/agentic/#project-overview","title":"Project Overview","text":"<p>This project focuses on automating the integration of Large Language Models (LLM) based workflow in the data access. It contains the following notebooks:</p> <ul> <li> <p>Planning_DPK_agent.ipynb: Planner for Data-Prep-Kit tasks with code generation. This notebook enables the data engineer (or data user) to efficiently build and run pipelines that performs required tasks defined by a natural language. It includes a langgraph LLM agent that has several components like planner, judge, and code generator. This agent can generate as a result a python code of a DPK pipeline which can be run by the user from command line.</p> </li> <li> <p>dpk_as_tools.ipynb: Use DPK transforms defined as langchain tools or  llama-index tools.  This notebook leverages LLM to generate a DPK transforms pipeline based on natural language inputs.  The LLM processes the provided input and produces the pipeline in the correct format, making it ready for execution. Subsequently, each transform in the pipeline is invoked by calling its lang-chain or llama-index implementations.</p> </li> </ul>"},{"location":"examples/agentic/#before-you-begin","title":"Before you begin","text":"<p>Ensure that you have python 3.11</p>"},{"location":"examples/agentic/#installation-guide","title":"Installation Guide","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone git@github.com:IBM/data-prep-kit.git\ncd examples/agentic\n</code></pre></p> </li> <li> <p>Create Python virtual environment: <pre><code>python -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install jupyter\npip install ipython &amp;&amp; pip install ipykernel\npip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Configure access to LLM:</p> </li> </ol> <p>We have have tested our project with the following LLM execution frameworks:    - Replicate     - Watsonx    - locally running Ollama (on mac)</p> <p>3.1 Setup Instructions for each framework:</p> <p>The notebook cell that defines the models contains all frameworks with only the <code>replicate</code> part uncomment.  To use one of the other frameworks uncomment its part in the cell while commenting out the other frameworks. Please note that the frameworks have been tested with a specific LLM and due to the inherent nature of LLMs, using a different model may not produce the same results.</p> <ul> <li>Replicate:<ul> <li>Obtain Replicate API token</li> <li>Store the following value in the <code>.env</code> file located in your project directory:      <pre><code>   REPLICATE_API_TOKEN=&lt;your Replicate API token&gt;\n</code></pre></li> </ul> </li> <li>Ollama: <ul> <li>Download Ollama.</li> <li>Download one of the supported models. We tested with <code>llama3.3</code> model.</li> <li>update the <code>model_ollama_*</code> names in the relevant cells if needed.</li> </ul> </li> <li>Watsonx:<ul> <li>Register for Watsonx</li> <li>Obtain its API key</li> <li>Store the following values in the <code>.env</code> file located in your project directory:      <pre><code>   WATSONX_URL=&lt;WatsonX entry point, e.g. https://us-south.ml.cloud.ibm.com&gt;\n   WATSON_PROJECT_ID=&lt;your Watsonx project ID&gt;\n   WATSONX_APIKEY=&lt;your Watsonx API key&gt;\n</code></pre></li> </ul> </li> </ul>"},{"location":"examples/agentic/#usage","title":"Usage","text":"<p>To launch the notebooks, execute the following command in your terminal: <pre><code>Jupyter notebook\n</code></pre></p> <p>Once the Jupyter interface is loaded, select the desired notebook to begin working with it.</p>"},{"location":"examples/agentic/llm_utils/dpk/","title":"Index","text":"<p>This directory contains implementation of DPK transforms as langchain or llama-index tools.</p> <ul> <li><code>langchain_tools</code>: directory contains code to define DPK transforms as langchain tools similar to the tools defined in here.</li> <li><code>llama_index_tools</code>: directory contains code to define DPK transforms as llama-index tools similar to the tools defined in here.</li> <li><code>dpk_common.py</code>: contains definitions used in both of the implementations defined above. </li> </ul> <p>For example usage please look at <code>dpk_as_tools.ipynb</code> notebook.</p> <p>In addition, this directory contains files that define the context for the <code>DPK agent</code>:</p> <ul> <li><code>tools.py</code>: Json dictionaries that describe the tools to pass it to the agent. Each dictionary describes a DPK transform.</li> <li><code>examples.py</code>: Examples of tasks and their matched plan. Used in the planner prompt to get more accurate results.</li> <li><code>constraints.py</code>: Constrains that the generated plan or pipeline should satisfy.</li> </ul>"},{"location":"examples/data-files/pdf-processing-1/","title":"Index","text":""},{"location":"examples/data-files/pdf-processing-1/#creating-input-pdfs-optional","title":"Creating Input PDFs (Optional)","text":"<p>Sample PDFs we use for this example are created from markdown documents using pandoc utility, as follows.</p> <pre><code>pandoc earth.md  -o earth.pdf\npandoc earth2.md  -o earth2.pdf\npandoc mars.md  -o mars.pdf\npandoc spam.md  -o spam.pdf\npandoc lorem-ipsum.md  -o lorem-ipsum.pdf\n</code></pre>"},{"location":"examples/data-files/pdf-processing-1/earth/","title":"Earth","text":""},{"location":"examples/data-files/pdf-processing-1/earth/#solar-system","title":"Solar System","text":"<p>Our solar system is a vast and fascinating expanse, comprising eight planets, five dwarf planets, numerous moons, asteroids, comets, and other celestial bodies. At its center lies the star we call the Sun.</p> <p>For more details about our Solar system see Chapter 1.</p>"},{"location":"examples/data-files/pdf-processing-1/earth/#earth_1","title":"Earth","text":"<p>Earth is the third planet from the Sun.  It's our home planet.  Earth is the only place we know of with life.</p> <p>Basic facts about Earth:</p> <ul> <li>Distance from the Sun: Average of 149.6 million kilometers (93 million miles)</li> <li>Rotation Period: 24 hours (one day)</li> <li>Moons: One moon, called Luna or simply \"the Moon\".</li> </ul>"},{"location":"examples/data-files/pdf-processing-1/earth2/","title":"Earth","text":""},{"location":"examples/data-files/pdf-processing-1/earth2/#solar-system","title":"Solar System","text":"<p>Our solar system is a vast and fascinating expanse, comprising eight planets, five dwarf planets, numerous moons, asteroids, comets, and other celestial bodies. At its center lies the star we call the Sun.</p> <p>For more details about the Solar system see Chapter 1.</p>"},{"location":"examples/data-files/pdf-processing-1/earth2/#earth_1","title":"Earth","text":"<p>Earth is the third planet from the Sun.  It's our home planet.  Earth is the only place we know of with life.</p> <p>Basic facts about Earth:</p> <ul> <li>Distance from the Sun: Average of 149.6 million kilometers (93 million miles)</li> <li>Rotation Period: 24 hours (one day)</li> <li>Moons: One moon, called Luna or simply \"the Moon\".</li> </ul>"},{"location":"examples/data-files/pdf-processing-1/lorem-ipsum/","title":"Lorem ipsum","text":"<p>Lorem ipsum Lorem ipsum Lorem ipsum</p>"},{"location":"examples/data-files/pdf-processing-1/mars/","title":"Mars","text":""},{"location":"examples/data-files/pdf-processing-1/mars/#solar-system","title":"Solar System","text":"<p>Our solar system is a vast and fascinating expanse, comprising eight planets, five dwarf planets, numerous moons, asteroids, comets, and other celestial bodies. At its center lies the star we call the Sun.</p> <p>For more details about the Solar system see Chapter 1.</p>"},{"location":"examples/data-files/pdf-processing-1/mars/#mars_1","title":"Mars","text":"<p>Mars, the fourth planet from the Sun, is a cold, desert world with a thin atmosphere composed primarily of carbon dioxide.  Its reddish hue comes from iron oxide, or rust, prevalent on its surface. </p> <p>Basic facts about Mars:</p> <ul> <li>Distance from the Sun: Average of 228 million kilometers (142 million miles)</li> <li>Rotation Period: 24.6 hours (one Martian day - called a \"sol\")</li> <li>Moons: Two small moons, Phobos and Deimos.</li> </ul>"},{"location":"examples/data-files/pdf-processing-1/spam/","title":"Spam","text":"<p>Free xxx</p>"},{"location":"examples/kfp-pipelines/superworkflows/ray/kfp_v2/","title":"Chaining transforms using KFP V2","text":"<p>As in super pipelines of KFP v1, we want to offer an option of running a series of transforms one after the other on the data. But, in KFP v2 we can make it easier to chain transforms using the nested pipelines that KFP v2 offers.</p> <p>One example of chaining <code>noop</code>, <code>document id</code> and <code>ededup</code> transforms can be found here. When running this pipeline it appears as hierarchical graph with three nested pipelines, one for each transform as shown in the following screenshots.</p> <p><code>root</code> Layer </p> <p><code>root -&gt; noop-ray-pipeline</code> Layer </p> <p><code>root -&gt; noop-ray-pipeline -&gt; exit-handler-1</code> Layer </p> <p>Another useful feature of the KFP v2 is the <code>Json</code> editor for the <code>dict</code> type input parameter as shown here: </p>"},{"location":"examples/kfp-pipelines/superworkflows/ray/kfp_v2/#main-differences-from-kfp-v1-superpipeline","title":"Main differences from KFP v1 superpipeline:","text":"<ul> <li>Uploading the transform pipelines before running the superpipeline is not required. When compiling the superpipeline code, it automatically retrieves the latest versions of the transforms.   To enable this, the path to the transform pipelines code must be included in <code>PYTHONPATH</code> before compiling the superpipeline. In this example, the Makefile ensures <code>PYTHONPATH</code> is updated to include the transforms directory. Additionally, if the pipeline references modules in subdirectories    (such as in the ededup case, where it refers to a module in the <code>src</code> subdirectory) these subdirectories must also be added to <code>PYTHONPATH</code>.</li> <li>It creates just one run that includes all the nested transfroms and their sub-tasks.</li> <li>No need for additional component as <code>executeSubWorkflowComponent.yaml</code>. All the implementation in the same pipeline file.</li> <li>In superpipelines of KFP v1 there exists an option to override the common parameters with specific values for each one of the transforms. This option is missing in the KFP v2 superpipelines.</li> <li>In kfp V2 pipelines the user is requested to insert a unique string for the ray cluster created at run creation time (called <code>ray_run_id_KFPv2</code>). This is because in KFPv2 <code>dsl.RUN_ID_PLACEHOLDER</code> is deprecated and cannot be used since SDK 2.5.0 and we cannot generate a unique string at run-time, see https://github.com/kubeflow/pipelines/issues/10187.</li> </ul>"},{"location":"examples/kfp-pipelines/superworkflows/ray/kfp_v2/#how-to-compile-the-superpipeline","title":"How to compile the superpipeline","text":"<pre><code>cd examples/kfp/superworkflows/ray/kfp_v2/\nmake clean\nexport KFPv2=1\nmake workflow-build\n</code></pre> <p>Please note that in KFP v2, there is an issue where pipeline input parameters with default values of <code>False</code>or an empty  string do not appear in the GUI, causing them to be required despite having default values. As  a workaround for empty string fields, inserting a single space can be used. This issue has been resolved in newer KFP versions.</p>"},{"location":"examples/notebooks/pdf-processing-1/","title":"PDF Processing with Data Prep Kit","text":"<p>Show cases Data Prep Kit capabilities of processing PDFs.</p> <p>We will demonstrate the following:</p> <ul> <li>Extracting text from PDF files</li> <li>removing duplicates (exact and fuzzy matches)</li> <li>accessing document quality and removing documents containing spam words, placeholder content like 'lorem ipsum' ..etc.</li> </ul> <p>Workflow</p> <p></p>"},{"location":"examples/notebooks/pdf-processing-1/#setting-up-python-environment","title":"Setting up Python Environment","text":"<p>The code can be run on either </p> <ol> <li>Google colab: very easy to run; no local setup needed.</li> <li>On your local Python environment.  Here is a quick guide.  You can  find instructions for latest version here</li> </ol> <pre><code>conda create -n data-prep-kit -y python=3.11\nconda activate data-prep-kit\n\n# install the following in 'data-prep-kit' environment\ncd examples/notebooks/pdf-processing-1\npip3 install  -r requirements.txt\n\n# start jupyter and run the notebooks with this jupyter\njupyter lab\n</code></pre>"},{"location":"examples/notebooks/pdf-processing-1/#data-files","title":"Data Files","text":"<p>PDF files are located in examples/data-files/pdf-processing-1</p>"},{"location":"examples/notebooks/pdf-processing-1/#running-the-code","title":"Running the code","text":"<p>python version </p> <p>ray version </p>"},{"location":"examples/notebooks/pdf-processing-1/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any errors loading libraries, try creating a custom kernel and using it to run the notebooks.</p> <pre><code>python -m ipykernel install --user --name=data-prep-kit --display-name \"dataprepkit\"\n# and select this kernel within jupyter notebook\n</code></pre>"},{"location":"examples/notebooks/rag-html-1/","title":"Using Data Prep Kit to Process HTML files","text":"<p>This example will show how to crawl a website, process HTML files and query them using RAG.</p> <p>Here is the process:</p> <p><code>website ---(crawler) --&gt; HTML files --- (html2pq)--&gt; markdown content ---(llama-index)--&gt; save to vector db (Milvus) ---(query)---&gt; LLM</code></p>"},{"location":"examples/notebooks/rag-html-1/#step-1-setup-python-env","title":"Step-1: Setup Python Env","text":"<pre><code>conda create -n dpk-html-processing-py311  python=3.11\n\nconda activate dpk-html-processing-py311\n</code></pre> <p>If on linux do this</p> <pre><code>conda install -y gcc_linux-64\nconda install -y gxx_linux-64\n</code></pre> <p>Install modules</p> <pre><code>pip install -r requirements.txt \n</code></pre>"},{"location":"examples/notebooks/rag-html-1/#step-2-configuration","title":"Step-2: Configuration","text":"<p>Inspect configuration here: my_config.py</p> <p>Here you can set:</p> <ul> <li>site to crawl</li> <li>how many files to download and crawl depth</li> <li>embedding model</li> <li>LLM to use</li> </ul>"},{"location":"examples/notebooks/rag-html-1/#step-3-crawl-a-website","title":"Step-3: Crawl a website","text":"<p>This step will crawl a site and download HTML files in <code>input</code> directory</p> <p>1_crawl_site.ipynb</p>"},{"location":"examples/notebooks/rag-html-1/#step-4-process-html-files","title":"Step-4: Process HTML files","text":"<p>We will process downloaded HTML files and extract the text as markdown.  The output will be saved in the<code>output/2-markdown</code> directory in markdown format</p> <p>2_extract_text_from_html.ipynb</p>"},{"location":"examples/notebooks/rag-html-1/#step-5-save-data-into-db","title":"Step-5: Save data into DB","text":"<p>We will save the extracted text (markdown) into a vector database (Milvus)</p> <p>3_save_to_vector_db.ipynb</p>"},{"location":"examples/notebooks/rag-html-1/#step-6-query-documents","title":"Step-6: Query documents","text":""},{"location":"examples/notebooks/rag-html-1/#61-setup-env-file-with-api-token","title":"6.1 - Setup <code>.env</code> file with API Token","text":"<p>For this step, we will be using Replicate API service.  We need a Replicate API token for this step.</p> <p>Follow these steps:</p> <ul> <li>Get a free account at replicate</li> <li>Use this invite to add some credit  \ud83d\udcb0  to your Replicate account!</li> <li>Create an API token on Replicate dashboard</li> </ul> <p>Once you have an API token, add it to the project like this:</p> <ul> <li>Copy the file <code>env.sample.txt</code> into <code>.env</code>  (note the dot in the beginning of the filename)</li> <li>Add your token to <code>REPLICATE_API_TOKEN</code> in the .env file.</li> </ul>"},{"location":"examples/notebooks/rag-html-1/#62-query","title":"6.2 - Query","text":"<p>Query documents using LLM</p> <p>4_query.ipynb</p>"},{"location":"examples/notebooks/rag-pdf-1/","title":"RAG with Data Prep Kit","text":"<p>This folder has examples of RAG applications with data prep kit (DPK).</p>"},{"location":"examples/notebooks/rag-pdf-1/#step-0-getting-started","title":"Step-0: Getting Started","text":"<pre><code>## Clone this repo\ngit  clone   https://github.com/IBM/data-prep-kit\n\ncd data-prep-kit\n## All commands from here on out, assume you are in project root directory\n</code></pre>"},{"location":"examples/notebooks/rag-pdf-1/#step-1-setup-python-environment","title":"Step-1: Setup Python Environment","text":"<p>setup-python-dev-env.md</p> <p>Make sure Jupyter is running after this step.  We will use this Jupyter instance to run the notebooks in next steps.</p>"},{"location":"examples/notebooks/rag-pdf-1/#rag-workflow","title":"RAG Workflow","text":"<p>Here is the overall work flow.  For details see RAG-explained</p> <p></p>"},{"location":"examples/notebooks/rag-pdf-1/#step-2-process-input-documents-rag-stage-1-2-3-4","title":"Step-2: Process Input Documents (RAG stage 1, 2,  3 &amp; 4)","text":"<p>This code uses DPK to </p> <ul> <li>Extract text from PDFs (RAG stage-1)</li> <li>Performs de-dupes (RAG stage-2)</li> <li>split the documents into chunks (RAG stage-3)</li> <li>vectorize the chunks (RAG stage-4)</li> </ul> <p>Here is the code: </p> <ul> <li>Python version: rag_1_dpk_process_python.ipynb</li> <li>Ray version: rag_1A_dpk_process_ray.ipynb</li> </ul>"},{"location":"examples/notebooks/rag-pdf-1/#step-3-load-data-into-vector-database-rag-stage-5","title":"Step-3: Load data into vector database  (RAG stage 5)","text":"<p>Our vector database is Milvus</p> <p>Run the code: rag_2_load_data_into_milvus.ipynb</p> <p>Be sure to shutdown the notebook before proceeding to the next step</p>"},{"location":"examples/notebooks/rag-pdf-1/#step-4-perform-vector-search-rag-stage-6-7-8","title":"Step-4: Perform vector search (RAG stage  6, 7 &amp; 8)","text":"<p>Let's do a few searches on our data.</p> <p>Code: rag_3_vector_search.ipynb</p> <p>Be sure to shutdown the notebook before proceeding to the next step</p>"},{"location":"examples/notebooks/rag-pdf-1/#step-5-query-the-documents-using-llm-rag-steps-9-10","title":"Step-5: Query the documents using LLM (RAG steps  9 &amp; 10)","text":"<p>We will use Llama as our LLM running on Replicate service.</p>"},{"location":"examples/notebooks/rag-pdf-1/#51-create-an-env-file","title":"5.1 - Create an <code>.env</code> file","text":"<p>To use replicate service, we will need a replicate API token.</p> <p>You can get one from Replicate</p> <p>Create an <code>.env</code> file (notice the dot in the file name in this directory with content like this</p> <pre><code>REPLICATE_API_TOKEN=your REPLICATE token goes here\n</code></pre>"},{"location":"examples/notebooks/rag-pdf-1/#52-run-the-query-code","title":"5.2 - Run the query code","text":"<p>Code: rag_4_query_replicate.ipynb</p>"},{"location":"examples/notebooks/rag-pdf-1/#step-6-optional-illama-index","title":"Step 6 (Optional): Illama Index","text":"<p>For comparision, we can use Llama-index framework to process PDFs and query</p>"},{"location":"examples/notebooks/rag-pdf-1/#step-61-process-documents-and-save-the-index-into-vector-db","title":"Step 6.1 - Process documents and save the index into vector DB","text":"<p>Code: rag_llamaindex_1_process.ipynb</p> <p>Be sure to shutdown the notebook before proceeding to the next step</p>"},{"location":"examples/notebooks/rag-pdf-1/#step-62-query-documents-with-llm","title":"Step 6.2 - Query documents with LLM","text":"<p>code: rag_llamaindex_2_query.ipynb</p>"},{"location":"examples/notebooks/rag-pdf-1/#tips-close-the-notebook-kernels-to-release-the-dblock","title":"Tips: Close the notebook kernels, to release the db.lock","text":"<p>When using embedded database, the program maintains a lock on the database file.  If the lock is active, other notebooks may not be able to access the same database.</p> <p>Here is how to close kernels:</p> <ul> <li>In vscode:  Restart the kernel. This will end the current kernel session and release the db.lock</li> <li>In Jupyter : Go to  <code>File --&gt; Close and shutdown notebook</code>.  This will end the current kernel session and release the db.lock</li> </ul>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/","title":"RAG explained","text":""},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#rag-process-explained","title":"RAG Process Explained","text":"<p>RAG conists of two phases</p> <ul> <li>1 - Import phase:  Preparing and indexing the data  (the top row in the diagram)</li> <li>2 - Querying phase: query the data prepared in phase 1  (bottom row in the  diagram)</li> </ul> <p></p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-1-ingest-extract-text-from-pdfs","title":"Step 1 (Ingest): Extract text from PDFs","text":"<p>We will extract text in markdown format from PDFs.</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-2-ingest-perform-any-de-duplication","title":"Step 2 (Ingest): Perform any De duplication","text":"<p>Eliminate any duplicate documents.</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-3-ingest-split-into-chunks","title":"Step 3 (Ingest): Split into chunks","text":"<p>Split the documents into manageable chunks or segments. There are various chunking stratergies.  Documents can be split into pages or paragraphs or sections.  The right chunking strategy depends on the document types being processed</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-4-ingest-vectorize-calculate-embeddings","title":"Step 4 (Ingest): Vectorize / Calculate Embeddings","text":"<p>In order to make text searchable, we need to 'vectorize' them.  This is done by using embedding models.  We will feature a variety of embedding models, open source ones and API based ones.</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-5-ingest-saving-data-into-vector-database","title":"Step 5 (Ingest). Saving Data into Vector Database","text":"<p>In order to effectivly retrieve relevant documents, we use Milvus - a very popular open source, vector database.</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-6-query-vectorize-question","title":"Step 6 (Query). Vectorize Question","text":"<p>When user asks a question, we are going to vectorize the question so we can fetch documents that may have the answer question.</p> <p>For example, if a user asks a financial question ('how much was the revenue in 2022') the answer is most likely in financial documents, not in employee handbooks.</p> <p>So we want to retrieve the relevant documents first.</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-7-query-vector-search","title":"Step 7 (Query): Vector Search","text":"<p>We send the 'vectorized query' to vector database to retrieve the relevant documents.</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-8-query-retrieve-relevant-documents","title":"Step 8 (Query): Retrieve Relevant Documents","text":"<p>Vector database looks at our query (in vectorized form), searches through the documents and returns the documents matching the query.</p> <p>This is an important step, because it cuts down the 'search space'.  For example, if have 1000 pdf documents, and only a small number of documents might contain the answer, it returns the relevant documents.</p> <p>The search has to be accurate, as these are the documents sent to LLM as 'context'.  LLM will look through these documents searching for the answer to our question</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-9-query-send-relevant-documents-and-query-llm","title":"Step 9 (Query): Send relevant documents and query LLM","text":"<p>We send the relevant documents (returned in the above step by Vector DB) and our query to LLM.</p> <p>LLMs can be accessed via API or we can run one locally.</p>"},{"location":"examples/notebooks/rag-pdf-1/RAG-explained/#step-10-query-answer-from-llm","title":"Step 10 (Query): Answer from LLM","text":"<p>Now we get to see the answer provided by LLM \ud83d\udc4f</p>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/","title":"Setup a Local Python Dev Environment","text":"<p>We can use </p> <ul> <li>Option-A: Use Anaconda environment</li> <li>Option-B: Use python virtual env</li> </ul> <p>Just follow one.  (A) is recommended!</p>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#option-a-recommended-anaconda-python-environment","title":"Option A (Recommended): Anaconda Python environment","text":"<p>You can install Anaconda by following the guide here.</p> <p>We will create an environment for this workshop with all the required libraries installed.</p>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#a-1-setup-a-conda-env","title":"A-1: Setup a conda env","text":"<pre><code>conda create -n data-prep-kit-rag -y python=3.11\n</code></pre> <p>activate the new conda environment</p> <pre><code>conda activate data-prep-kit-rag\n</code></pre> <p>Make sure env is swithced to data-prep-kit-rag</p> <p>Check python version</p> <pre><code>python --version\n</code></pre> <p>should say : 3.11</p> <p>Note: If you are on a linux system install these too</p> <pre><code>conda install -y gcc_linux-64\n\nconda install -y gxx_linux-64\n</code></pre>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#a-2-install-dependencies","title":"A-2: Install dependencies","text":"<pre><code>cd examples/notebooks/rag-pdf-1\n</code></pre> <pre><code>pip  install  -r requirements.txt\n</code></pre> <p>If any issues see troubleshooting tips</p>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#a-3-start-jupyter","title":"A-3: Start Jupyter","text":"<p><code>jupyter lab</code></p> <p>This will usually open a browser window/tab.  We will use this to run the notebooks</p>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#option-b-python-virtual-env","title":"Option B: Python virtual env","text":""},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#b-1-have-python-version-311","title":"B-1: Have python version 3.11","text":"<pre><code>## Check python version\npython --version\n# should say : 3.11\n</code></pre>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#b-2-create-a-venv","title":"B-2: Create a venv","text":"<pre><code>cd examples/notebooks/rag\n\n\npython -m venv venv\n\n## activate venv\nsource ./venv/bin/activate\n\n## Install requirements\npip install -r requirements.txt\n</code></pre> <p>If any issues see troubleshooting tips</p>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#b-3-launch-jupyter","title":"B-3: Launch Jupyter","text":"<p><code>./venv/bin/jupyter lab</code></p> <p>This will usually open a browser window/tab.  We will use this to run the notebooks</p> <p>Note:: Make sure to run <code>./venv/bin/jupyter lab</code>, so it can load installed dependencies correctly.</p>"},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#troubleshooting-tips","title":"Troubleshooting Tips","text":""},{"location":"examples/notebooks/rag-pdf-1/setup-python-dev-env/#fasttext-compile-issue-with-gccg-compiler-version-13","title":"fasttext compile issue with GCC/G++ compiler version 13","text":"<p><code>pip install</code> may fail because one of the python dependencies, <code>fasttext==0.9.2</code> compiles with GCC/G++ version 11, not version 13.</p> <p>Here is how to fix this error:</p> <pre><code>## These instructions are for Ubuntu 22.04 and later\n\nsudo apt update\n\n## install GCC/G++ compilers version 11 \nsudo apt install -y gcc-11  g++-11\n\n## Verify installation\ngcc-11  --version\ng++-11  --version\n# should say 11\n\n## Set the compiler before doing pip install\nCC=gcc-11  pip install -r requirements.txt \n</code></pre>"},{"location":"kfp/","title":"Automation with Kubeflow Pipelines","text":""},{"location":"kfp/#map-betweens-transforms-and-kfp-pipelines","title":"Map betweens transforms and KFP pipelines","text":"Transform KFP pipeline language/lang_id lang_id_wf.py language/html2parquet html2parquet_wf.py code/malware malware_wf.py code/code2parquet code2parquet_wf.py code/code_quality code_quality_wf.py code/proglang_select proglang_select_wf.py code/license_select license_select_wf.py universal/doc_id doc_id_wf.py universal/ededup ededup_wf.py universal/fdedup fdedup_wf.py universal/filtering filter_wf.py universal/noop noop_wf.py universal/profiler profiler_wf.py universal/tokenization tokenization_wf.py universal/hap hap_wf.py"},{"location":"kfp/#set-up-and-working-steps","title":"Set up and working steps","text":"<ul> <li>Set up a Kubernetes clusters for KFP execution</li> <li>Simple Transform pipeline tutorial</li> <li>Execution several transformers</li> <li>Clean up the cluster</li> </ul>"},{"location":"kfp/RELEASE/","title":"Release Process","text":"<p>This document describes the release process for the following components:</p> <ul> <li> <p><code>kfp_support_lib</code> Python packages in <code>kfp_support_lib</code> directory.</p> </li> <li> <p><code>kfp-data-processing</code> docker image built based on the Docker file in <code>kfp_ray_components</code> directory.</p> </li> <li> <p>kubeflow pipelines in <code>transform_workflows</code> directory. For example the one that is generated from <code>transform_workflows/universal/noop/noop_wf.py</code> file.</p> </li> </ul>"},{"location":"kfp/RELEASE/#1-update-makeversions-file","title":"1. Update <code>.make.versions</code> file","text":"<p>The .make.versions file specifies the target versions for the building components, as well as the  desired versions for the dependencies.  The KFP package build uses the following variables from the file: - RELEASE_VERSION_SUFFIX - the common suffix for all building components - DPK_LIB_KFP_VERSION - the version of <code>kfp_v1_workflow_support</code> - DPK_LIB_KFP_VERSION_v2 - the version of <code>kfp_v2_workflow_support</code> - DPK_LIB_KFP_SHARED - the version of <code>kfp_shared_workflow_support</code></p> <p>Note: The docker images are dependent on the libraries but use the python source code from the repository, so inorder  to build docker images, the python modules (libraries) do not have to be deployed. </p>"},{"location":"kfp/RELEASE/#2-choose-the-supported-kfp-version","title":"2. Choose the supported KFP version.","text":"<p>The docker images and some <code>workflow_support</code> libraries depend on KFP version. In order to build images and libraries for KFP v2, run the following command:</p> <pre><code>export KFPv2=1\n</code></pre>"},{"location":"kfp/RELEASE/#3-optional-build-the-library","title":"3. (Optional) Build the library","text":"<p>Run the <code>make -C shared_workflow_support build</code> command to build the shared library. If you need a library for KFPv1 Run <code>make -C kfp_v1_workflow_support build</code> For KFP v2 set the environment variable <code>KFPv2</code>, se above, and run <code>make -C kfp_v2_workflow_support build</code></p>"},{"location":"kfp/RELEASE/#4-optional-publish-the-library","title":"4. (Optional) Publish the library","text":"<p>Run <code>make -C shared_workflow_support publish</code>, and either <code>make -C kfp_v1_workflow_support publish</code> or  <code>make -C kfp_v2_workflow_support publish</code>command to push the libraries to the TestPyPI repository.</p>"},{"location":"kfp/RELEASE/#5-build-the-image","title":"5. Build the image","text":"<p>Run <code>make -C kfp_ray_components build</code> command to build the <code>kfp-data-processing</code> docker image, or <code>kfp-data-processing_v2</code>, when <code>KFPv2==1</code></p>"},{"location":"kfp/RELEASE/#5-push-the-image","title":"5. Push the image","text":"<p>Run <code>make -C kfp_ray_components publish</code> command to push the docker image.</p>"},{"location":"kfp/doc/deployment_on_MacOS/","title":"Development and Testing on macOS","text":"<p>Users can develop and test the system locally. This can be achieved by executing a local kind  Kubernetes cluster. On Linux or Windows (WSL) operating systems, users can utilize  Docker Engine. However, Docker on macOS necessitates  Docker Desktop, which for  commercial usage requires a paid subscription. Alternatively, Podman can serve as a free substitute. This document provides instructions for installing and  configuring Podman.</p>"},{"location":"kfp/doc/deployment_on_MacOS/#podman-installation","title":"Podman Installation","text":"<ul> <li>Download and install Podman from its website. (It can be either Podman Desktop or just Podman CLI)</li> <li>Running this project with KubeFlow pipelines  requires more CPU and Memory than what the default Podman virtual machine provides. Therefore, its settings should be  adjusted.</li> <li>For Podman CLI execute <code>podman machine init --cpus 5 --memory 61440</code></li> <li>For Podman Desktop, open the \"Settings\" and set required CPUS and Memory.  </li> <li>Execute <code>make setup</code> from the <code>scripts/k8s-setup</code> directory.</li> <li>The installation of Kubeflow pipelines takes time, and some of its pods may experience crashes during startup.    Therefore, we kindly ask for your patience and to wait until ALL pods are in the Ready state. </li> </ul>"},{"location":"kfp/doc/multi_transform_pipeline/","title":"Run several transforms","text":"<p>In a common scenario, users want to run a pipeline or a series of several transforms one after the other. Here we can show how to execute several transforms.</p> <p>Let's assume that the input folder path is <code>path/to/input</code> and a user wants to save the output of the data after several transforms in <code>path/to/output</code>. Let's assume also that the user wants to apply the transform <code>A</code> then the transform <code>B</code> on his data.</p> <p>In Executing pipeline and watching execution results you can learn how to run a KFP pipeline of one transform. We want to use the same method to combine several transform pipelines together.</p> <p>Let's start with transform A (in this example it is exact deduplication). After uploading the pipeline you can create a run from this page:</p> <p></p> <p>After clicking <code>create run</code> a list of input parameters is shown on the screen. In this document we want to deal with the <code>data_s3_config</code> input parameter. This parameter specifies the input folder path and output folder path. For the transform A we should have <code>input_folder = path/to/input</code> and the <code>output_folder=/path/to/&lt;B&gt;_input</code> which is an intermediate folder that we will use as an input for the next transform.</p> <p></p> <p>After completing the first transformation, we can continue to the next one. As in the previous step, we update the pipeline input parameters for the transformer <code>B</code> (in this example it is fuzzy deduplication) and create a Run.</p> <p></p> <p>In the list of its input parameters, we also see <code>data_s3_config</code>. Now, we have <code>input_folder = path/to/B_input</code> (the output folder of the previous transformation pipeline) and <code>output_folder=/path/to/output</code>, the desired output folder for the whole task. if we want to execute several transformation pipelines, we have to define more intermediate folders.</p> <p></p>"},{"location":"kfp/doc/multi_transform_pipeline/#examples","title":"Examples","text":"<p>The sections that follow display two super pipelines as examples:</p> <p>1) dedups super pipeline 1) programming languages super pipeline</p>"},{"location":"kfp/doc/multi_transform_pipeline/#dedups-super-pipeline","title":"Dedups super pipeline","text":"<p>This pipeline combines several transforms, <code>doc_id</code>, <code>ededup</code>, and <code>fdedup</code>, can be found in superworkflow_dedups_sample_wf.py.</p> <p></p> <p>The input parameters of the super pipelines are described in this section.</p>"},{"location":"kfp/doc/multi_transform_pipeline/#programming-languages-super-pipeline","title":"Programming languages super pipeline","text":"<p>This pipeline combines transforms for programming languages data preprocessing: <code>ededup</code>, <code>doc_id</code>, <code>fdedup</code>, <code>proglang_select</code>, <code>code_quality</code>,  <code>malware</code> and <code>tokenization</code>. It can be found in superworkflow_code_wf.py.</p> <p></p> <p>The input parameters of the super pipelines are described in this section.</p>"},{"location":"kfp/doc/multi_transform_pipeline/#super-pipeline-input-parameters","title":"Super Pipeline Input Parameters","text":"<p>There are several <code>groups</code> of input parameters for super pipelines, each group of parameters has a prefix of \"p_\" string, where  is an order number. <p>\"p1_\" prefix group of parameters:</p> <ul> <li>The pipeline names of the tasks that form the super pipeline.</li> </ul> <p>\"p2_\" group:</p> <ul> <li> <p>parameters of the ray cluster (head and workers options) that are common to all tasks.</p> </li> <li> <p>input/output paths: There are three parameters that specify the input and output paths for all the pipeline steps. The parameters are <code>p2_pipeline_input_parent_path</code>, <code>p2_pipeline_output_parent_path</code> <code>p2_pipeline_parent_path_suffix</code>.</p> </li> </ul> <p>The input of the first step is <code>p2_pipeline_input_parent_path</code> + <code>/</code> + <code>p2_pipeline_parent_path_suffix</code>.</p> <pre><code>The output of step S is: `p2_pipeline_output_parent_path` + `_&lt;task1Name&gt;_\u2026_&lt;taskSName&gt;` + `/` + `p2_pipeline_parent_path_suffix`.\n\nThe output of step S will be the input of the step S+1 (the next step).\n</code></pre> <ul> <li><code>p2_pipeline_additional_params</code>: it is a json string that includes a several parameters that passed to all steps of the super pipeline (similar to default values that can be overwritten in each step).</li> </ul> <p><code>p3_</code> to <code>p&lt;x&gt;_</code>: </p> <ul> <li> <p>Each group of <code>p&lt;s&gt;_</code> is a list of parameters of the step <code>&lt;s&gt;</code>. It includes a</p> <ul> <li> <p>Step name that is used in the output paths.</p> </li> <li> <p>Several parameters that pass to the step pipeline.</p> </li> <li> <p>Skip parameter: if <code>True</code> then skip this step.</p> </li> <li> <p>JSON string parameter named <code>overriding_params</code> that can be used to modify <code>p2_</code> prefixed input pipeline parameters and add additional inputs that are not listed as separate input parameters to the step pipeline.  Note: if there are Boolean parameters it should be specified as a string value of <code>\"True\"</code> or <code>\"False\"</code> (with quotation mark).</p> </li> </ul> </li> </ul>"},{"location":"kfp/doc/setup/","title":"Set up a Kubernetes clusters for KFP execution","text":""},{"location":"kfp/doc/setup/#table-of-contents","title":"\ud83d\udcdd Table of Contents","text":"<ul> <li>A Kind deployment supported platforms</li> <li>Preinstalled software components</li> <li>A Kind deployment</li> <li>An existing cluster</li> <li>Installation steps</li> <li>Installation on an existing Kubernetes cluster</li> <li>Clean up the cluster</li> </ul> <p>The project provides instructions and deployment automation to run all components in an all-inclusive fashion on a  single machine using a Kind cluster and a local data storage (MinIO). However, this topology is not suitable for processing medium and large datasets, and deployment should be carried out  on a real Kubernetes or OpenShift cluster. Therefore, we recommend using Kind cluster for only for local testing and  debugging, not production loads. For production loads use a real Kubernetes cluster.</p> <p>Running a Kind Kubernetes cluster with Kubeflow pipelines (KFP) and MinIO requires significant memory. We recommend deploying it on machines with at least 32 GB of RAM and 8-9 CPU cores. RHEL OS requires  more resources, e.g. 64 GB RAM and 32 CPU cores.</p>"},{"location":"kfp/doc/setup/#a-kind-deployment-supported-platforms","title":"A Kind deployment supported Platforms","text":"<p>Executing KFP, MinIO, and Ray on a single Kind cluster pushes the system to its load limits. Therefore, although we are  working on extending support for additional platforms, not all platforms/configurations are currently supported.</p> Operating System Container Agent Support Comments RHEL 7 any - Kind doesn't support RHEL 7 RHEL 8 RHEL 9.4 Docker Yes RHEL 9.4 Podman No Issues with Ray job executions Ubuntu 24-04 Docker Yes Ubuntu 24-04 Podman Windows WSL2 Docker Yes Windows WSL2 Podman MacOS amd64 Docker Yes MacOS amd64 Podman MacOS arm64 Docker MacOS arm64 Podman No Issues with Ray job executions"},{"location":"kfp/doc/setup/#preinstalled-software-components","title":"Preinstalled software components","text":"<p>Depending on whether a Kind cluster or an existing Kubernetes cluster is used, different software packages need to be preinstalled.</p>"},{"location":"kfp/doc/setup/#kind-deployment","title":"Kind deployment","text":"<p>The following programs should be manually installed:</p> <ul> <li>Helm 3.10.0 or greater must be installed and configured on your machine.</li> <li>Kind tool for running local Kubernetes clusters 0.14.0 or newer must be installed on your machine.</li> <li>Kubectl 1.26 or newer must be installed on your machine.</li> <li>MinIO Client (mc) must be installed on your machine. Please  choose your OS system, and process according to \"(Optional) Install the MinIO Client\". You have to install the <code>mc</code> client only.</li> <li>git client, we use git client to clone installation repository</li> <li>lsof usually it is part of Linux or MacOS distribution.</li> <li>Container agent such as Docker or Podman</li> </ul>"},{"location":"kfp/doc/setup/#existing-kubernetes-cluster","title":"Existing Kubernetes cluster","text":"<p>Deployment on an existing cluster requires less pre-installed software Only the following programs should be manually installed:</p> <ul> <li>Helm 3.10.0 or greater must be installed and configured on your machine.</li> <li>Kubectl 1.26 or newer must be installed on your machine, and be  able to connect to the external cluster.</li> <li>Deployment of the test data requires MinIO Client (mc) Please  choose your OS system, and process according to \"(Optional) Install the MinIO Client\". Only the <code>mc</code> client should be installed.</li> </ul>"},{"location":"kfp/doc/setup/#installation-steps","title":"Installation steps","text":"<p>Before installation, you have to decide which KFP version do you want to use.  In order to use KFP v2, please set the following environment variable:</p> <pre><code>export KFPv2=1\n</code></pre> <p>Now, you can create a Kind cluster with all required software installed using the following command: </p> <p><pre><code> make -C scripts/k8s-setup setup\n</code></pre> from this main package directory. If you do not want to upload the testing data into the locally deployed Minio, and reduce memory footprint, please set: <pre><code>export POPULATE_TEST_DATA=0\n</code></pre> You can access the KFP dashboard at http://localhost:8080/ and the MinIO dashboard at http://localhost:8090/</p>"},{"location":"kfp/doc/setup/#installation-on-an-existing-kubernetes-cluster","title":"Installation on an existing Kubernetes cluster","text":"<p>Alternatively you can deploy pipeline to the existing Kubernetes cluster. </p> <p>In order to execute data transformers on the remote Kubernetes cluster, the following packages should be installed on the cluster:</p> <ul> <li>KubeFlow Pipelines (KFP). Currently, we use  upstream Argo-based KFP v1.</li> <li>KubeRay controller and  KubeRay API Server </li> </ul> <p>You can install the software from their repositories, or you can use our installation scripts.</p> <p>Once your local kubectl is configured to connect to the external cluster do the following: <pre><code>export EXTERNAL_CLUSTER=1\nmake -C scripts/k8s-setup setup\n</code></pre></p> <ul> <li> <p>In addition, you should configure external access to the KFP UI (<code>svc/ml-pipeline-ui</code> in the <code>kubeflow</code> ns) and the Ray  Server API (<code>svc/kuberay-apiserver-service</code> in the <code>kuberay</code> ns). Depends on your cluster and its deployment it can be  LoadBalancer services, Ingresses or Routes. </p> </li> <li> <p>Optionally, you can upload the test data into the MinIO Object Store, deployed as part of KFP. In  order to do this, please provide external access to the Minio (<code>svc/minio-service</code> in the <code>kubeflow</code> ns) and execute the  following commands from the root directory:  <pre><code>export MINIO_SERVER=&lt;Minio external URL&gt;\nkubectl apply -f scripts/k8s-setup/s3_secret.yaml\nscripts/k8s-setup/populate_minio.sh\n</code></pre></p> </li> </ul>"},{"location":"kfp/doc/setup/#clean-up-the-cluster","title":"Clean up the cluster","text":"<p>If you use an external Kubernetes cluster set the <code>EXTERNAL_CLUSTER</code> environment variable.</p> <p><pre><code>export EXTERNAL_CLUSTER=1\n</code></pre> Now, you can cleanup the external or Kind Kubernetes clusters by running the following command:</p> <pre><code>make -C scripts/k8s-setup clean\n</code></pre>"},{"location":"kfp/doc/simple_transform_pipeline/","title":"Simplest Transform pipeline tutorial","text":"<p>In this example, we implement a pipeline to automate execution of the simple  noop transform</p> <p>In this tutorial, we will show the following:</p> <ul> <li>How to write the <code>noop</code> transform automation pipeline, leveraging KFP components.</li> <li>How to compile a pipeline and deploy it to KFP</li> <li>How to execute pipeline and view execution results</li> </ul> <p>Note: the project and the explanation below are based on KFPv1</p> <p>Note: For details on using the HugginFace token in a pipeline for relevant transforms,  please refer to the HugginFace token section.</p>"},{"location":"kfp/doc/simple_transform_pipeline/#table-of-contents","title":"\ud83d\udcdd Table of Contents","text":"<ul> <li>Implementing pipeline</li> <li>Imports definition </li> <li>Components definition</li> <li>Input parameters definition</li> <li>Pipeline definition</li> <li>Additional configuration</li> <li>Tolerations and node selector</li> <li>Compiling a pipeline</li> <li>Deploying a pipeline</li> <li>Executing pipeline and watching execution results</li> <li>Clean up the cluster</li> </ul>"},{"location":"kfp/doc/simple_transform_pipeline/#implementing-pipeline","title":"Implementing pipeline","text":"<p>Overall implementation roughly contains 5 major sections:</p> <ul> <li>Imports</li> <li>Components definition - definition of the main steps of our pipeline</li> <li>Input parameters definition </li> <li>Pipeline wiring - definition of the sequence of invocation (with parameter passing) of participating components</li> <li>Additional configuration</li> </ul>"},{"location":"kfp/doc/simple_transform_pipeline/#imports-definition","title":"Imports definition","text":"<pre><code>import kfp.compiler as compiler\nimport kfp.components as comp\nimport kfp.dsl as dsl\nimport os\nfrom kfp_support.workflow_support.runtime_utils import (\n  DEFAULT_KFP_COMPONENT_SPEC_PATH,\n  ONE_HOUR_SEC,\n  ONE_WEEK_SEC,\n  ComponentUtils,\n)\nfrom kubernetes import client as k8s_client\n</code></pre>"},{"location":"kfp/doc/simple_transform_pipeline/#components-definition","title":"Components definition","text":"<p>Our pipeline includes 4 steps - compute execution parameters, create Ray cluster, submit and watch Ray job, clean up  Ray cluster. For each step we have to define a component that will execute them:</p> <p><pre><code>    # components\n    base_kfp_image = \"quay.io/dataprep1/data-prep-kit/kfp-data-processing:latest\"\n    component_spec_path = os.getenv(\"KFP_COMPONENT_SPEC_PATH\", DEFAULT_KFP_COMPONENT_SPEC_PATH)\n    # KFPv1 and KFP2 uses different methods to create a component from a function. KFPv1 uses the\n    # `create_component_from_func` function, but it is deprecated by KFPv2 and so has a different import path.\n    # KFPv2 recommends using the `@dsl.component` decorator, which doesn't exist in KFPv1. Therefore, here we use\n    # this if/else statement and explicitly call the decorator.\n    if os.getenv(\"KFPv2\", \"0\") == \"1\":\n      compute_exec_params_op = dsl.component_decorator.component(\n                func=compute_exec_params_func, base_image=base_kfp_image\n                )\n    else:\n      compute_exec_params_op = comp.create_component_from_func(func=compute_exec_params_func, base_image=base_kfp_image)\n    # create Ray cluster\n    create_ray_op = comp.load_component_from_file(component_spec_path + \"createRayClusterComponent.yaml\")\n    # execute job\n    execute_ray_jobs_op = comp.load_component_from_file(component_spec_path + \"executeRayJobComponent.yaml\")\n    # clean up Ray\n    cleanup_ray_op = comp.load_component_from_file(component_spec_path + \"deleteRayClusterComponent.yaml\")\n    # Task name is part of the pipeline name, the ray cluster name and the job name in DMF.\n    TASK_NAME: str = \"noop\"\n</code></pre> Note: here we are using shared components described in this document for <code>create_ray_op</code>,  <code>execute_ray_jobs_op</code> and <code>cleanup_ray_op</code>,  while <code>compute_exec_params_op</code> component is built inline, because it might differ significantly. For \"simple\" pipeline cases we can use the  default implementation, while, for example for exact dedup, we are using a very specialized one.</p>"},{"location":"kfp/doc/simple_transform_pipeline/#input-parameters-definition","title":"Input parameters definition","text":"<p>The input parameters section defines all the parameters required for the pipeline execution:</p> <pre><code>    # Ray cluster\n    ray_name: str = \"noop-kfp-ray\",  # name of Ray cluster\n    ray_run_id_KFPv2: str = \"\",\n    ray_head_options: str = '{\"cpu\": 1, \"memory\": 4, \\\n                 \"image\": \"' + task_image + '\" }',\n    ray_worker_options: str = '{\"replicas\": 2, \"max_replicas\": 2, \"min_replicas\": 2, \"cpu\": 2, \"memory\": 4, \\\n                \"image\": \"' + task_image + '\" }',\n    server_url: str = \"http://kuberay-apiserver-service.kuberay.svc.cluster.local:8888\",\n    # data access\n    data_s3_config: str = \"{'input_folder': 'test/noop/input/', 'output_folder': 'test/noop/output/'}\",\n    data_s3_access_secret: str = \"s3-secret\",\n    data_max_files: int = -1,\n    data_num_samples: int = -1,\n    data_checkpointing: bool = False,\n    # orchestrator\n    actor_options: str = \"{'num_cpus': 0.8}\",\n    pipeline_id: str = \"pipeline_id\",\n    code_location: str = \"{'github': 'github', 'commit_hash': '12345', 'path': 'path'}\",\n    # noop parameters\n    noop_sleep_sec: int = 10,\n    # additional parameters\n    additional_params: str = '{\"wait_interval\": 2, \"wait_cluster_ready_tmout\": 400, \"wait_cluster_up_tmout\": 300, \"wait_job_ready_tmout\": 400, \"wait_print_tmout\": 30, \"http_retries\": 5, \"delete_cluster_delay_minutes\": 0}',\n</code></pre> <p>The parameters used here are as follows:</p> <ul> <li>ray_name: name of the Ray cluster</li> <li>ray_run_id_KFPv2: Ray cluster unique ID used only in KFP v2</li> <li>ray_head_options: head node options, containing the following:</li> <li>cpu - number of cpus</li> <li>memory - memory</li> <li>image - image to use</li> <li>image_pull_secret - image pull secret</li> <li>tolerations - (optional) tolerations for the ray pods</li> <li>ray_worker_options: worker node options (we here are using only 1 worker pool), containing the following:</li> <li>replicas - number of replicas to create</li> <li>max_replicas - max number of replicas</li> <li>min_replicas - min number of replicas</li> <li>cpu - number of cpus</li> <li>memory - memory</li> <li>image - image to use</li> <li>image_pull_secret - image pull secret</li> <li>tolerations - (optional) tolerations for the ray pods</li> <li>server_url - server url</li> <li>additional_params: additional (support) parameters, containing the following:</li> <li>wait_interval - wait interval for API server, sec</li> <li>wait_cluster_ready_tmout - time to wait for cluster ready, sec</li> <li>wait_cluster_up_tmout - time to wait for cluster up, sec</li> <li>wait_job_ready_tmout - time to wait for job ready, sec</li> <li>wait_print_tmout - time between prints, sec</li> <li>http_retries - http retries for API server calls</li> <li>data_s3_access_secret - s3 access secret</li> <li>data_s3_config - s3 configuration</li> <li>data_max_files - max files to process</li> <li>data_num_samples - num samples to process</li> <li>actor_options - actor options</li> <li>pipeline_id - pipeline id</li> <li>code_location - code location</li> <li>noop_sleep_sec - noop sleep time</li> </ul> <p>Note that here we are specifying initial values for all parameters that will be propagated to the workflow UI (see below)</p>"},{"location":"kfp/doc/simple_transform_pipeline/#pipeline-definition","title":"Pipeline definition","text":"<p>Now, when all components and input parameters are defined, we can implement pipeline wiring defining sequence of  component execution and parameters submitted to every component. </p> <pre><code>    # In KFPv2 dsl.RUN_ID_PLACEHOLDER is deprecated and cannot be used since SDK 2.5.0. On another hand we cannot create\n    # a unique string in a component (at runtime) and pass it to the `clean_up_task` of `ExitHandler`, due to\n    # https://github.com/kubeflow/pipelines/issues/10187. Therefore, meantime the user is requested to insert\n    # a unique string created at run creation time.\n    if os.getenv(\"KFPv2\", \"0\") == \"1\":\n        print(\"WARNING: the ray cluster name can be non-unique at runtime, please do not execute simultaneous Runs of the \"\n              \"same version of the same pipeline !!!\")\n        run_id = ray_run_id_KFPv2\n    else:\n        run_id = dsl.RUN_ID_PLACEHOLDER\n    # create clean_up task\n    clean_up_task = cleanup_ray_op(ray_name=ray_name, run_id=run_id, server_url=server_url, additional_params=additional_params)\n    ComponentUtils.add_settings_to_component(clean_up_task, ONE_HOUR_SEC * 2)\n    # pipeline definition\n    with dsl.ExitHandler(clean_up_task):\n      # compute execution params\n      compute_exec_params = compute_exec_params_op(\n          worker_options=ray_worker_options,\n          actor_options=runtime_actor_options,\n          data_s3_config=data_s3_config,\n          data_max_files=data_max_files,\n          data_num_samples=data_num_samples,\n          data_checkpointing=data_checkpointing,\n          runtime_pipeline_id=runtime_pipeline_id,\n          runtime_job_id=run_id,\n          runtime_code_location=runtime_code_location,\n          noop_sleep_sec=noop_sleep_sec,\n      )\n      ComponentUtils.add_settings_to_component(compute_exec_params, ONE_HOUR_SEC * 2)\n      # start Ray cluster\n      ray_cluster = create_ray_op(\n        ray_name=ray_name,\n        run_id=run_id,\n        ray_head_options=ray_head_options,\n        ray_worker_options=ray_worker_options,\n        server_url=server_url,\n        additional_params=additional_params,\n      )\n      ComponentUtils.add_settings_to_component(ray_cluster, ONE_HOUR_SEC * 2)\n      ray_cluster.after(compute_exec_params)\n      # Execute job\n      execute_job = execute_ray_jobs_op(\n        ray_name=ray_name,\n        run_id=run_id,\n        additional_params=additional_params,\n        # note that the parameters below are specific for NOOP transform\n        exec_params={\n          \"data_s3_config\": data_s3_config,\n          \"data_max_files\": data_max_files,\n          \"data_num_samples\": data_num_samples,\n          \"num_workers\": compute_exec_params.output,\n          \"worker_options\": actor_options,\n          \"pipeline_id\": pipeline_id,\n          \"job_id\": run_id,\n          \"code_location\": code_location,\n          \"noop_sleep_sec\": noop_sleep_sec,\n        },\n        exec_script_name=EXEC_SCRIPT_NAME,\n        server_url=server_url,\n      )\n      ComponentUtils.add_settings_to_component(execute_job, ONE_WEEK_SEC)\n      ComponentUtils.set_s3_env_vars_to_component(execute_job, data_s3_access_secret)\n      execute_job.after(ray_cluster)\n</code></pre> <p>Here we first create <code>cleanup_task</code> and the use it as an  exit handler which will be  invoked either the steps into it succeeded or failed.</p> <p>Then we create each individual component passing it required parameters and specify execution sequence, for example (<code>ray_cluster.after(compute_exec_params)</code>).</p>"},{"location":"kfp/doc/simple_transform_pipeline/#additional-configuration","title":"Additional configuration","text":"<p>The final thing that we need to do is set some pipeline global configuration:</p> <pre><code>    # Configure the pipeline level to one week (in seconds)\n    dsl.get_pipeline_conf().set_timeout(ONE_WEEK_SEC)\n</code></pre>"},{"location":"kfp/doc/simple_transform_pipeline/#kfp-pods-toleration-and-node-selector-optional","title":"KFP pods Toleration and node selector (Optional)","text":"<p>To apply kuberenetes Tolerations or nodeSelector to KFP pods, you need to set <code>KFP_TOLERATIONS</code> or <code>KFP_NODE_SELECTOR</code> environment variables respectively before compiling the pipeline. Here's an example:</p> <p><pre><code>export KFP_TOLERATIONS='[{\"key\": \"key\",\"operator\": \"Equal\", \"value1\": \"value\", \"effect\": \"NoSchedule\"}]'\n\nexport KFP_NODE_SELECTOR='{\"label_key\":\"cloud.google.com/gke-accelerator\",\"label_value\":\"nvidia-tesla-p4\"}'\n</code></pre> In KFP v1, setting <code>KFP_TOLERATIONS</code> will apply to the Ray pods, overriding any tolerations specified in the <code>ray_head_options</code> and <code>ray_worker_options</code> pipeline parameters if they are present.</p>"},{"location":"kfp/doc/simple_transform_pipeline/#compiling-a-pipeline","title":"Compiling a pipeline","text":"<p>To compile pipeline execute <code>make workflow-build</code> command in the same directory where your pipeline is. </p>"},{"location":"kfp/doc/simple_transform_pipeline/#deploying-a-pipeline","title":"Deploying a pipeline","text":"<p>Prepare local Kind or external Kubernetes cluster as described in Set Up a cluster</p> <p>Once the cluster is up, go to the kfp endpoint (<code>localhost:8080/kfp/</code> for Kind cluster, the end point of the external existing  cluster depends on the KFP end-point configuration), which will bring up  KFP UI, see below:</p> <p></p> <p>Click on the <code>Upload pipeline</code> link and follow instructions on the screen to upload your file (<code>noop_wf.yaml</code>) and name pipeline noop. Once this is done, you should see something as follows:</p> <p></p>"},{"location":"kfp/doc/simple_transform_pipeline/#executing-pipeline-and-watching-execution-results","title":"Executing pipeline and watching execution results","text":"<p>Before we can run the pipeline we need to create required secrets (one for image loading in case of secured  image registry and one for S3 access). As KFP is deployed in <code>kubeflow</code> namespace, workflow execution will happen there as well, which means that secrets have to be created there as well.</p> <p>When the MinIO Object Store, deployed as part of KFP, is used, its access secret is deployed as part of the cluster preparation,  see s3_secret.yaml.  Creation a secret to pull images from a private repository described here</p> <p>Once this is done we can execute the workflow. </p> <p>On the pipeline page (above) click on the <code>create run</code> button. You will see the list of the parameters, that you can redefine or use the default values that we specified above. After that, go to the bottom of the page and click the <code>start</code> button</p> <p>This will start workflow execution. Once it completes you will see something similar to below </p> <p></p> <p>Note that the log (on the left) has the complete execution log.</p> <p>Additionally, the log is saved to S3 (location is denoted but the last line in the log)</p>"},{"location":"kfp/doc/simple_transform_pipeline/#clean-up-the-cluster","title":"Clean up the cluster","text":"<p>The cluster clean up is described at Clean up the cluster</p>"},{"location":"kfp/doc/simple_transform_pipeline/#using-hugginface-token","title":"Using HugginFace token","text":"<p>This section explains how to use the HugginFace token for transforms that require it. To prevent exposing the token in the pipeline, it is assumed that the token is stored as a kubernetes secret in the namespace where the pipeline runs. To support that, the transform pipeline should include the following code after the Components definition section:</p> <pre><code>from python_apiserver_client.params import (\n    EnvVarFrom,\n    EnvironmentVariables,\n    EnvVarSource,\n)\n\n# The name of the secret that holds the HugginFace token\nHF_SECRET = \"hf-secret\"\n# The secret key that holds the HugginFace token\nHF_SECRET_KEY = \"hf-token\"\n# HuggingFace token is exported as environment variables in Ray node pods.\nenv_v = EnvVarFrom(source=EnvVarSource.SECRET, name=HF_SECRET, key=HF_SECRET_KEY)\nenvs = EnvironmentVariables(from_ref={\"HF_READ_ACCESS_TOKEN\": env_v})\n</code></pre> <p>In addition, <code>\"environment\": envs.to_dict()</code> should be added to <code>ray_head_options</code> and <code>ray_worker_options</code> in Input parameters definition section. For example, for <code>ray_head_options</code>: <pre><code>ray_head_options: dict = {\"cpu\": 1, \"memory\": 4, \"image\": task_image, \"environment\": envs.to_dict()},\n</code></pre></p> <p>and for <code>ray_worker_options</code>: <pre><code>ray_worker_options: dict = {\n        \"replicas\": 2,\n        \"max_replicas\": 2,\n        \"min_replicas\": 2,\n        \"cpu\": 2,\n        \"memory\": 4,\n        \"image\": task_image,\n        \"environment\": envs.to_dict(),\n    },\n</code></pre></p> <p>Before running the pipeline, create a secret named <code>hf-secret</code> as shown below, ensuring that the <code>HF_READ_ACCESS_TOKEN</code> environment variable is defined first, holding the HugginFace token.</p> <p><pre><code>export HF_READ_ACCESS_TOKEN=&lt;HugginFace token&gt;\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: hf-secret\n  namespace: kubeflow\ntype: Opaque\nstringData:\n      hf-token: \"${HF_READ_ACCESS_TOKEN}\"\n</code></pre></p>"},{"location":"kfp/kfp_ray_components/","title":"KFP components","text":""},{"location":"kfp/kfp_ray_components/#kfp-components_1","title":"KFP components","text":"<p>All data processing pipelines have the same <code>shape</code>. They all compute execution parameters, create Ray cluster, execute Ray job and then delete the cluster. With the exception of computing execution parameters all of the steps, although receiving different parameters are identical.</p> <p>To simplify implementation of the data processing KFP pipelines, this directory provides several components.</p> <p>As defined by KFP documentation <pre><code>A pipeline component is a self-contained set of code that performs one step in a workflow. \n</code></pre></p> <p>The first step in creation of components its implementation. The framework automation includes the following 3 components:</p> <ul> <li>Create Ray cluster is responsible for creation of the Ray cluster. Its implementation is    based on the RayRemoteJobs class</li> <li>execute Ray job is responsible for submission of the Ray job, watching its execution,   periodically printing job execution log and completing, once the job is completed. Its implementation is   based on the RayRemoteJobs class</li> <li>clean up Ray cluster is responsible for deletion of the Ray cluster, thus freeing   up cluster resources. Its implementation is based on the    RayRemoteJobs class</li> </ul> <p>Once the components are implemented we also implement their interfaces as a component specification which defines:</p> <ul> <li>The component\u2019s inputs and outputs.</li> <li>The container image that your component\u2019s code runs in, the command to use to run your component\u2019s code, and the  command-line arguments to pass to your component\u2019s code.</li> <li>The component\u2019s metadata, such as the name and description.</li> </ul> <p>Components specifications are provided here:</p> <ul> <li>Create Ray cluster Component</li> <li>execute Ray job component</li> <li>clean up Ray cluster component</li> </ul>"},{"location":"kfp/kfp_ray_components/#building-the-docker-image","title":"Building the docker image","text":"<p>To build the component docker image first execute the following commands to set the details of the docker registry as environment variables:</p> <pre><code>export DOCKER_SERVER=&lt;&gt; # for example us.icr.io \nexport DOCKER_USERNAME=iamapikey\nexport DOCKER_EMAIL=iamapikey\nexport DOCKER_PASSWORD=&lt;PASSWORD&gt;\n</code></pre> <p>Then build the image:</p> <pre><code>make build\nmake publish\n</code></pre>"},{"location":"kfp/kfp_support_lib/","title":"KFP support library","text":"<p>This provides support for implementing KFP pipelines automating transform's execution. It comprises 3 main modules</p> <ul> <li>shared_workflow_support </li> <li>kfp_v1_workflow_support</li> <li>kfp_v2_workflow_support</li> </ul> <p>Depends on the using KFV version either <code>kfp_v1_workflow_support</code> or <code>kfp_v2_workflow_support</code> should be used.</p> <p>See also how these libraries are used for kfp components implementation and implementation of the actual workflow</p>"},{"location":"kfp/kfp_support_lib/#development","title":"Development","text":""},{"location":"kfp/kfp_support_lib/#requirements","title":"Requirements","text":"<ol> <li>python 3.10 or later</li> <li>git command line tools</li> <li>pre-commit</li> <li>twine (pip install twine)<ul> <li>but on Mac you may have to include a dir in your PATH, such as <code>export PATH=$PATH:/Library/Frameworks/Python.framework/Versions/3.10/bin</code></li> </ul> </li> </ol>"},{"location":"kfp/kfp_support_lib/#git","title":"Git","text":"<p>Simple clone the repo and set up the pre-commit hooks. <pre><code>git clone git@github.com:IBM/data-prep-kit.git\ncd kfp/kfp_support_lib\npre-commit install\n</code></pre> If you don't have pre-commit, you can install from here</p>"},{"location":"kfp/kfp_support_lib/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>The process of creating a release for <code>fm_data_processing_kfp</code> package  involves the following steps:</p> <p>cd to the package directory.</p> <p>update the version in .make.versions file.</p> <p>run <code>make set-versions</code> and <code>make build</code> and <code>make publish</code>.</p>"},{"location":"kfp/kfp_support_lib/#testing","title":"Testing","text":"<p>To run the package tests perform the following:</p> <p>To begin with, establish a Kind cluster and deploy all required components by executing the makfefile command in the main directory of this repository. As an alternative, you can manually execute the instructions provided in the README.md file.</p> <pre><code>make setup\n</code></pre> <p>The next step is to deploy the <code>data-prep-kit-kfp</code> package locally within a Python virtual environment.</p> <pre><code>make  build\n</code></pre> <p>lastly, execute the tests:</p> <pre><code>make test\n</code></pre>"},{"location":"kfp/kfp_support_lib/#cleanup","title":"Cleanup","text":"<p>It is advisable to execute the following command prior to running <code>make test</code> once more. This will ensure that any  previous test runs resources are removed before starting new tests.</p> <pre><code>kubectl delete workflows -n kubeflow --all\n</code></pre>"},{"location":"kfp/kfp_support_lib/kfp_v1_workflow_support/","title":"Workflow Support Library that depends on KFPv1","text":"<p>This provides support for implementing KFP pipelines automating transform's execution.</p>"},{"location":"kfp/kfp_support_lib/kfp_v2_workflow_support/","title":"Workflow Support Library that depends on KFPv2","text":"<p>This provides support for implementing KFP pipelines automating transform's execution.</p>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/","title":"Shared Workflow Support","text":"<p>This provides support for implementing KFP pipelines automating transform's execution. This library is not dependent on  KFP version. KFP dependent modules are in kfp_v1_workflow_support and  kfp_v2_workflow_support</p> <p>this module combines 2 inner modules</p> <ul> <li>python apiserver client, which is a copy of Kuberay API server-client python APIs We added it into the project, because these APIs are not exposed by any PyPi.</li> <li>runtime_utils </li> </ul>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#development","title":"Development","text":""},{"location":"kfp/kfp_support_lib/shared_workflow_support/#requirements","title":"Requirements","text":"<ol> <li>python 3.10 or later</li> <li>git command line tools</li> <li>pre-commit</li> <li>twine (pip install twine)<ul> <li>but on Mac you may have to include a dir in your PATH, such as <code>export PATH=$PATH:/Library/Frameworks/Python.framework/Versions/3.10/bin</code></li> </ul> </li> </ol>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#git","title":"Git","text":"<p>Simple clone the repo and set up the pre-commit hooks. <pre><code>git clone git@github.com:IBM/data-prep-kit.git\ncd kfp/kfp_support_lib/shared_workflow_support\npre-commit install\n</code></pre> If you don't have pre-commit, you can install from here</p>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#library-artifact-build-and-publish","title":"Library Artifact Build and Publish","text":"<p>The process of creating a release for the package involves the following steps:</p> <ul> <li>cd to the package directory.</li> <li>update the <code>DPK_LIB_KFP_SHARED</code> version in .make.versions file.</li> <li>run <code>make set-versions</code> and <code>make build</code> and <code>make publish</code>.</li> </ul>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#testing","title":"Testing","text":"<p>To run the package tests perform the following:</p> <p>To begin with, establish a Kind cluster and deploy all required components by executing the makfefile command in the main directory of this repository. As an alternative, you can manually execute the instructions provided in the README.md file.</p> <pre><code>make setup\n</code></pre> <p>The next step is to deploy the <code>data-prep-kit-kfp</code> package locally within a Python virtual environment.</p> <pre><code>make  build\n</code></pre> <p>lastly, execute the tests:</p> <pre><code>make test\n</code></pre>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/#cleanup","title":"Cleanup","text":"<p>It is advisable to execute the following command prior to running <code>make test</code> once more. This will ensure that any  previous test runs resources are removed before starting new tests.</p> <p>```bash kubectl delete workflows -n kubeflow --all</p>"},{"location":"kfp/kfp_support_lib/shared_workflow_support/src/python_apiserver_client/","title":"KubeRay API server APIs","text":"<p>This is a copy of Kuberay API server-client python APIs Because these APIs are not exposed by any PyPi, we added them to the project</p>"},{"location":"kfp/pipeline_generator/single-pipeline/","title":"Index","text":""},{"location":"kfp/pipeline_generator/single-pipeline/#steps-to-generate-a-new-pipeline","title":"Steps to generate a new pipeline","text":"<ul> <li>create a <code>pipeline_definitions.yaml</code> file for the required task (similar to the example pipeline_definitions.yaml for the noop task).</li> <li>execute <code>make -C ../../../transforms workflow-venv</code> from this directory</li> <li>execute <code>source ../../../transforms/venv/bin/activate</code></li> <li>execute <code>./run.sh --config_file &lt;pipeline_definitions_file_path&gt; --output_dir_file &lt;destination directory&gt;</code>. When <code>pipeline_definitions_file_path</code> is the path of the <code>pipeline_definitions.yaml</code> file that defines the pipeline and <code>destination directory</code> is a directory where new pipeline file  will be generated.</li> </ul>"},{"location":"kfp/pipeline_generator/superpipeline/","title":"Index","text":""},{"location":"kfp/pipeline_generator/superpipeline/#steps-to-generate-a-new-super-pipeline-in-kfp-v1","title":"Steps to generate a new super pipeline in KFP v1.","text":"<ul> <li>The super pipeline allows you to execute several transforms within a single pipeline. For more details, refer multi_transform_pipeline.md.</li> <li>Create a <code>super_pipeline_definitions.yaml</code> file for the required task. You can refer to the example super_pipeline_definitions.yaml.</li> <li>execute <code>make -C ../../../transforms workflow-venv</code> from this directory</li> <li>execute <code>source ../../../transforms/venv/bin/activate</code></li> <li>Execute <code>./run.sh --config_file &lt; super_pipeline_definitions.yaml&gt; --output_dir_file &lt;destination_directory&gt;</code>. Here, <code>super_pipeline_definitions.yaml</code> is the super pipeline definition file, that you created above, and <code>destination_directory</code> is the directory where the new super pipeline file will be generated.</li> </ul> <p>NOTE: the <code>component_spec_path</code> is the path to the <code>kfp_ray_components</code> folder and depends on where the workflow is compiled.</p>"},{"location":"scripts/k8s-setup/","title":"Building Kind cluster with everything installed","text":""},{"location":"scripts/k8s-setup/#pre-requirements","title":"Pre-requirements","text":"<ul> <li>Supported platforms</li> <li>Preinstalled software components</li> <li>Preparing a Kind cluster for testing</li> </ul> <p>As an alternative, you can execute the following manual installation instructions:</p>"},{"location":"scripts/k8s-setup/#create-cluster","title":"Create cluster","text":"<p>Run the following command to create the cluster:</p> <pre><code>cd /tmp\ngit clone https://github.com/IBM/data-prep-kit.git\ncd data-prep-kit\nexport REPOROOT=$PWD\nkind create cluster --name dataprep --config ${REPOROOT}/scripts/k8s-setup/kind-cluster-config.yaml\n</code></pre> <p>Note that by default this will create a kind cluster with 2 worker nodes. If you would like a different amount of node, modify cluster configuration</p>"},{"location":"scripts/k8s-setup/#install-kfp","title":"Install KFP","text":"<p>Install Kubeflow Pipelines and wait for it to be ready:</p> <pre><code># Set required KFP version. You can reference to the latest supported version in the [requirements.env](https://github.com/ian-cho/data-prep-kit/blob/dev/scripts/k8s-setup/requirements.env) file.\n# Currently, we support 1.8.5 for KFPv1 and 2.2.0 for KFP v2\nexport PIPELINE_VERSION=1.8.5\ncd $REPOROOT/scripts/k8s-setup/tools/ &amp;&amp; ./install_kubeflow.sh deploy &amp;&amp; cd -\nkubectl wait --for=condition=ready --all pod -n kubeflow --timeout=300s\n</code></pre>"},{"location":"scripts/k8s-setup/#install-kuberay","title":"Install KubeRay","text":"<p>Install Kuberay:</p> <pre><code>cd $REPOROOT/scripts/k8s-setup/tools/ &amp;&amp; KUBERAY_APISERVER=1.1.0 KUBERAY_OPERATOR=1.0.0 ./install_kuberay.sh deploy &amp;&amp; cd -\nkubectl wait --for=condition=ready --all pod -n kuberay --timeout=300s\n</code></pre>"},{"location":"scripts/k8s-setup/#install-ngnix","title":"Install NGNIX","text":"<p>To access the API server and Kubeflow pipeline UI externally, we make use NGINX ingress.</p> <p>Install Ingress NGNIX for KFP, RAY and MinIO and wait for it to be ready:</p> <pre><code>${REPOROOT}/scripts/k8s-setup/tools/install_nginx.sh deploy\nkubectl wait --namespace ingress-nginx \\\n          --for=condition=ready pod \\\n          --selector=app.kubernetes.io/component=controller \\\n          --timeout=90s\n</code></pre> <p>To deploy the ingress for Ray API Server, KFP and MinIO execute the following: <pre><code>kubectl apply -f $REPOROOT/scripts/k8s-setup/ray_api_server_ingress.yaml\nkubectl apply -f $REPOROOT/scripts/k8s-setup/kfp_ingress.yaml\nkubectl apply -f $REPOROOT/scripts/k8s-setup/minio_ingress.yaml\n</code></pre></p> <p>Open the Kubeflow Pipelines UI at  http://localhost:8080/</p>"},{"location":"scripts/k8s-setup/#working-with-a-minio-server-instead-of-s3-storage","title":"Working with a MinIO server instead of S3 storage","text":"<p>You can work with a real S3 storage, but for testing you can use the Mino server which is deployed as part of the KFP installation. You can access the Minio dashboard at http://localhost:8090/</p>"},{"location":"scripts/k8s-setup/#create-a-secret","title":"Create a secret","text":"<p>The MinIO service, deployed as a part of KFP, uses a username (<code>minio</code>) as an access_key/password (<code>minio123</code>) as the secret key. A secret needs to be created for accessing MinIO using the following command:</p> <pre><code>kubectl apply -f $REPOROOT/scripts/k8s-setup/s3_secret.yaml\n</code></pre>"},{"location":"scripts/k8s-setup/#copy-test-data","title":"Copy test data","text":"<p>Populating Minio server with test data can be done using <code>mc</code>. Use the following command:</p> <pre><code>$REPOROOT/scripts/k8s-setup/populate_minio.sh\n</code></pre> <p>This file creates an mc alias, creates the test bucket and copies the local test data into MinIO. If you need to load additional data, please load it using additional <code>mc</code> commands, similar to the ones being used by <code>populate_minio.sh</code></p>"},{"location":"scripts/k8s-setup/#cleanup","title":"Cleanup","text":"<p>See Clean up the cluster</p>"},{"location":"tools/ingest2parquet/","title":"INGEST2PARQUET","text":"<p>Please note: This tool is deprecated and will be removed soon.  It is superseded by the transform-based implementation,  code2parquet,  providing identical capability,  but with support for ray-based scalability.</p>"},{"location":"tools/ingest2parquet/#summary","title":"Summary","text":"<p>This Python script is designed to convert raw data files, particularly ZIP files, into Parquet format. It is built to handle concurrent processing of multiple files using multiprocessing for efficient execution. Each file contained within the ZIP is transformed into a distinct row within the Parquet dataset, adhering to the below schema.</p> <p>title: (string)</p> <ul> <li>Description: Path to the file within the ZIP archive.</li> <li>Example: <code>\"title\": \"data/file.txt\"</code></li> </ul> <p>document: (string)</p> <ul> <li>Description: Name of the ZIP file containing the current file.</li> <li>Example: <code>\"document\": \"example.zip\"</code></li> </ul> <p>repo_name:</p> <ul> <li>Description: Name of the repository the code belongs to. Repo_name is same as zip file name.</li> <li>Example: <code>\"repo_name\": \"data\"</code></li> </ul> <p>contents: (string)</p> <ul> <li>Description: Content of the file, converted to a string.</li> <li>Example: <code>\"contents\": \"This is the content of the file.\"</code></li> </ul> <p>document_id: (string)</p> <ul> <li>Description: Unique identifier generated for each file.</li> <li>Example: <code>\"document_id\": \"b1e4a879-41c5-4a6d-a4a8-0d7a53ec7e8f\"</code></li> </ul> <p>ext: (string)</p> <ul> <li>Description: File extension extracted from the file path.</li> <li>Example: <code>\"ext\": \".txt\"</code></li> </ul> <p>hash: (string)</p> <ul> <li>Description: sha256 hash value computed from the file content string.</li> <li>Example: <code>\"hash\": \"a1b2c3d4\"</code></li> </ul> <p>size: (int64)</p> <ul> <li>Description: Size of the file content in bytes.</li> <li>Example: <code>\"size\": 1024</code></li> </ul> <p>date_acquired: (string)</p> <ul> <li>Description: Timestamp indicating when the file was processed.</li> <li>Example: <code>\"date_acquired\": \"2024-03-25T12:00:00\"</code></li> </ul> <p>snapshot: (string)(optional)</p> <ul> <li>Description: Name indicating which dataset it belong to.</li> <li>Example: <code>\"snapshot\": \"github\"</code></li> </ul> <p>programming_language: (string)(optional)</p> <ul> <li>Description: Programming language detected using the file extension.</li> <li>Example: <code>\"programming_language\": \"Java\"</code></li> </ul> <p>domain: (string)(optional)</p> <ul> <li>Description: Name indicating which domain it belong to, whether code, natural language etc..</li> <li>Example: <code>\"domain\": \"code\"</code></li> </ul>"},{"location":"tools/ingest2parquet/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding ingest2parquet  configuration for values are as follows: <pre><code>  --detect_programming_lang DETECT_PROGRAMMING_LANG\n                        generate programming language from the file extension\n  --snapshot SNAPSHOT\n                        Name the dataset\n  --domain DOMAIN\n                        To identify whether data is code or natural language\n  --data_s3_cred DATA_S3_CRED\n                        AST string of options for s3 credentials. Only required for S3 data access. access_key: access key help text secret_key: secret key help text url: optional s3 url region: optional s3 region Example: { 'access_key':\n                        'access', 'secret_key': 'secret', 'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud', 'region': 'us-east-1' }\n  --data_s3_config DATA_S3_CONFIG\n                        AST string containing input/output paths. input_folder: Path to input folder of files to be processed output_folder: Path to output folder of processed files Example: { 'input_folder': 's3-path/your-input-bucket',\n                        'output_folder': 's3-path/your-output-bucket' }\n  --data_local_config DATA_LOCAL_CONFIG\n                        ast string containing input/output folders using local fs. input_folder: Path to input folder of files to be processed output_folder: Path to output folder of processed files Example: { 'input_folder': './input',\n                        'output_folder': '/tmp/output' }\n  --data_max_files DATA_MAX_FILES\n                        Max amount of files to process\n  --data_checkpointing DATA_CHECKPOINTING\n                        checkpointing flag\n  --data_data_sets DATA_DATA_SETS\n                        List of data sets\n  --data_files_to_use DATA_FILES_TO_USE\n                        list of files extensions to choose\n  --data_num_samples DATA_NUM_SAMPLES\n                        number of random files to process\n</code></pre></p>"},{"location":"tools/ingest2parquet/#running","title":"Running","text":"<p>We provide several demos of the script usage for different data storage options: </p>"},{"location":"tools/ingest2parquet/#local-file-system","title":"local file system","text":"<p>This script processes data stored locally on the system. It sets up parameters for local file paths and invokes the ingest2parquet() function from ingest2parquet.py to convert raw data files to Parquet format.</p> <p>Run using make targets.</p> <p><code>run-local-sample</code> - runs src/ingest2parquet_local.py</p> <p>Run the script without any command-line arguments.</p> <pre><code>make venv\nsource venv/bin/activate\ncd src\npython ingest2parquet_local.py\n</code></pre> <p>Run the script via command-line </p> <pre><code>python ingest2parquet.py \\\n    --detect_programming_lang True \\\n    --snapshot github \\\n    --domain code \\\n    --data_local_config '{\"input_folder\": \"../test-data/input\", \"output_folder\": \"../test-data/output\"}' \\\n    --data_files_to_use '[\".zip\"]'\n</code></pre>"},{"location":"tools/ingest2parquet/#s3","title":"s3","text":"<p>This script is designed to process data stored on an S3 bucket. It sets up necessary parameters for accessing the S3 bucket and invokes the ingest2parquet() function from ingest2parquet.py to convert raw data files to Parquet format.</p> <p>To execute the script with S3 functionality, we utilize minio. </p> <p><code>make minio-start</code> - To start minio server <code>make minio-load</code> - To load the data into minio <code>make minio-stop</code> - To stop the minio server</p> <p>Please consult the documentation for further guidance: transform-s3-testing </p> <p>** Run script using make targets ** <code>run-s3-sample</code> : Starts minio server and load data from test-data into local minio for S3 access and runs src/ ingest2parquet_s3.py</p> <p>Run the script without any command-line arguments.</p> <pre><code>make venv\nsource venv/bin/activate\ncd src\npython ingest2parquet_s3.py\n</code></pre> <p>Run the script via command-line </p> <pre><code>python ingest2parquet.py \\\n    --detect_programming_lang True \\\n    --snapshot github \\\n    --domain code \\\n    --data_s3_cred '{\"access_key\": \"localminioaccesskey\", \"secret_key\": \"localminiosecretkey\", \"url\": \"http://localhost:9000\"}' \\\n    --data_s3_config '{\"input_folder\": \"test/ingest2parquet/input\", \"output_folder\": \"test/ingest2parquet/output\"}' \\\n    --data_files_to_use '[\".zip\"]'\n</code></pre> <p>The output directory will contain both the new genrated parquet files  and the <code>metadata.json</code> file.</p>"},{"location":"tools/ingest2parquet/#metadata-fields","title":"Metadata Fields","text":"<p>The metadata.json file contains following essential information regarding the processing of raw data files to Parquet format:</p> <p><code>total_files_given</code>: Total number of raw data files provided for processing. <code>total_files_processed</code>: Number of files successfully processed and converted to Parquet format. <code>total_files_failed_to_processed</code>: Count of files that encountered processing errors and failed conversion. <code>total_no_of_rows</code>: Aggregate count of rows across all successfully processed files. <code>total_bytes_in_memory</code>: Total memory usage in bytes for all processed data files. <code>failure_details</code>: Information about files that failed processing, including their paths and error messages.</p>"},{"location":"tools/ingest2parquet/#building-the-docker-image","title":"Building the Docker Image","text":"<pre><code>% make image \n</code></pre>"},{"location":"tools/ingest2parquet/#run-using-docker-image","title":"Run using docker image","text":"<pre><code>docker run -it -v $(pwd)/test-data/input:/test-data/input -v $(pwd)/test-data/output:/test-data/output quay.io/dataprep1/data-prep-kit/ingest2parquet:0.1 sh -c \"python ingest2parquet.py \\\n    --detect_programming_lang True \\\n    --snapshot github \\\n    --domain code \\\n    --data_local_config '{\\\"input_folder\\\": \\\"/test-data/input\\\", \\\"output_folder\\\":\\\"/test-data/output\\\"}' \\\n    --data_files_to_use '[\\\".zip\\\"]'\"\n</code></pre> <p>In addition, there are some useful <code>make</code> targets (see conventions above): * <code>make venv</code> - creates the virtual environment. * <code>make test</code> - runs the tests in test directory * <code>make build</code> - to build the docker image * <code>make help</code> - displays the available <code>make</code> targets and help text.</p>"},{"location":"transforms/","title":"Transforms","text":"<p>The transformation framework is designed to operate on arbitrary input data, usually tabular data read from parquet files as pyarrow tables, but also any binary data. For details on some of the key classes, listed here, * AbstractBinaryTransform * AbstractTableTransform * TransformConfiguration * DefaultTransformRuntime</p> <p>please refer to  DPK core library documentation.</p> <p>If starting to develop a new transform, you will likely want to start with one of the following:</p> <ul> <li>quick start tutorial - provides a gentle introduction to developing your first transform.</li> <li>NOOP transform - this can be used as a template to start your new transform. </li> </ul>"},{"location":"transforms/#importing-transforms","title":"Importing Transforms","text":"<p>These are generally binary transforms that take in one format and convert to another usually, a parquet formatted PyArrow table.</p>"},{"location":"transforms/#annotating-transforms","title":"Annotating Transforms","text":"<p>Annotating transforms examine 1 or more columns of data, typically a content column containing a document to be annotated.  The content is often spoken/text or programming language, generally to build a large language model (LLM).  Examples of annotation might include:</p> <ul> <li>Language identification - an additional string column is added to identify the language of the document content.</li> <li>Document quality - an additional float column is added to associated a quality score with the document.</li> <li>Block listing - an addtional boolean column is added that indicates if the content source url   (in one of the columns) is from a blocked domain.</li> </ul>"},{"location":"transforms/#filtering-transforms","title":"Filtering Transforms","text":"<p>Filtering transforms modify the rows and/or columns, usually based on associated column values. For example,</p> <ul> <li>Language selection - remove rows that do not match the desired language</li> <li>Document quality threshold - remove rows that do not meet a minimum document quality value.</li> <li>Block listing - remove rows that have been flagged as having been sourced from undesirable domains.</li> </ul>"},{"location":"transforms/#transform-organization","title":"Transform Organization","text":"<p>This directory hierarchy of transforms is organized as follows:</p> <ul> <li><code>universal</code> - transforms applicable across code and language model data include</li> <li><code>language</code> - spoken language model specific transforms</li> <li><code>code</code> - programming language specific transforms.</li> </ul> <p>Each of the <code>universal</code>, <code>language</code> and <code>code</code>  directories contains a directory for a specific transform. Each transform is expected to be a standalone entity that can be run locally on small data sets or runs at scale to process terabytes of data.  They each run in their own virtual environments.</p>"},{"location":"transforms/#transform-project-conventions","title":"Transform Project Conventions","text":"<p>For transform projects it is encourraged to use a common set of conventions  including code layout, build, documentation and IDE recommendations. For a transformed named <code>xyz</code>, it is expected to have its project located under one of</p> <p><code>transforms/code/xyz</code> <code>transforms/language/xyz</code>, OR  <code>transforms/universal/xyz</code>.</p> <p>Additional conventions follow.</p>"},{"location":"transforms/#makefile","title":"Makefile","text":"<p>The Makefile is the primary entry point for performing most CLI-based functions for the build and management of a transform, including  git CI/CD workflows. This includes </p> <ul> <li>creating the virtual environment</li> <li>testing</li> <li>building docker images</li> <li>cleanup</li> </ul> <p>Use <code>make help</code> in any directory with a Makefile to see the available targets. Each Makefile generally requires the following macro definitions:</p> <ul> <li>REPOROOT - specifies a relative path to the local directory that is the root of the repository.</li> <li>TRANSFORM_NAME - specifies the short name of the transform that will be used in creating pypi artifacts and docker images. This is set by default, for the examples above, <code>xyz</code> based on the directory name.</li> </ul> <p>A transform's Makefile is most often based on the  Makefile.transform.template found in this directory.</p>"},{"location":"transforms/#mimimum-transform-project-structure","title":"Mimimum Transform Project Structure","text":"<p>Each transform project directory (e.g., <code>universal/xyz</code> implementing the xyz  transform) contains implementation of the python transform. The directory structure is as follows (and recall that  NOOP transform can serve as a good template if developing a new transform). </p> <p><pre><code>data-prep-kit\n\u2502\n\u2514\u2500\u2500\u2500transforms\n\u2502   |\n\u2502   \u2514\u2500\u2500\u2500universal\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500\u2500xyz\n\u2502            |\n\u2502            \u2514\u2500\u2500\u2500dpk_xyz * \n\u2502            |      \u2502\n\u2502            |      \u2502 __init__.py\n\u2502            |      \u2502 transform.py \n\u2502            \u2514\u2500\u2500\u2500test  * \n\u2502            \u2502    |\n\u2502            \u2502    | test_xyz.py \n\u2502            \u2502\n\u2502            \u2514\u2500\u2500\u2500test-data\n\u2502            \u2502     |\n\u2502            |     \u2514\u2500\u2500\u2500input\n\u2502            |     |     \u2502\n\u2502            |     |     \u2502 testfile.parquet\n\u2502            |     |     \n\u2502            |     \u2514\u2500\u2500\u2500expected\n\u2502            |          \u2502\n\u2502            |          \u2502 testfile.parquet\n\u2502            |          \u2502 metadata.json\n\u2502            | \n\u2502            | Makefile  *\n\u2502            | requirements.txt  *\n\u2502            | README.md  \n</code></pre> <code>*</code> Required to make use of and pass CI/CD.  </p> <p>Contents are described as follows: * <code>dpk_xyz/</code> - directory contains all python implementation code where <code>xyz</code> is the name   of the parent directory (e.g., <code>transforms/universal/xyz</code>).      * <code>transform.py</code> generally contains the core transform implementation:         * <code>XYZTransform</code> class implementing the transformation         * <code>XYXTransformConfiguration</code> class that defines CLI configuration for the transform     * Additional files can be added as necessary in support of the transform. * <code>test/</code> - directory contains test code      * <code>test_xyz.py</code> - a standalone (non-runtime launched) transform test.  This is best for initial debugging. Inherits from an abstract test class so that to test one needs only to provide test data.     * NOTE: Tests are expected to be run from anywhere and so need to use       <code>__file__</code> location to create absolute directory paths to the data in the <code>../test-data</code> directory.       From the command line, <code>make test</code> sets up the virtual environment and PYTHONPATH to include <code>dpk_xyz</code>       Do not add <code>sys.path.append(...)</code> in the test python code.       All test data should be referenced as <code>../test-data</code> relative       to <code>os.path.abspath(os.path.dirname(__file__))</code>. * <code>test-data/</code> - optional directory containing data used in the tests * <code>Makefile</code> - provides most targets expected by CI/CD and is usually   based on a copy of Makefile.transform.template in this directory. <code>make help</code> to see a list of targets. * <code>requirements.txt</code> - defines requirements specific to the python transform (Ray and Spark requirements are handled by automation). * <code>README.md</code> -  documents use and implementation.  Consider using README.md.template in this directory as a starting point.</p> <p>The command <code>make conventions</code> run from within a transform directory will examine the runtime project structure and make recommendations.</p>"},{"location":"transforms/#adding-runtime-support","title":"Adding Runtime Support","text":"<p>If adding support for Ray and/or Spark runtimes the following additions are generally required. <pre><code>data-prep-kit\n\u2502\n\u2514\u2500\u2500\u2500transforms\n\u2502   |\n\u2502   \u2514\u2500\u2500\u2500universal\n\u2502       \u2502\n\u2502       \u2514\u2500\u2500\u2500xyz\n\u2502            |\n\u2502            \u2514\u2500\u2500\u2500dpk_xyz \n\u2502            |      | runtime.py *, **\n\u2502            |      \u2514\u2500\u2500\u2500ray  \n\u2502            |           \u2502 __init__.py\n\u2502            |           | runtime.py *, **\n\u2502            |           \u2502 \n\u2502            |      \u2514\u2500\u2500\u2500spark  \n\u2502            |           \u2502 __init__.py\n\u2502            |           | runtime.py  *, **\n\u2502            |           \u2502 \n\u2502            \u2514\u2500\u2500\u2500test  \n\u2502            \u2502    |\n\u2502            \u2502    |test_xyz_python.py  **\n\u2502            \u2502    |test_xyz_ray.py  **\n\u2502            \u2502    |test_xyz_spark.py  **\n\u2502            \u2502\n\u2502            | Dockerfile.python ** \n\u2502            | Dockerfile.ray ** \n\u2502            | Dockerfile.spark ** \n</code></pre> <code>*</code> Recommended naming.  Other naming requires changes to Makefile. <code>**</code> Required for corresponding Python, Ray and/or Spark runtime images. </p> <p>The contents are defined as follows: * <code>dpk_xyz</code>     * <code>runtime.py</code> - runs the transform on input using the python runtime         * <code>XYZPythonTransformConfiguration</code> class         * main() to start the <code>PythonTransformLauncher</code> with the above.     * <code>ray/</code> - directory contains code to enable the python transform in a Ray runtime.         * <code>runtime.py</code> - ray runtime artifacts to enable the transform in the DPK Ray runtime.             * <code>XYZRayTransformConfiguration</code> class             * main() to start the <code>RayTransformLauncher</code> with the above.     * <code>spark/</code> - directory contains code to enable the python transform in a Spark runtime.         * <code>runtime.py</code> - spark runtime artifacts to enable the transform in the DPK Spark runtime.             * <code>XYZSparkTransformConfiguration</code> class             * main() to start the <code>SparkTransformLauncher</code> with the above.  * <code>test/</code>     * <code>test_xyz_python.py</code> - defines the transform tests running in the Python launcher.         * Again, inherits from an abstract test class so that to test one needs only to provide test data.     * <code>test_xyz_ray.py</code> - defines the transform tests running in the Ray launcher.     * <code>test_xyz_spark.py</code> - defines the transform tests running in the Spark launcher.  * <code>Dockerfile.python</code> - to build the transform and python runtime into a docker image,   generally, based on the template Dockerfile.python.template in this directory. * <code>Dockerfile.ray</code> - to build the transform and ray runtime into a docker image,   generally, based on the template Dockerfile.ray.template in this directory. * <code>Dockerfile.spark</code> -  to build the transform and spark runtime into a docker image,   generally, based on the template Dockerfile.spark.template in this directory.</p>"},{"location":"transforms/#configuration-and-command-line-options","title":"Configuration and command line options","text":"<p>A transform generally accepts a dictionary of configuration to control its operation.  For example, the size of a table, the location of a model, etc. These are set either explicitly in dictionaries (e.g. during testing) or from the command line when run from a runtime launcher.</p> <p>When specified on the command line, transform <code>xyz</code> should use an <code>xyz_</code> prefix with <code>--xyz_</code> (dash dash) to define its command line options. For example, <code>--xyz_some_cfg somevalue</code> sets the value for the <code>xyz_some_cfg</code> configuration key value to <code>somevalue</code>. To avoid potential collisions with options for the runtime launcher,  Data Access Factory and others, it is strongly encouraged to not use single dash options with a single or small number of characters (e.g. -n).</p>"},{"location":"transforms/#docker-images","title":"Docker Images","text":"<p>Generally, to build the docker images, one uses the Makefile with the following make targets</p> <ul> <li><code>image-python</code> - uses <code>Dockerfile.python</code> to build the python runtime image for the transform.</li> <li><code>image-ray</code> - uses <code>Dockerfile.ray</code> to build the python runtime image for the transform.</li> <li><code>image-spark</code> - uses <code>Dockerfile.spark</code> to build the python runtime image for the transform.</li> <li><code>image</code> - build all images.</li> </ul> <p>Similarly, to test the docker images, </p> <ul> <li><code>test-image-python</code> - runs a simple --help run of the transform in the python image. </li> <li><code>test-image-ray</code> - runs a simple --help run of the transform in the Ray image. </li> <li><code>test-image-spark</code> - runs a simple --help run of the transform in the Spark image. </li> </ul>"},{"location":"transforms/#ide-setup","title":"IDE Setup","text":"<p>When running in an IDE, such as PyCharm or VS Code, the following are generally required:</p> <ul> <li>From the command line, build the venv using <code>make venv</code>.</li> <li>In the IDE<ul> <li>Set your project/run configuration to use the venv/bin/python as your runtime virtual environment.<ul> <li>In PyCharm, this can be done through the PyCharm-&gt;Settings-&gt;Project...-&gt;Python Interpreter page</li> <li>In VS Code, click on the current Python Interpreter in the bottom right corner and make sure that the Interpreter path is venv/bin/python</li> </ul> </li> <li>Mark the <code>dpk_xyz</code> directory as a source root so that it is included in your PYTHONPATH when running .py files in the IDE<ul> <li>In Pycharm this can be done by selecting the <code>dpk_xyz</code> directory, and then selecting <code>Mark Directory as</code> -&gt; <code>Sources Root</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"transforms/README-list/","title":"DPK Python Transforms","text":""},{"location":"transforms/README-list/#installation","title":"installation","text":"<p>The transforms are delivered as a standard pyton library available on pypi and can be installed using pip install:</p> <p><code>python -m pip install data-prep-toolkit-transforms[all]</code> or <code>python -m pip install data-prep-toolkit-transforms[ray, all]</code> or <code>python -m pip install data-prep-toolkit-transforms[language]</code></p> <p>installing the python transforms will also install  <code>data-prep-toolkit</code></p> <p>installing the ray transforms will also install  <code>data-prep-toolkit[ray]</code></p>"},{"location":"transforms/README-list/#list-of-transforms-in-current-package","title":"List of Transforms in current package","text":"<p>Note: This list includes the transforms that were part of the release starting with data-prep-toolkit-transforms:0.2.1. This list may not always reflect up to date information. Users are encourage to raise an issue in git when they discover missing components or packages that are listed below but not in the current release they get from pypi.</p> <ul> <li>code<ul> <li>code2parquet</li> <li>header_cleanser (Not available on MacOS)</li> <li>code_quality</li> <li>proglang_select</li> <li>code_profiler</li> </ul> </li> <li>language<ul> <li>doc_chunk</li> <li>doc_quality</li> <li>lang_id</li> <li>pdf2parquet</li> <li>text_encoder</li> <li>pii_redactor</li> </ul> </li> <li>universal<ul> <li>ededup</li> <li>fdedup</li> <li>filter</li> <li>resize</li> <li>tokenization</li> <li>doc_id</li> <li>web2parquet</li> </ul> </li> </ul>"},{"location":"transforms/README-list/#release-notes","title":"Release notes:","text":""},{"location":"transforms/README-list/#101dev1","title":"1.0.1.dev1","text":"<pre><code>Added Gneissweb transforms\nfdedup fix for windows\n</code></pre>"},{"location":"transforms/README-list/#101dev0","title":"1.0.1.dev0","text":"<pre><code>PR #979 (code_profiler)\n</code></pre>"},{"location":"transforms/README-list/#100a6","title":"1.0.0.a6","text":"<pre><code>Added Profiler\nAdded Resize\n</code></pre>"},{"location":"transforms/README-list/#100a5","title":"1.0.0.a5","text":"<pre><code>Added Pii Redactor\nRelax fasttext requirement &gt;= 0.9.2\n</code></pre>"},{"location":"transforms/README-list/#100a4","title":"1.0.0.a4","text":"<pre><code>Added missing ray implementation for lang_id, doc_quality, tokenization and filter\nAdded ray notebooks for lang id, Doc Quality, tokenization, and Filter\n</code></pre>"},{"location":"transforms/README-list/#100a3","title":"1.0.0.a3","text":"<pre><code>Added code_profiler\n</code></pre>"},{"location":"transforms/README-list/#100a2","title":"1.0.0.a2","text":"<p>Relax dependencies on pandas (use latest or whatever is installed by application)    Relax dependencies on requests (use latest or whatever is installed by application)</p>"},{"location":"transforms/add_new_kfp_workflow/","title":"Adding new KFP workflows","text":"<p>This README outlines the steps to add a new KFP workflow for a new transform under transforms directory.</p> <p>1) Create a new <code>kfp_ray</code> directory in the transform directory, similar to this directory.</p> <p>2) Create the workflow and add it to <code>kfp_ray</code> directory. It is recommended to use the pipeline generator for that. If the workflow was generated using the pipeline generator also include <code>pipeline_definitions.yaml</code> file used to generate the workflow in the <code>kfp_ray</code> directory.</p> <p>3) Add <code>Makefile</code> file to <code>kfp_ray</code> directory similar to this Makefile example.</p> <p>3) Add the path to the transform input directory in the populate_minio script. This path is used when testing the workflow. 4) Create a GitHub Action for the kfp workflow using the <code>make</code> command in the .github/workflows/ directory. 5) Update the workflows list in README.md file.</p>"},{"location":"transforms/code/code2parquet/","title":"Code2Parquet Transform","text":"<p>This code2parquet transform is designed to convert raw particularly ZIP files contain programming files (.py, .c, .java, etc) ,  into Parquet format.  Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation and python runtime.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/code2parquet/kfp_ray/","title":"Code to Parquet Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/code2parquet/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/code2parquet/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/code2parquet/python/","title":"Code2Parquet","text":""},{"location":"transforms/code/code2parquet/python/#summary","title":"Summary","text":"<p>This code2parquet transform is designed to convert raw particularly ZIP files contain programming files (.py, .c, .java, etc) ,  into Parquet format.  As a transform It is built to handle concurrent processing of Ray-based multiple files using multiprocessing for efficient execution. Each file contained within the ZIP is transformed into a distinct row within the Parquet dataset, adhering to the below schema.</p> <p>title: (string)</p> <ul> <li>Description: Path to the file within the ZIP archive.</li> <li>Example: <code>\"title\": \"data/file.txt\"</code></li> </ul> <p>document: (string)</p> <ul> <li>Description: Name of the ZIP file containing the current file.</li> <li>Example: <code>\"document\": \"example.zip\"</code></li> </ul> <p>repo_name:</p> <ul> <li>Description: The name of the repository to which the code belongs. This should match the name of the zip file containing the repository.</li> <li>Example: <code>\"repo_name\": \"example\"</code></li> </ul> <p>contents: (string)</p> <ul> <li>Description: Content of the file, converted to a string.</li> <li>Example: <code>\"contents\": \"This is the content of the file.\"</code></li> </ul> <p>document_id: (string)</p> <ul> <li>Description: Unique identifier computed as a uuid. </li> <li>Example: <code>\"document_id\": \"b1e4a879-41c5-4a6d-a4a8-0d7a53ec7e8f\"</code></li> </ul> <p>ext: (string)</p> <ul> <li>Description: File extension extracted from the file path.</li> <li>Example: <code>\"ext\": \".txt\"</code></li> </ul> <p>hash: (string)</p> <ul> <li>Description: sha256 hash value computed from the file content string.</li> <li>Example: <code>\"hash\": \"a1b2c3d4\"</code></li> </ul> <p>size: (int64)</p> <ul> <li>Description: Size of the file content in bytes.</li> <li>Example: <code>\"size\": 1024</code></li> </ul> <p>date_acquired: (string)</p> <ul> <li>Description: Timestamp indicating when the file was processed.</li> <li>Example: <code>\"date_acquired\": \"2024-03-25T12:00:00\"</code></li> </ul> <p>snapshot: (string)(optional)</p> <ul> <li>Description: Name indicating which dataset it belong to.</li> <li>Example: <code>\"snapshot\": \"github\"</code></li> </ul> <p>programming_language: (string)(optional)</p> <ul> <li>Description: Programming language detected using the file extension.</li> <li>Example: <code>\"programming_language\": \"Java\"</code></li> </ul> <p>domain: (string)(optional)</p> <ul> <li>Description: Name indicating which domain it belong to, whether code, natural language etc..</li> <li>Example: <code>\"domain\": \"code\"</code></li> </ul>"},{"location":"transforms/code/code2parquet/python/#configuration","title":"Configuration","text":"<p>The set of dictionary keys holding code2parquet  configuration for values are as follows:</p> <p>The transform can be configured with the following key/value pairs from the configuration dictionary. * <code>supported_languages</code> - a dictionary mapping file extensions to language names. * <code>supported_langs_file</code> - used if <code>supported_languages</code> key is not provided,   and specifies the path to a JSON file containing the mapping of languages   to extensions. The json file is expected to contain a dictionary of   languages names as keys, with values being a list of strings specifying the   associated extensions. As an example, see    lang_extensions . * <code>data_access_factory</code> - used to create the DataAccess instance used to read the file specified in <code>supported_langs_file</code>. * <code>detect_programming_lang</code> - a flag that indicates if the language:extension mappings   should be applied in a new column value named <code>programming_language</code>. * <code>domain</code> - optional value assigned to the imported data in the 'domain' column. * <code>snapshot</code> -  optional value assigned to the imported data in the 'snapshot' column.</p>"},{"location":"transforms/code/code2parquet/python/#running","title":"Running","text":""},{"location":"transforms/code/code2parquet/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to the options provided by the launcher.</p> <ul> <li><code>--code2parquet_supported_langs_file</code> - set the <code>supported_langs_file</code> configuration key. </li> <li><code>--code2parquet_detect_programming_lang</code> - set the <code>detect_programming_lang</code> configuration key. </li> <li><code>--code2parquet_domain</code> - set the <code>domain</code> configuration key. </li> <li><code>--code2parquet_snapshot</code> -  set the <code>snapshot</code> configuration key. </li> </ul>"},{"location":"transforms/code/code2parquet/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/code2parquet_transform_ray.py using command line args</li> <li><code>run-local-sample</code> - runs src/code2parquet.py</li> <li><code>run-s3-sample</code> - runs src/code2parquet.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code2parquet/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code2parquet/ray/","title":"NOOP Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/code2parquet/ray/#summary","title":"Summary","text":"<p>This project wraps the code2parquet transform with a Ray runtime.</p>"},{"location":"transforms/code/code2parquet/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>code2parquet transform configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/code/code2parquet/ray/#running","title":"Running","text":""},{"location":"transforms/code/code2parquet/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  launcher options are available.</p>"},{"location":"transforms/code/code2parquet/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/code2parquet_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/code2parquet_local_ray.py</li> <li><code>run-s3-sample</code> - runs src/code2parquet_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code2parquet/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code_profiler/","title":"Code Profiler Transform","text":"<p>This module extracts the base syntactic concepts from the multi-language source codes and represent these concepts in a unified langauge-agnostic representation that can be further used for multi-language data profiling. While programming languages expose similar syntactic building blocks to represent programming intent, such as importing packages/libraries, functions, classes, loops, conditionals, comments and others, these concepts are expressed through language-specific grammar, defined by distinct keywords and syntactic form. Our framework abstracts language-specific concepts by transforming them into a unified, language-agnostic representation called universal base syntactic representation (UBSR), referred to as a concept, which is consistently encoded within the proposed schema structure. The current version supports the base syntactic concept for importing/including package/libraries, comments, functions. </p> <p>Table 1 outlines the fields of the UBSR, which maps AST nodes to a structured schema. This schema captures syntactic nodes (based on AST node types) and the relationships between those nodes (derived from AST edges). The UBSR framework currently supports 21 languages, grouped according to their syntactic paradigms.</p> <p>Table 1: UBSR Schema Representation</p> Key Possible Values Description \"nodes\": <code>\"id\"</code> Integer (e.g., <code>0</code>, <code>1</code>) Unique identifier of the node. <code>\"code_snippet\"</code> String (e.g., <code>\"ubsr_package math\"</code>) A snippet of code or a description of the node. <code>\"node_type\"</code> String (e.g., <code>\"ubsr_root\"</code>, <code>\"ubsr_package\"</code>, etc.) Type of node representing various syntactic concepts. <code>\"parents\"</code> Array of Integers (e.g., <code>[1, 2]</code>) List of parent node IDs. <code>\"children\"</code> Array of Integers (e.g., <code>[1, 2]</code>) List of child node IDs. \"metadata\" (within nodes): <code>\"info\"</code> String General information about the node. <code>\"language\"</code> String (<code>\"cpp\"</code>, <code>\"python\"</code>, etc.) Programming language of the node. <code>\"original_code\"</code> String (e.g., <code>\"int main() {...}\"</code>) Original code snippet corresponding to the node. <code>\"loc_original_code\"</code> Integer Line of code of the concept. \"edges\": <code>\"directed_relation\"</code> String (<code>\"parent_node\"</code>) Type of relationship between nodes e.g. parent-child. <code>\"metadata\"</code> Object Additional metadata for the edge, which can be empty. <p>As shown in Table 2, the framework standardizes code representation by categorizing languages within these paradigms for 21 languages. In cases where certain concepts are absent in a language, they are marked as NA in the table. The base syntactic concepts extracted from the UBSR derived from code can be used to derive syntactic and semantic insights of the code data.</p> <p>Table 2: Base Syntactic Concepts Supported by the UBSR across Different Syntactical Paradigms</p> Syntactical Paradigms Languages Supported (Known*) Package Function Comment C-like Syntax C*, Java*, C#, CPP, Objective C, Rust, Golang, Kotlin Yes Yes Yes Scripting and Dynamic Syntax Python*, JavaScript*, Dart, Typescript Yes Yes Yes QML Yes NA Yes Perl Yes Yes NA Functional and Expression-Oriented Syntax Haskell*, Elm*, Agda, D, Nim, Scala Yes Yes Yes Ocaml Yes NA Yes <p>Offline Path for Syntactic Rule Generation</p> <p>The offline path is critical for expanding and refining the syntactic rule database, enabling the UBSR framework to adapt to new languages and syntactic constructs. This process leverages LLMs to generate syntactic rules for languages that are not yet included in the rule database. To achieve this, we utilize a Few-shot Chain of Thought prompting technique, guiding the LLM through a step-by-step rule generation process. By providing carefully curated training exemplars and detailed instructions, this method ensures the LLM can accurately generalize from these examples to produce effective syntactic rules for a wide range of languages. This structured approach enhances the flexibility of the UBSR framework, allowing it to seamlessly handle evolving language constructs.</p> <p>The implementation for UI-based offline customization tool is present here. To run the tool, use the following command.</p> <p><code>streamlit run generic_LLM_runner_app.py</code></p> <p>The high-level system design is as follows:</p> <p></p> <p>For each new target language, the offline phase is utilized to create deterministic rules by harnessing the capabilities of LLMs and working with exemplar code samples from the target language. In this process, Workflow W1 facilitates the creation of rules around syntactic structures based on exemplar code samples, while Workflow W2 is used to establish semantic dimensions for profiling. Subsequently, we derive rules that connect syntactic constructs to the predefined semantic concepts. These rules are then stored in a rule database, ready to be employed during the online phase.</p> <p>In the online phase, the system dynamically generates profiling outputs for any incoming code snippets. This is achieved by extracting concepts from the snippets using the rules in the database and storing these extractions in a tabular format. The structured tabular format allows for generating additional concept columns, which are then utilized to create comprehensive profiling reports.</p>"},{"location":"transforms/code/code_profiler/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding code_profiler_transform  configuration for values are as follows:</p> <ul> <li>content - specifies the column name in the dataframe that has the code snippet</li> <li>programming_language - specifies the programming languages of the code snippet</li> </ul>"},{"location":"transforms/code/code_profiler/#running","title":"Running","text":"<p>Copy your input parquet file to <code>transforms/code/code_profiler/test-data/input</code>, the output will be created in a directory <code>transforms/code/code_profiler/output</code></p>"},{"location":"transforms/code/code_profiler/#running-the-samples","title":"Running the samples","text":"<p>The code profiler can be run on mach-arm64 and x86_64 host architecture. Depending on your host architecture, please change the <code>RUNTIME_HOST_ARCH</code> in the Makefile. <pre><code># values possible mach-arm64, x86_64\nexport RUNTIME_HOST_ARCH=x86_64\n</code></pre> If you are using mac, you may need to permit your Mac to load the .so from the security settings. Generally, you get the pop-up under the tab security while running the transform.</p> <p></p> <p>To run the samples, use the following <code>make</code> target</p> <ul> <li><code>run-cli-sample</code> - runs dpk_code_profiler/transform.py using command line args</li> </ul> <p>This target will activate the virtual environment and sets up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code_profiler/#testing","title":"Testing","text":"<p>Following the testing strategy of data-processing-lib</p> <p>Currently we have: - Unit test - Integration test</p>"},{"location":"transforms/code/code_profiler/#document-quality-ray-transform","title":"Document Quality Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/code_profiler/#configuration-and-command-line-options_1","title":"Configuration and command line Options","text":"<p>Document Quality configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/code/code_profiler/#running_1","title":"Running","text":""},{"location":"transforms/code/code_profiler/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e., TransformLauncher), In addition to those available to the transform as defined here, the set of  launcher options are available.</p>"},{"location":"transforms/code/code_profiler/#running-the-samples_1","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target</p> <ul> <li><code>run-ray-cli-sample</code> - runs dpk_code_profiler/ray/transform.py using command line args</li> </ul> <p>This target will activate the virtual environment and sets up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-ray-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code_profiler/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code_quality/","title":"Code Quality Transform","text":"<p>The Code Quality transforms  captures code specific metrics of input data. Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/code_quality/kfp_ray/","title":"Code Quality Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/code_quality/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the Code Quality Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/code_quality/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/code_quality/python/","title":"Code Quality","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/code_quality/python/#summary","title":"Summary","text":"<p>This module captures code specific metrics of input data. The implementation is borrowed from the work done in CodeParrot and StarCoder projects. In the current implementation, the module includes the following metrics &amp; reports each metrics in individual column:</p> <ul> <li>line specific metrics include mean &amp; max line length</li> <li>character and token ratio - uses the input tokenizer to tokenize the input data &amp; measure the ratio between the characters and tokens</li> <li>identifies the high occurrence of the keywords \"test \" or \"config\" and tags them as config or test samples</li> <li>tags the samples as autogenerated if the sample contains keywords like <code>auto-generated</code>, <code>autogenerated</code> or <code>automatically generated</code></li> <li>programming language specific identification, where:<ul> <li>if the input sample is <code>python</code> programming language and sample has no reference to constructs like def, class, it is highlighted as <code>has_no_keywords</code> </li> </ul> </li> </ul> <p>This module adds the following fields into the output file:</p> <ul> <li>line_mean</li> <li>line_max</li> <li>total_num_lines</li> <li>avg_longest_lines</li> <li>alphanum_frac</li> <li>char_token_ratio</li> <li>autogenerated</li> <li>config_or_test</li> <li>has_no_keywords</li> <li>has_few_assignments</li> <li>is_xml</li> <li>is_html</li> </ul> <p>It uses a tokenizer to collect metrics specific to token ratio.  It is designed to download the tokenizer from the Huggingface if the input tokenizer is not found in the local cache. By default, it uses codeparrot/codeparrot tokenizer.</p>"},{"location":"transforms/code/code_quality/python/#running","title":"Running","text":""},{"location":"transforms/code/code_quality/python/#launcher-command-line-options","title":"Launcher Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by the launcher</p> <ul> <li>\"--contents_column_name\" - input a column name which contains data to process. The default column name: <code>contents</code></li> <li>\"--language_column_name\" - input a column name which contains programming language details. The default column name: <code>language</code></li> <li>\"--tokenizer\" - input a tokenizer to convert the data into tokens. The default tokenizer is <code>codeparrot/codeparrot</code></li> <li>\"--hf_token\" - input the Hugging Face auth token to download the tokenizer. This option is only required for the tokenizer's whose access is restricted in Hugging Face.</li> </ul>"},{"location":"transforms/code/code_quality/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/code_quality_transform_python.py using command line args</li> <li><code>run-local-sample</code> - runs src/code_quality_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code_quality/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/code_quality/ray/","title":"Code Quality","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/code_quality/ray/#summary","title":"Summary","text":"<p>This project enables the python malware transform to be run in a Ray runtime. Please see the python project for details on the transform implementation and use.</p>"},{"location":"transforms/code/code_quality/ray/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>Transform configuration options are the same as the base python transform.</p>"},{"location":"transforms/code/code_quality/ray/#running","title":"Running","text":""},{"location":"transforms/code/code_quality/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of launcher options are available.</p>"},{"location":"transforms/code/code_quality/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-ray-sample</code> - runs src/code_quality_transform.py using command line args</li> <li><code>run-local-ray-sample</code> - runs src/code_quality_local_ray.py</li> <li><code>run-s3-ray-sample</code> - runs src/code_quality_s3_ray.py<ul> <li>Requires prior installation of minio, depending on your platform (e.g., from here  and here   and invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/code_quality/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/header_cleanser/","title":"Header Cleanser Transform","text":"<p>The Header cleanser transforms  Detect and remove license and copyright of input data. Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime.</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/header_cleanser/kfp_ray/","title":"Header Cleanser Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/header_cleanser/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the header cleanser Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/header_cleanser/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/header_cleanser/python/","title":"Header cleanser","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/header_cleanser/python/#contributors","title":"Contributors","text":"<ul> <li>Yash Kalathiya (yashkalathiya164@gmail.com)</li> </ul>"},{"location":"transforms/code/header_cleanser/python/#desciption","title":"Desciption","text":"<p>The Header Cleanser module is a versatile tool designed to remove license and copyright headers from code files. It supports over 90 programming languages and utilizes the ScanCode Toolkit to identify license and copyright information within the codebase.</p>"},{"location":"transforms/code/header_cleanser/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding configuration for values are as follows:</p> <ul> <li>contents_column_name - used to define input column name. Default value is 'contents'.</li> <li>license - write 'true' to remove license from input data else 'false'. By default set as 'true'.</li> <li>copyright - write 'true' to remove copyright from input data else 'false'. by default set as 'true'.</li> </ul>"},{"location":"transforms/code/header_cleanser/python/#running","title":"Running","text":"<p>You can run the header_cleanser_local.py (python-only implementation) or header_cleanser_local_ray.py (ray-based  implementation) to transform the <code>test1.parquet</code> file in test input data to an <code>output</code> directory.  The directory will contain both the new annotated <code>test1.parquet</code> file and the <code>metadata.json</code> file.</p>"},{"location":"transforms/code/header_cleanser/python/#running_1","title":"Running","text":""},{"location":"transforms/code/header_cleanser/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the launcher. * --header_cleanser_contents_column_name - set the contents_column_name configuration key. * --header_cleanser_document_id_column_name - set the document_id_column_name configuration key. * --header_cleanser_license - set the license configuration key. * --header_cleanser_copyright - set the copyright configuration key.  * --header_cleanser_n_processes - set the n_processes configuration key.  * --header_cleanser_tmp_dir - set the tmp_dir configuration key.  * --header_cleanser_timeout - set the timeout configuration key.  * --header_cleanser_skip_timeout - set the skip_timeout configuration key. </p>"},{"location":"transforms/code/header_cleanser/python/#input-and-output","title":"Input and Output","text":""},{"location":"transforms/code/header_cleanser/python/#input","title":"Input","text":"<ul> <li>File Format: Parquet file containing code.</li> <li>Input Column: The code should be in a column named <code>content</code>.</li> <li>Sample Input: Sample Input File</li> </ul>"},{"location":"transforms/code/header_cleanser/python/#output","title":"Output","text":"<ul> <li>File Format: Parquet file with the updated code in the same column.</li> <li>Sample Output: Sample Output File</li> </ul>"},{"location":"transforms/code/header_cleanser/python/#cli-syntax","title":"CLI Syntax","text":"<p>When invoking the CLI, use the following syntax for these parameters: <pre><code>--header_cleanser_&lt;parameter_name&gt;\n</code></pre> For example: <pre><code>--header_cleanser_content_column_name='content'\n</code></pre></p>"},{"location":"transforms/code/header_cleanser/python/#example","title":"Example","text":""},{"location":"transforms/code/header_cleanser/python/#sample-input-code","title":"Sample Input Code:","text":"<pre><code>/*\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage com.jstevenperry.intro;\n\nimport java.util.logging.Logger;\n\n// This is the main public class representing a Person\npublic class Person {\n    private static final Logger logger = Logger.getLogger(Person.class.getName());\n\n    private String name;\n    private int age;\n    private int height;\n    private int weight;\n    private String eyeColor;\n    private String gender;\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    public void setAge(int age) {\n        this.age = age;\n    }\n\n    public int getHeight() {\n        return height;\n    }\n\n    public void setHeight(int height) {\n        this.height = height;\n    }\n\n    public int getWeight() {\n        return weight;\n    }\n\n    public void setWeight(int weight) {\n        this.weight = weight;\n    }\n\n    public String getEyeColor() {\n        return eyeColor;\n    }\n\n    public void setEyeColor(String eyeColor) {\n        this.eyeColor = eyeColor;\n    }\n\n    public String getGender() {\n        return gender;\n    }\n\n    public void setGender(String gender) {\n        this.gender = gender;\n    }\n\n    public Person(String name, int age, int height, int weight, String eyeColor, String gender) {\n        super();\n        this.name = name;\n        this.age = age;\n        this.height = height;\n        this.weight = weight;\n        this.eyeColor = eyeColor;\n        this.gender = gender;\n\n        logger.info(\"Created Person object with name '\" + getName() + \"'\");\n    }\n}\n</code></pre>"},{"location":"transforms/code/header_cleanser/python/#sample-output-with-default-parameters","title":"Sample Output (with default parameters):","text":"<pre><code>package com.jstevenperry.intro;\n\nimport java.util.logging.Logger;\n\n/// This is the main public class representing a Person\npublic class Person {\n\n    private static final Logger logger = Logger.getLogger(Person.class.getName());\n\n    private String name;\n    private int age;\n    private int height;\n    private int weight;\n    private String eyeColor;\n    private String gender;\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public int getAge() {\n        return age;\n    }\n\n    public void setAge(int age) {\n        this.age = age;\n    }\n\n    public int getHeight() {\n        return height;\n    }\n\n    public void setHeight(int height) {\n        this.height = height;\n    }\n\n    public int getWeight() {\n        return weight;\n    }\n\n    public void setWeight(int weight) {\n        this.weight = weight;\n    }\n\n    public String getEyeColor() {\n        return eyeColor;\n    }\n\n    public void setEyeColor(String eyeColor) {\n        this.eyeColor = eyeColor;\n    }\n\n    public String getGender() {\n        return gender;\n    }\n\n    public void setGender(String gender) {\n        this.gender = gender;\n    }\n\n    public Person(String name, int age, int height, int weight, String eyeColor, String gender) {\n        super();\n        this.name = name;\n        this.age = age;\n        this.height = height;\n        this.weight = weight;\n        this.eyeColor = eyeColor;\n        this.gender = gender;\n\n        logger.info(\"Created Person object with name '\" + getName() + \"'\");\n    }\n}\n</code></pre>"},{"location":"transforms/code/header_cleanser/python/#sample-notebook","title":"Sample Notebook","text":"<p>Check out the example notebook for further details.</p>"},{"location":"transforms/code/header_cleanser/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/header_cleanser/ray/","title":"Header cleanser","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/header_cleanser/ray/#summary","title":"Summary","text":"<p>This module is designed to detect and remove license and copyright information from code files. It leverages the ScanCode Toolkit to accurately identify and process licenses and copyrights in various programming languages.</p> <p>After locating the position of license or copyright in the input code/sample, this module delete/remove those lines and returns the updated code as parquet file.</p>"},{"location":"transforms/code/header_cleanser/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>This project wraps the header cleanser transform with a Ray runtime.</p>"},{"location":"transforms/code/header_cleanser/ray/#running","title":"Running","text":""},{"location":"transforms/code/header_cleanser/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here,  the set of launcher options are available.</p>"},{"location":"transforms/code/header_cleanser/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-ray-sample</code> - runs src/header_cleanser_transform.py using command line args</li> <li><code>run-local-ray-sample</code> - runs src/header_cleanser_local_ray.py</li> <li><code>run-s3-ray-sample</code> - runs src/header_cleanser_s3_ray.py<ul> <li>Requires prior invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-ray-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/header_cleanser/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/license_select/","title":"License Select","text":"<p>The License Select transform checks if the <code>license</code> of input data is in approved/denied list. It is implemented as per the set of transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/license_select/python/","title":"License Select","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/license_select/python/#summary","title":"Summary","text":"<p>The License Select transform checks if the <code>license</code> of input data is in approved/denied list. It is implemented as per the set of transform project conventions the following runtimes are available: This filter scans the license column of an input dataset and appends the <code>license_status</code> column to the dataset.</p> <p>The type of the license column can be either string or list of strings. For strings, the license name is checked against the list of approved licenses. For list of strings, each license name in the list is checked against the list of approved licenses, and all must be approved.</p> <p>If the license is approved, the license_status column contains True; otherwise False. </p>"},{"location":"transforms/code/license_select/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding license_select configuration for values are as follows:</p> <p>The transform can be configured with the following key/value pairs from the configuration dictionary.</p> <pre><code># Sample params dictionary passed to the transform\n\n{ \n\"license_select_params\" : {\n        \"license_column_name\": \"license\",\n        \"deny_licenses\": False,\n        \"licenses\": [ 'MIT', 'Apache'],\n        \"allow_no_license\": False,\n    }\n}\n</code></pre> <p>license_column_name - The name of the column with licenses.</p> <p>deny_licenses - A boolean value, True for denied licesnes, False for approved licenses.</p> <p>licenses - A list of licenses used as approve/deny list.</p> <p>allow_no_license - A boolean value, used to retain the values with no license in the column <code>license_column_name</code> </p>"},{"location":"transforms/code/license_select/python/#running","title":"Running","text":""},{"location":"transforms/code/license_select/python/#launcher-command-line-options","title":"Launcher Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by the launcher.</p> <p><code>--lc_license_column_name</code> - set the name of the column holds license to process</p> <p><code>--lc_allow_no_license</code> - allow entries with no associated license (default: false)</p> <p><code>--lc_licenses_file</code> - S3 or local path to allowed/denied licenses JSON file</p> <p><code>--lc_deny_licenses</code> - allow all licences except those in licenses_file (default: false)</p> <ul> <li> <p>The optional <code>lc_license_column_name</code> parameter is used to specify the column name in the input dataset that contains the license information. The default column name is license.</p> </li> <li> <p>The optional <code>lc_allow_no_license</code> option allows any records without a license to be accepted by the filter. If this option is not set, records without a license are rejected.</p> </li> <li> <p>The required <code>lc_licenses_file</code> options allows a list of licenses to be specified. An S3 or local file path should be supplied (including bucket name, for example: bucket-name/path/to/licenses.json) with the file contents being a JSON list of strings. For example:</p> </li> </ul> <p>[     'Apache-2.0',     'MIT'    ]</p> <ul> <li>The optional <code>lc_deny_licenses</code> flag is used when <code>lc_licenses_file</code> specifies the licenses that will be rejected, with all other licenses being accepted. These parameters do not affect handling of records with no license information, which is dictated by the allow_no_license option.</li> </ul>"},{"location":"transforms/code/license_select/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following make targets</p> <p><code>run-cli-sample</code></p> <p><code>run-local-python-sample</code> </p> <p>These targets will activate the virtual environment and set up any configuration needed. Use the -n option of make to see the detail of what is done to run the sample.</p> <p>For example, <pre><code>make run-cli-sample\n</code></pre> ... Then</p> <p>ls output To see results of the transform.</p>"},{"location":"transforms/code/license_select/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/license_select/ray/","title":"License Select","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/license_select/ray/#summary","title":"Summary","text":"<p>This project wraps the license select transform with a Ray runtime.</p>"},{"location":"transforms/code/license_select/ray/#running","title":"Running","text":""},{"location":"transforms/code/license_select/ray/#launcher-command-line-options","title":"Launcher Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  launcher options are available.</p>"},{"location":"transforms/code/license_select/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-ray-sample</code> </li> <li><code>run-local-ray-sample</code> </li> <li><code>run-s3-ray-sample</code> <ul> <li>Requires prior invocation of <code>make minio-start</code> to load data into local minio for S3 access.</li> </ul> </li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example, </p> <pre><code>make run-cli-ray-sample\n...\n</code></pre> <p>Then </p> <p><pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/license_select/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/malware/","title":"Malware Transform","text":"<p>The Malware Transform  scans code parquet files containing code documents to identify the potential presence of a virus signature adding a column to the input parquet file identifying the virus signature. Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/malware/kfp_ray/","title":"Code Malware Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/malware/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/malware/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/malware/python/","title":"Malware Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/malware/python/#summary","title":"Summary","text":"<p>This filter scans the 'contents' column of an input table using ClamAV, and outputs corresponding tables containing 'virus_detection' column (by default).</p> <p>If a virus is detected, the 'virus_detection' column contains the detected virus signature name; otherwise null.</p>"},{"location":"transforms/code/malware/python/#pre-requisites-for-mac","title":"Pre-requisites for Mac","text":"<p>For testing and running this transform on local, we are using a unix socket shared with a docker container. However, docker for mac doesn't support a shared unix socket. For Mac users, ClamAV will be set up by running <code>make venv</code>. If thet script doesn't work for you, please ensure that you have installed <code>clamd</code> command, and it runs with a local unix socket: <code>/var/run/clamav/clamd.ctl</code>.</p>"},{"location":"transforms/code/malware/python/#example-for-manual-set-up-for-mac","title":"Example for manual set up for Mac:","text":"<ol> <li>Install ClamAV with Homebrew     <pre><code>brew install clamav\n</code></pre></li> <li>Copy and edit config files.     <pre><code>cp $(brew --prefix)/etc/clamav/clamd.conf.sample $(brew --prefix)/etc/clamav/clamd.conf\nsed -i '' -e 's/^Example/# Example/' $(brew --prefix)/etc/clamav/clamd.conf\necho \"DatabaseDirectory /var/lib/clamav\" &gt;&gt; $(brew --prefix)/etc/clamav/clamd.conf\necho \"LocalSocket /var/run/clamav/clamd.ctl\" &gt;&gt; $(brew --prefix)/etc/clamav/clamd.conf\ncp $(brew --prefix)/etc/clamav/freshclam.conf.sample $(brew --prefix)/etc/clamav/freshclam.conf\nsed -i '' -e 's/^Example/# Example/' $(brew --prefix)/etc/clamav/freshclam.conf\necho \"DatabaseDirectory /var/lib/clamav\" &gt;&gt; $(brew --prefix)/etc/clamav/freshclam.conf\n</code></pre></li> <li>Create a directory for a local unix socket     <pre><code>sudo mkdir -p /var/run/clamav\nsudo chown $(id -u):$(id -g) /var/run/clamav\n</code></pre></li> <li>Create a direcotry for a database of ClamAV     <pre><code>sudo mkdir -p /var/lib/clamav\nsudo chown $(id -u):$(id -g) /var/lib/clamav\n</code></pre></li> <li>Update a database of ClamAV     <pre><code>freshclam\n</code></pre></li> <li>Edit <code>venv/bin/activate</code>, and add following lines to start <code>clamd</code> by <code>source venv/bin/activate</code> <pre><code>if [ ! -e /var/run/clamav/clamd.ctl ]; then\n    clamd --config-file=$(brew --prefix)/etc/clamav/clamd.conf\nfi\n</code></pre></li> </ol>"},{"location":"transforms/code/malware/python/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>The set of dictionary keys holding MalwareTransform  configuration for values are as follows:</p> <ul> <li>malware_input_column - specifies the input column's name to scan. (default: <code>contents</code>)</li> <li>malware_output_column - specifies the output column's name of the detected virus signature name. (default: <code>virus_detection</code>)</li> </ul>"},{"location":"transforms/code/malware/python/#metadata-fields","title":"Metadata Fields","text":"<p>As shown in the output of the local run of malware transform, the metadata contains several statistics: * Global statistics:   * <code>infected</code>: total number of documents (rows) in which any malwares were detected.    * <code>clean</code>: total number of documents (rows) in which no malwares were detected.</p>"},{"location":"transforms/code/malware/python/#running","title":"Running","text":""},{"location":"transforms/code/malware/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by the launcher.</p> <pre><code>  --malware_input_column MALWARE_INPUT_COLUMN\n                        input column name\n  --malware_output_column MALWARE_OUTPUT_COLUMN\n                        output column name\n</code></pre>"},{"location":"transforms/code/malware/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/malware_transform_python.py using command line args</li> <li><code>run-local-sample</code> - runs src/malware_local.py</li> <li><code>run-local-python-sample</code> - runs src/malware_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/malware/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/malware/ray/","title":"Malware Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/malware/ray/#summary","title":"Summary","text":"<p>This project enables the python malware transform to be run in a Ray runtime. Please see the python project for details on the transform implementation and use.</p>"},{"location":"transforms/code/malware/ray/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>Transform configuration options are the same as the base python transform.</p>"},{"location":"transforms/code/malware/ray/#running","title":"Running","text":""},{"location":"transforms/code/malware/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of launcher options are available.</p>"},{"location":"transforms/code/malware/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/malware_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/malware_local.py</li> <li><code>run-local-ray-sample</code> - runs src/malware_local_ray.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/malware/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/proglang_select/","title":"Programming Language Selection Transform","text":"<p>The Programming Language Selection Transform  annotates input parquet files to add a True/False column indicating if the row's language matches one of those specified in the transform configuration. Per the set of  transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation  implementation.</li> <li>ray - enables the running of the base python transformation in a Ray runtime</li> <li>kfp_ray - enables running the ray docker image  in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul>"},{"location":"transforms/code/proglang_select/kfp_ray/","title":"Programming Language Select Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/code/proglang_select/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/code/proglang_select/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/code/proglang_select/python/","title":"Programming Language Select","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/proglang_select/python/#summary","title":"Summary","text":"<p>This is a transform which can be used while preprocessing code data. It allows the user to specify the programming languages for which the data should be identifies as matching a defined set of programming languages. It adds a new annotation column which can specify boolean True/False based on whether the rows belong to the specified programming languages. The rows which belongs to the programming languages which are not matched are annotated as False.</p> <p>It requires a text file specifying the allowed languages. It is specified by the command line param <code>proglang_select_allowed_langs_file</code>.  A sample file is included at <code>test-data/languages/allowed-code-languages.lst</code>. The column specifying programming languages is to be specified by commandline params <code>proglang_select_language_column</code>.</p>"},{"location":"transforms/code/proglang_select/python/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding configuration for values are as follows:</p> <ul> <li>proglang_select_allowed_langs_file - specifies the location of the list of supported languages</li> <li>proglang_select_language_column - specifies the name of the column containing the language</li> <li>proglang_select_output_column - specifies the name of the annotation column appended to the parquet. </li> <li>proglang_select_return_known - specifies whether to return supported or unsupported languages</li> </ul>"},{"location":"transforms/code/proglang_select/python/#running","title":"Running","text":""},{"location":"transforms/code/proglang_select/python/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by the launcher.</p> <pre><code>  --proglang_select_allowed_langs_file PROGLANG_MATCH_ALLOWED_LANGS_FILE\n                        Path to file containing the list of languages to be matched.\n  --proglang_select_language_column PROGLANG_MATCH_LANGUAGE_COLUMN\n                        The column name holding the name of the programming language assigned to the document\n  --proglang_select_output_column PROGLANG_MATCH_OUTPUT_COLUMN\n                        The column name to add and that contains the matching information\n  --proglang_select_s3_cred PROGLANG_MATCH_S3_CRED\n                        AST string of options for s3 credentials. Only required for S3 data access.\n                        access_key: access key help text\n                        secret_key: secret key help text\n                        url: optional s3 url\n                        region: optional s3 region```\n</code></pre>"},{"location":"transforms/code/proglang_select/python/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/proglang_select_transform.py using command line args</li> <li><code>run-local-sample</code> - runs src/proglang_select_local.py</li> <li><code>run-local-python-sample</code> - runs src/proglang_select_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/proglang_select/python/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/proglang_select/ray/","title":"Programming Language Select","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/proglang_select/ray/#summary","title":"Summary","text":"<p>This project enables the python malware transform to be run in a Ray runtime. Please see the python project for details on the transform implementation and use.</p>"},{"location":"transforms/code/proglang_select/ray/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>Transform configuration options are the same as the base python transform.</p>"},{"location":"transforms/code/proglang_select/ray/#running","title":"Running","text":""},{"location":"transforms/code/proglang_select/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of launcher options are available.</p>"},{"location":"transforms/code/proglang_select/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/proglang_select_transform_ray.py using command line args</li> <li><code>run-local-ray-sample</code> - runs src/proglang_select_local_ray.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/proglang_select/ray/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/code/repo_level_ordering/","title":"Repo Level Order Transform","text":"<p>The repo level order transforms can arrange code data by repos and additionally run transformations on it like sorting, grouping by language and combining rows to a single row.  Per the set of transform project conventions the following runtimes are available:</p> <ul> <li>ray - enables the running of the base python transformation in a Ray runtime</li> </ul>"},{"location":"transforms/code/repo_level_ordering/ray/","title":"Repo Level Order Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#summary","title":"Summary","text":"<p>This transform does repository level packing of data and arranging them to prioritise semantic dependancies. This  was done to prepare long context data for Scaling Granite Code Models to 128K Context  . Quoting the paper. </p> <p>To create long-context data, we develop a new approach that packs files from the same repository together, arranging them to prioritize semantic dependencies. We identify these dependencies by analyzing file imports and create a directed acyclic graph, where each file is a node and edges represent API imports between files. After breaking any cycles in the graph, we perform a topological sort to establish an ordering of files based on their semantic dependencies. We then organize the files in a repository by placing documentation and build files first, followed by the ordered set of files with semantic dependencies, and finally the remaining non-connected files. These non-connected files are arranged according to their folder structure, using a depth-first search to traverse the repository. Finally, we determine the dominant programming language of a repository based on file extensions and presence of build files, to organise repo-ordered files by programming languages</p> <p>This transform can group the data by <code>repo_name</code> and apply additional transformations like( sorting or output_by_language or combining rows) on the  grouped data. This transform requires the input data to have at least the following columns: </p> <ul> <li> <p>repo name: Name of the repo, it is used for grouping in this transform.</p> </li> <li> <p>title : Which is usually file path.</p> </li> <li> <p>language: Programming language of content</p> </li> </ul> <p>The input data for this transform should be in parquet format. The input data is expected to have code data arranged in rows such that each row represents a file. The required columns in the input data shoud correspond to a) repository name b) file path c) content. This transform supports searching the repo across the whole dataset and writing the files of a repo as a single  parquet file. </p> <p>The transform gives the option to write the repo to file in the following ways.</p> <p>a) sort the repo content by file path and write a parquet with multiple rows</p> <p>b) sort the repo content by file path and write a parquet with a single combined row.</p> <p>c) sort the repo content in semantic ordering and write the parquet with multiple rows.</p> <p>d) sort the repo content in semantic ordering and write the parquet with a single combined row.</p> <p>Additionally this transform can grooup the repos in the folders named after the most dominant language in the repo. For more information on this transform, please refer to here.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>This transform is dependant on ray runtime. </p> <p>Transform Configuration.</p> <ul> <li>For output:    either the output is directly a file or if dominant language flag is enabled, it should output    it in folder of that langauge.</li> <li>Enable sorting:     if sorting is enabled, we should be able to choose one of the available ones (SORT_BY_PATH, SORT_SEMANTIC, SORT_SEMANTIC_NORMALISED)<ul> <li>SORT_BY_PATH: Normal ascending sort by filename</li> <li>SORT_SEMANTIC: Uses semantic analysis to sort the files within a repo.</li> <li>SORT_SEMANTIC_NORMALISED: Normalises the title to remove https:// prefix, if present, and then runs SORT_SEMANTIC</li> </ul> </li> <li>Enable superrows:    of writing superrows is enabled, then it combines rows of a repo to a single row otherwise writes normal.</li> </ul> <p>Limitation of transform. This expects a flat input folder structure with parquet files.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#running","title":"Running","text":""},{"location":"transforms/code/repo_level_ordering/ray/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>Ray runtime launcher options are available.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-local-sample</code> - runs src/repo_level_order_local_ray.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-local-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/code/repo_level_ordering/ray/#using-the-cli","title":"Using the CLI","text":"<ol> <li>Simplest Usage:</li> </ol> <p>Running on local computer setup with data available on local filesystem. Sorting the data at repo level using default algorithm. (SORT_BY_PATH). In this configuration, the workers run locally, store is local. This is a recommended method if number of available cpus is less.</p> <pre><code>export INPUT_FOLDER=\"input_data_folder/\"\nexport OUTPUT_FOLDER=\"output_data_folder/\"\n\n\nlocal_conf=\"{'input_folder' : '$INPUT_FOLDER', 'output_folder' : '$OUTPUT_FOLDER'  }\"\nrm -rf /tmp/mystore # remove if it exists\n\npython src/repo_level_order_transform_ray.py \\\n       --run_locally True \\\n       --data_local_config \"$local_conf\" \\\n       --repo_lvl_store_type  local  \\\n       --repo_lvl_store_backend_dir '/tmp/mystore' \\\n       --repo_lvl_sorting_enabled true,\n       --repo_lvl_sorting_algo SORT_SEMANTIC\n</code></pre> <p>NOTE: Make sure you use an empty folder as <code>repo_lvl_store_backend_dir</code>. </p> <ol> <li>Running on Cluster</li> </ol> <p>This configuration can be used when data is on S3 like COS storage and computation is done on a ray cluster.</p> <p>Recommended <code>repo_lvl_store_type</code> for cluster is 'ray'. We need to dedicate some ray actors specially for ray store</p> <p>for example, if we have a ray cluster with 100 actors available. We can dedicate some for the store. Let us randomly use 35 for computation and 10 for store and let remaining free. </p> <p>NOTE: The transform reads parquet files in two stages and each stage has its own actor pool for computations. The  actor pool to read parquet data in the first stage is managed by the library and the actor pool used in the second stage is managed by the ray runtime of the transform. The number of actors in each pool is same and is configured using <code>--runtime_num_workers</code>. So we have to carefully choose number of actors required based on the number resources available.  The number of workers/actors should be less than 35% of total resources available. </p> <p>We need to add the following cli args:</p> <p><code>--runtime_num_workers 35</code> <code>--repo_lvl_store_type \"ray\" --repo_lvl_store_ray_cpus 0.2 --repo_lvl_store_ray_nworkers 10</code> </p> <p>When we want the output with the following configuration enabled:</p> <p>NOTE: store_backend=<code>s3/local</code> are persistent stores and retain the mappings stores in them, they need to be cleaned/deleted after use. It is recommented to use a different location for store if data is different. They are added to aid in large data processing in multiple stages.   store_backend=<code>ray</code>, is not presistent and ephimeral and does not retain any data. It is the simplest to use if resources are available.</p> <pre><code>export INPUT_FOLDER=\"input_cos_bucket\"\nexport OUTPUT_FOLDER=\"output_cos_bucket\"\n\ns3_kreds=\"{ ... }\" # in the deafult way used for all transforms. \ns3_conf=\"{'input_folder' : '$INPUT_FOLDER', 'output_folder' : '$OUTPUT_FOLDER'  }\"\n\npython src/repo_level_order_transform_ray.py \\\n       --run_locally True \\\n       --data_s3_cred \"$s3_kreds\" \\\n       --data_s3_config \"$s3_conf\" \\\n       --repo_lvl_store_type  ray  \\\n       --repo_lvl_combine_rows True\\\n       --repo_lvl_sorting_enabled True\\\n       --repo_lvl_store_ray_cpus 0.2 \\\n       --repo_lvl_store_ray_nworkers 1 \\\n       --repo_lvl_sorting_algo SORT_SEMANTIC \\\n       --repo_lvl_output_by_langs True   \n</code></pre>"},{"location":"transforms/code/repo_level_ordering/ray/src/dpk_repo_level_order/internal/sorting/","title":"Index","text":""},{"location":"transforms/code/repo_level_ordering/ray/src/dpk_repo_level_order/internal/sorting/#sorting-module","title":"Sorting Module","text":"<p>This module supports sorting the pandas dataframes by files. It can sort files in lexical  or semantic order.</p> <p>It uses emerge-viz for source code analysis and networkx for building network.</p> <p>Used like:</p> <pre><code>  from dpk_repo_level_order.internal.sorting.semantic_ordering import (\n    sort_by_path,\n    sort_sem,\n    check_and_update_title,\n)\n</code></pre> <p>Types of sorting:</p> <p>They require <code>title</code> field. </p> <p>There is a <code>check_and_update_title</code> utility function to normalise title for usage with these sorting functions. </p> <ol> <li>Lexical sorting of paths: <code>sort_by_path</code></li> </ol> <pre><code> df: pd.DataFrame\n sorted_df=sort_by_path(df=df, \n              logger=logger, \n              title_column_name=title_column_name)\n</code></pre> <ol> <li>Semantic sorting: <code>sort_sem</code> </li> </ol> <pre><code> df: pd.DataFrame\n\n check_and_update_title(df)\n sorted_df = sort_sem(df=df, \n           logger=logger, \n           title_column_name=title_column_name)\n</code></pre>"},{"location":"transforms/language/doc_chunk/","title":"Chunk documents Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/doc_chunk/#contributors","title":"Contributors","text":"<ul> <li>Michele Dolfi (dol@zurich.ibm.com)</li> </ul>"},{"location":"transforms/language/doc_chunk/#description","title":"Description","text":"<p>This transform is chunking documents. It supports multiple chunker modules (see the <code>chunking_type</code> parameter).</p> <p>When using documents converted to JSON, the transform leverages the Docling Core <code>HierarchicalChunker</code> to chunk according to the document layout segmentation, i.e. respecting the original document components as paragraphs, tables, enumerations, etc. It relies on documents converted with the Docling library in the pdf2parquet transform using the option <code>contents_type: \"application/json\"</code>, which provides the required JSON structure.</p> <p>When using documents converted to Markdown, the transform leverages the Llama Index <code>MarkdownNodeParser</code>, which is relying on its internal Markdown splitting logic.</p>"},{"location":"transforms/language/doc_chunk/#input","title":"Input","text":"input column name data type description the one specified in content_column_name configuration string the content used in this transform"},{"location":"transforms/language/doc_chunk/#output-format","title":"Output format","text":"<p>The output parquet file will contain all the original columns, but the content will be replaced with the individual chunks.</p>"},{"location":"transforms/language/doc_chunk/#tracing-the-origin-of-the-chunks","title":"Tracing the origin of the chunks","text":"<p>The transform allows to trace the origin of the chunk with the <code>source_doc_id</code> which is set to the value of the <code>document_id</code> column (if present) in the input table. The actual name of columns can be customized with the parameters described below.</p>"},{"location":"transforms/language/doc_chunk/#configuration","title":"Configuration","text":"<p>The transform can be tuned with the following parameters.</p> Parameter Default Description <code>chunking_type</code> <code>dl_json</code> Chunking type to apply. Valid options are <code>li_markdown</code> for using the LlamaIndex Markdown chunking, <code>dl_json</code> for using the Docling JSON chunking, <code>li_token_text</code> for using the LlamaIndex Token Text Splitter, which chunks the text into fixed-sized windows of tokens. <code>content_column_name</code> <code>contents</code> Name of the column containing the text to be chunked. <code>doc_id_column_name</code> <code>document_id</code> Name of the column containing the doc_id to be propagated in the output. <code>chunk_size_tokens</code> <code>128</code> Size of the chunk in tokens for the token text chunker. <code>chunk_overlap_tokens</code> <code>30</code> Number of tokens overlapping between chunks for the token text chunker. <code>output_chunk_column_name</code> <code>contents</code> Column name to store the chunks in the output table. <code>output_source_doc_id_column_name</code> <code>source_document_id</code> Column name to store the <code>doc_id</code> from the input table. <code>output_jsonpath_column_name</code> <code>doc_jsonpath</code> Column name to store the document path of the chunk in the output table. <code>output_pageno_column_name</code> <code>page_number</code> Column name to store the page number of the chunk in the output table. <code>output_bbox_column_name</code> <code>bbox</code> Column name to store the bbox of the chunk in the output table."},{"location":"transforms/language/doc_chunk/#usage","title":"Usage","text":""},{"location":"transforms/language/doc_chunk/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When invoking the CLI, the parameters must be set as <code>--doc_chunk_&lt;name&gt;</code>, e.g. <code>--doc_chunk_column_name_key=myoutput</code>.</p>"},{"location":"transforms/language/doc_chunk/#code-example","title":"Code example","text":"<p>See a sample notebook</p>"},{"location":"transforms/language/doc_chunk/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/doc_chunk/#testing","title":"Testing","text":"<p>Following the testing strategy of data-processing-lib</p> <p>Currently we have: - Unit test</p>"},{"location":"transforms/language/doc_chunk/#further-resource","title":"Further Resource","text":"<ul> <li>For the Docling Core <code>HierarchicalChunker</code></li> <li>https://ds4sd.github.io/docling/</li> <li>For the Markdown chunker in LlamaIndex</li> <li>Markdown chunking</li> <li>For the Token Text Splitter in LlamaIndex</li> <li>Token Text Splitter</li> </ul>"},{"location":"transforms/language/doc_chunk/#chunk-documents-ray-transform","title":"Chunk documents Ray Transform","text":""},{"location":"transforms/language/doc_chunk/#summary","title":"Summary","text":"<p>This project wraps the doc_chunck transform python implementation with a Ray runtime.</p>"},{"location":"transforms/language/doc_chunk/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>chunk documents configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/doc_chunk/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined above, the set of  launcher options are available.</p>"},{"location":"transforms/language/doc_chunk/#transforming-data-using-the-transform-image_1","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/doc_chunk/kfp_ray/","title":"chunk documents Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/doc_chunk/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the doc_chunk Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/doc_chunk/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/doc_quality/","title":"Document Quality Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/doc_quality/#contributors","title":"Contributors","text":"<ul> <li>Daiki Tsuzuku (dtsuzuku@jp.ibm.com)</li> </ul>"},{"location":"transforms/language/doc_quality/#description","title":"Description","text":"<p>This transform will calculate and annotate several metrics which are useful to assess the quality of the document. The document quality transform operates on text documents only</p>"},{"location":"transforms/language/doc_quality/#input","title":"Input","text":"input column name data type description the one specified in doc_content_column configuration string text whose quality will be calculated by this transform"},{"location":"transforms/language/doc_quality/#output-columns-annotated-by-this-transform","title":"Output columns annotated by this transform","text":"output column name data type description supported language docq_total_words int the total number of words ALL docq_mean_word_len int the mean of words' lengths ALL docq_symbol_to_word_ratio float the ratio of symbol-to-word ratio (Reference for symbols like emojis: https://textacy.readthedocs.io/en/0.11.0/api_reference/preprocessing.html, currently used symbol: <code>#</code>, <code>...</code>) ALL docq_sentence_count int the number of sentences ALL docq_curly_bracket_ratio float the ratio between the number of occurrences of <code>{</code> or <code>}</code> over the text length ALL docq_lorem_ipsum_ratio float the ratio between the number of occurrences of <code>lorem ipsum</code> over the text length. Lorem ipsum, or lipsum as it is sometimes known, is dummy text used in laying out print, graphic or web designs. ALL docq_contain_bad_word bool whether text containst bad words ALL docq_bullet_point_ratio float the ratio of lines starting with a bullet point ALL docq_ellipsis_line_ratio float the ratio of lines ending with an ellipsis ALL docq_alphabet_word_ratio float the ratio of words having at least one alphabetic character ALL docq_contain_common_en_words bool whether the given <code>text</code> contains common English words like <code>the</code>, <code>and</code>, <code>to</code>, <code>that</code>, <code>of</code>, <code>with</code>, <code>be</code>, and <code>have</code> ALL docq_avg_ja_sentence_len int average sentence length for an input text, inspired by an OSS HojiChar. ja docq_first_ja_alphabet_pos int first position of occurrence of Japanese alphabets (i.e., Hiragana or Katakana) ja <p>You can see more detailed backgrounds of some columns in Deepmind's Gopher paper</p>"},{"location":"transforms/language/doc_quality/#configuration","title":"Configuration","text":"<p>The set of dictionary keys holding DocQualityTransform  configuration for values are as follows:</p> <ul> <li>text_lang - specifies language used in the text content. By default, \"en\" is used.</li> <li>doc_content_column - specifies column name that contains document text. By default, \"contents\" is used.</li> <li>bad_word_filepath - specifies a path to bad word file: local folder (file or directory) that points to bad word file. You don't have to set this parameter if you don't need to set bad words.</li> </ul> <p>Example <pre><code>{\n    text_lang_key: \"en\",\n    doc_content_column_key: \"contents\",\n    bad_word_filepath_key: os.path.join(basedir, \"ldnoobw\", \"en\"),\n}\n</code></pre></p>"},{"location":"transforms/language/doc_quality/#usage","title":"Usage","text":""},{"location":"transforms/language/doc_quality/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available <pre><code>  --docq_text_lang DOCQ_TEXT_LANG   language used in the text content. By default, \"en\" is used.\n  --docq_doc_content_column DOCQ_DOC_CONTENT_COLUMN   column name that contain document text. By default, \"contents\" is used.\n  --docq_bad_word_filepath DOCQ_BAD_WORD_FILEPATH   path to bad word file: local folder (file or directory) that points to bad word file. You don't have to set this parameter if you don't need to set bad words.\n</code></pre> These correspond to the configuration keys described above.</p>"},{"location":"transforms/language/doc_quality/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target</p> <ul> <li><code>run-cli-sample</code> - runs dpk_doc_quality/transform.py using command line args</li> </ul> <p>This target will activate the virtual environment and sets up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/doc_quality/#code-example","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/language/doc_quality/#testing","title":"Testing","text":"<p>Following the testing strategy of data-processing-lib</p> <p>Currently we have: - Unit test - Integration test</p>"},{"location":"transforms/language/doc_quality/#further-resource","title":"Further Resource","text":"<ul> <li>For those who want to learn C4 heuristic rules</li> <li>https://arxiv.org/pdf/1910.10683.pdf</li> <li>For those who want to learn Gopher statistics</li> <li>https://arxiv.org/pdf/2112.11446.pdf</li> <li>For those who want to see the source of badwords used by default</li> <li>https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words</li> </ul>"},{"location":"transforms/language/doc_quality/#consideration","title":"Consideration","text":""},{"location":"transforms/language/doc_quality/#troubleshooting-guide","title":"Troubleshooting guide","text":"<p>For M1 Mac user, if you see following error during make command, <code>error: command '/usr/bin/clang' failed with exit code 1</code>, you should follow this step</p>"},{"location":"transforms/language/doc_quality/#document-quality-ray-transform","title":"Document Quality Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/doc_quality/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Document Quality configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/doc_quality/#running","title":"Running","text":""},{"location":"transforms/language/doc_quality/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e., TransformLauncher), In addition to those available to the transform as defined here, the set of  launcher options are available.</p>"},{"location":"transforms/language/doc_quality/#running-the-samples_1","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target</p> <ul> <li><code>run-ray-cli-sample</code> - runs dpk_doc_quality/ray/transform.py using command line args</li> </ul> <p>This target will activate the virtual environment and sets up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-ray-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/doc_quality/#code-example-ray","title":"Code example (Ray)","text":"<p>notebook</p>"},{"location":"transforms/language/doc_quality/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/extreme_tokenized/","title":"Extreme Tokenized Documents Annotator","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/extreme_tokenized/#contributors","title":"Contributors","text":"<ul> <li>Hajar Emami Gohari (Hajar.Emami@ibm.com)</li> <li>Constantin Adam (cmadam@us.ibm.com)</li> </ul>"},{"location":"transforms/language/extreme_tokenized/#summary","title":"Summary","text":"<p>This annotator retrieves the tokens generated for a set of documents. Then, it calculates, for each document, the size and the total number of characters. The number of tokens is divided by the size and by the number of characters, and the resulting values are stored in two columns ( <code>tokens_per_doc_size</code> and <code>tokens_per_doc_num_chars</code>).</p> <p>The annotator transform annotates the input table with 5 columns:   - doc_num_tokens - number of tokens for each document   - doc_size_kbs - document size in kb   - doc_num_chars - number of characters in the document   - tokens_per_doc_size - ratio between number of tokens and document size   - tokens_per_doc_num_chars - ratio between number of tokens and number of characters in document</p> <p>Documents with extremely high or low number of tokens per character (or tokens per byte) are identified as extreme-tokenized documents and can be excluded in the filtering step.</p>"},{"location":"transforms/language/extreme_tokenized/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding ExtremeTokenizedTransform configuration for values are as follows:</p> <ul> <li>et_contents_column_name - specifies the name of the column holding the document text. The default is <code>text</code>.</li> <li>et_arrow_path - location of the folder containing the arrow (tokenization) files.</li> </ul> <p>Additionally, a set of data access-specific arguments are provided that enable the specification of the location of domain list files, so that these files could be stored in the local file system or in S3 storage, for example. The arguments are as follows (and generally match the TransformLauncher's  data access arguments but with the `extreme_tokenized_' prefix).</p> <ul> <li>et_local_config - specifies the input and output folders.</li> <li>et_s3_config - specifies the input and output paths in s3.</li> <li>et_s3_credentials - provides credentials to access the s3 storage. </li> </ul> <p>See the Command Line options below for specifics on these.</p>"},{"location":"transforms/language/extreme_tokenized/#running","title":"Running","text":"<p>You can run the extreme_tokenized module to transform the <code>test1.parquet</code> file in test input data  to an <code>output</code> directory.  The output directory will contain both the new annotated <code>test1.parquet</code> file and the <code>metadata.json</code> file.</p> <pre>\n(venv) cma:extreme_tokenized$ make venv PYTHON=python3.11\n(venv) cma:extreme_tokenized$ source venv/bin/activate\n(venv) cma:extreme_tokenized$ python -m dpk_extreme_tokenized.runtime --et_arrow_path test-data/input/arrow --data_local_config \"{ 'input_folder': 'test-data/input', 'output_folder': 'output' }\"\n09:41:22 INFO - Launching Extreme Tokenized Annotator transform\n09:41:22 INFO - data factory et_ is using local configuration without input/output path\n09:41:22 INFO - data factory et_ max_files -1, n_sample -1\n09:41:22 INFO - data factory et_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n09:41:22 INFO - pipeline id pipeline_id\n09:41:22 INFO - code location None\n09:41:22 INFO - data factory data_ is using local data access: input_folder - test-data/input output_folder - output\n09:41:22 INFO - data factory data_ max_files -1, n_sample -1\n09:41:22 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n09:41:22 INFO - orchestrator et started at 2025-01-27 09:41:22\n09:41:22 INFO - Number of files is 1, source profile {'max_file_size': 0.029085159301757812, 'min_file_size': 0.029085159301757812, 'total_file_size': 0.029085159301757812}\n09:41:22 INFO - Transforming table with 10 rows from file /home/cma/de/data-prep-kit/transforms/language/extreme_tokenized/test-data/input/test1.parquet\n09:41:22 INFO - Completed 1 files (100.0%) in 0.001 min\n09:41:22 INFO - Done processing 1 files, waiting for flush() completion.\n(venv) cma:extreme_tokenized$ deactivate\n</pre>"},{"location":"transforms/language/extreme_tokenized/#building-the-docker-images","title":"Building the Docker Images","text":"<p><pre><code>(venv) cma:extreme_tokenized$ make image \n...\n(venv) cma:extreme_tokenized$ podman images\nREPOSITORY                                                  TAG       IMAGE ID       CREATED              SIZE\nextreme_tokenized-ray                                       latest    b933331aab92   41 seconds ago       2.77GB\nquay.io/dataprep1/data-prep-kit/extreme_tokenized-ray       latest    b933331aab92   41 seconds ago       2.77GB\nextreme_tokenized-python                                    latest    a5df95eba200   About a minute ago   696MB\nquay.io/dataprep1/data-prep-kit/extreme_tokenized-python    latest    a5df95eba200   About a minute ago   696MB\n````\nIn addition, there are some useful `make` targets (see conventions above)\nor use `make help` to see a list of available targets.\n\n### Launched Command Line Options \nWhen running the transform with the Ray launcher (i.e. TransformLauncher),\nthe following command line arguments are available in addition to \n[the options provided by the launcher](../../../data-processing-lib/doc/launcher-options.md).\n</code></pre> options:   -h, --help            show this help message and exit   --et_contents_column_name ET_CONTENTS_COLUMN_NAME                         Name of the column holding the document text   --et_arrow_path ET_ARROW_PATH                          Arrow folder location.   --et_s3_cred ET_S3_CRED                         AST string of options for s3 credentials. Only required for S3 data access.                         access_key: access key help text                         secret_key: secret key help text                         url: optional s3 url                         region: optional s3 region                         Example: { 'access_key': 'access', 'secret_key': 'secret',                          'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud',                          'region': 'us-east-1' }   --data_s3_cred DATA_S3_CRED                         AST string of options for s3 credentials. Only required for S3 data access.                         access_key: access key help text                         secret_key: secret key help text                         url: optional s3 url                         region: optional s3 region                         Example: { 'access_key': 'access', 'secret_key': 'secret',                          'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud',                          'region': 'us-east-1' }   --data_s3_config DATA_S3_CONFIG                         AST string containing input/output paths.                         input_folder: Path to input folder of files to be processed                         output_folder: Path to output folder of processed files                         Example: { 'input_folder': 's3-path/your-input-bucket',                          'output_folder': 's3-path/your-output-bucket' }   --data_local_config DATA_LOCAL_CONFIG                         ast string containing input/output folders using local fs.                         input_folder: Path to input folder of files to be processed                         output_folder: Path to output folder of processed files                         Example: { 'input_folder': './input', 'output_folder': '/tmp/output' }   --data_max_files DATA_MAX_FILES                         Max amount of files to process   --data_checkpointing DATA_CHECKPOINTING                         checkpointing flag   --data_files_to_checkpoint DATA_FILES_TO_CHECKPOINT                         list of file extensions to choose for checkpointing.   --data_data_sets DATA_DATA_SETS                         List of sub-directories of input directory to use for input. For example, ['dir1', 'dir2']   --data_files_to_use DATA_FILES_TO_USE                         list of file extensions to choose for input.   --data_num_samples DATA_NUM_SAMPLES                         number of random input files to process   --runtime_num_processors RUNTIME_NUM_PROCESSORS                         size of multiprocessing pool   --runtime_pipeline_id RUNTIME_PIPELINE_ID                         pipeline id   --runtime_job_id RUNTIME_JOB_ID                         job id   --runtime_code_location RUNTIME_CODE_LOCATION                         AST string containing code location                         github: Github repository URL.                         commit_hash: github commit hash                         path: Path within the repository                         Example: { 'github': 'https://github.com/somerepo', 'commit_hash': '1324',                          'path': 'transforms/universal/code' } ```</p>"},{"location":"transforms/language/gneissweb_classification/","title":"Gneissweb Classification Transform","text":"<p>The Gneissweb Classification transform serves as a simple exemplar to demonstrate the development of a simple 1:1 transform. Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/gneissweb_classification/#contributors","title":"Contributors","text":"<ul> <li>Ran Iwamoto (ran.iwamoto1@ibm.com)</li> </ul>"},{"location":"transforms/language/gneissweb_classification/#summary","title":"Summary","text":"<p>This transform will classify each text with confidence score with fasttext classification model such as: - ibm-granite/GneissWeb.Quality_annotator - ibm-granite/GneissWeb.Sci_classifier - ibm-granite/GneissWeb.Tech_classifier - ibm-granite/GneissWeb.Edu_classifier - ibm-granite/GneissWeb.Med_classifier</p>"},{"location":"transforms/language/gneissweb_classification/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding ClassificationTransform  configuration for values are as follows:</p> Configuration Parameters Default Description gcls_model_credential unset specifies the credential you use to get models. This will be huggingface token. Guide to get huggingface token gcls_model_file_name unset specifies what filename of models you use to get models, like [<code>fasttext_gneissweb_quality_annotator.bin</code>,<code>fasttext_science.bin</code>,<code>fasttext_technology_computing.bin</code>,<code>fasttext_education.bin</code>,<code>fasttext_medical.bin</code>] gcls_model_url unset specifies urls that models locate. For fasttext, this will be repo name of the models, like [<code>ibm-granite/GneissWeb.Quality_annotator</code>,<code>ibm-granite/GneissWeb.Sci_classifier</code>,<code>ibm-granite/GneissWeb.Tech_classifier</code>,<code>ibm-granite/GneissWeb.Edu_classifier</code>,<code>ibm-granite/GneissWeb.Med_classifier</code>] gcls_n_processes 1 number of processes. Must be a positive integer gcls_content_column_name <code>contents</code> specifies name of the column containing documents gcls_output_lablel_column_name [<code>label_quality</code>,<code>label_sci</code>,<code>label_tech</code>,<code>label_edu</code>,<code>label_med</code>] specifies name of the output column to hold predicted classes gcls_output_score_column_name [<code>score_quality</code>,<code>score_sci</code>,<code>score_tech</code>,<code>score_edu</code>,<code>score_med</code>] specifies name of the output column to hold score of prediction"},{"location":"transforms/language/gneissweb_classification/#running","title":"Running","text":""},{"location":"transforms/language/gneissweb_classification/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the launcher. The prefix gcls is short name for Gneissweb CLaSsification. <pre><code>  --gcls_model_credential GCLS_MODEL_CREDENTIAL   the credential you use to get models. This will be huggingface token.\n  --gcls_model_file_name GCLS_MODEL_KIND   filename of models you use to get models. Currently,like [`fasttext_gneissweb_quality_annotator.bin`,`fasttext_science.bin`,`fasttext_technology_computing.bin`,`fasttext_education.bin`,`fasttext_medical.bin`]\n  --gcls_model_url GCLS_MODEL_URL   urls that models locate. For fasttext, this will be repo name of the models, like [`ibm-granite/GneissWeb.Quality_annotator`,`ibm-granite/GneissWeb.Sci_classifier`,`ibm-granite/GneissWeb.Tech_classifier`,`ibm-granite/GneissWeb.Edu_classifier`,`ibm-granite/GneissWeb.Med_classifier`]\n  --gcls_content_column_name GCLS_CONTENT_COLUMN_NAME   A name of the column containing documents\n  --gcls_output_lable_column_name GCLS_OUTPUT_LABEL_COLUMN_NAME   Column names to store classification results, like [`label_quality`,`label_sci`,`label_tech`,`label_edu`,`label_med`]\n  --gcls_output_score_column_name GCLS_OUTPUT_SCORE_COLUMN_NAME   Column names to store the score of prediction, like [`score_quality`,`score_sci`,`score_tech`,`score_edu`,`score_med`]\n  --gcls_n_processes NUMBER_OF_PROCESSES   number of processes, an integer value. Larger value will give a better throughput in compensation for memory consumption\n</code></pre> These correspond to the configuration keys described above.</p>"},{"location":"transforms/language/gneissweb_classification/#code-example","title":"Code example","text":"<p>Here is a sample notebook</p>"},{"location":"transforms/language/gneissweb_classification/#troubleshooting-guide","title":"Troubleshooting guide","text":"<p>For M1 Mac user, if you see following error during make command, <code>error: command '/usr/bin/clang' failed with exit code 1</code>, you should follow this step</p>"},{"location":"transforms/language/gneissweb_classification/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/gneissweb_classification/#gneissweb-classification-ray-transform","title":"Gneissweb Classification Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/gneissweb_classification/#summary_1","title":"Summary","text":"<p>This project wraps the gneissweb classification transform with a Ray runtime.</p>"},{"location":"transforms/language/gneissweb_classification/#configuration-and-command-line-options_1","title":"Configuration and command line Options","text":"<p>Gneissweb Classification configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/gneissweb_classification/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined here, the set of  launcher options are available.</p>"},{"location":"transforms/language/gneissweb_classification/#code-example-ray-version","title":"Code example (Ray version)","text":"<p>Here is a sample notebook</p>"},{"location":"transforms/language/gneissweb_classification/#transforming-data-using-the-transform-image_1","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/html2parquet/","title":"HTML to Parquet Transform","text":""},{"location":"transforms/language/html2parquet/#description","title":"Description","text":"<p>This transform iterates through zipped collections of HTML files or single HTML files and generates Parquet files containing the extracted content, leveraging the Trafilatura library for extraction of text, tables, images, and other components.</p>"},{"location":"transforms/language/html2parquet/#contributors","title":"Contributors","text":"<ul> <li>Sungeun An (sungeun.an@ibm.com)</li> <li>Syed Zawad (szawad@ibm.com)</li> </ul>"},{"location":"transforms/language/html2parquet/#date","title":"Date","text":"<p>Last updated: 10/16/24 Update details: Enhanced table and image extraction features by adding the corresponding Trafilatura parameters.</p>"},{"location":"transforms/language/html2parquet/#input-and-output","title":"Input and Output","text":""},{"location":"transforms/language/html2parquet/#input","title":"Input","text":"<ul> <li>Accepted Formats: Single HTML files or zipped collections of HTML files.  </li> <li>Sample Input Files: sample html files </li> </ul>"},{"location":"transforms/language/html2parquet/#output","title":"Output","text":"<ul> <li>Format: Parquet files with the following structure:</li> </ul> <pre><code>{\n    \"title\": \"string\",             // the member filename\n    \"document\": \"string\",          // the base of the source archive\n    \"contents\": \"string\",          // the content of the HTML\n    \"document_id\": \"string\",      // the document id, a hash of `contents`\n    \"size\": \"string\",             // the size of `contents`\n    \"date_acquired\": \"date\",      // the date when the transform was executing\n}\n</code></pre>"},{"location":"transforms/language/html2parquet/#parameters","title":"Parameters","text":""},{"location":"transforms/language/html2parquet/#user-configurable-parameters","title":"User-Configurable Parameters","text":"<p>The table below provides the parameters that users can adjust to control the behavior of the extraction:</p> Parameter Default Description <code>output_format</code> <code>markdown</code> Specifies the format of the extracted content. Options: <code>markdown</code>, <code>txt</code>. <code>favor_precision</code> <code>True</code> Prefers less content but more accurate extraction. Options: <code>True</code>, <code>False</code>. <code>favor_recall</code> <code>True</code> Extracts more content when uncertain. Options: <code>True</code>, <code>False</code>."},{"location":"transforms/language/html2parquet/#default-parameters","title":"Default Parameters","text":"<p>The table below provides the parameters that are enabled by default to ensure a comprehensive extraction process:</p> Parameter Default Description <code>include_tables</code> <code>True</code> Extracts content from HTML <code>&lt;table&gt;</code> elements. <code>include_images</code> <code>True</code> Extracts image references (experimental feature). <code>include_links</code> <code>True</code> Extracts hyperlinks from the HTML content. <code>include_formatting</code> <code>True</code> Preserves basic HTML formatting (e.g., bold, italic) in the extracted content. <p>Note: If both <code>favor_precision</code> and <code>favor_recall</code> are set to <code>True</code>, <code>favor_recall</code> takes precedence.</p> <ul> <li>To set the output format to plain text, use <code>output_format='txt'</code>.</li> <li>To prioritize extracting more content over accuracy, set <code>favor_recall=True</code> and <code>favor_precision=False</code>.</li> <li>When invoking the CLI, use the following syntax for these parameters: <code>--html2parquet_&lt;parameter_name&gt;</code>. For example: <code>--html2parquet_output_format='markdown'</code>.</li> </ul>"},{"location":"transforms/language/html2parquet/#example","title":"Example","text":""},{"location":"transforms/language/html2parquet/#sample-notebook","title":"Sample Notebook","text":"<p>See the sample notebook for an example.</p>"},{"location":"transforms/language/html2parquet/#sample-html","title":"Sample HTML","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Sample HTML File&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to My Sample HTML Page&lt;/h1&gt;\n    &lt;h2&gt;Overview&lt;/h2&gt;\n    &lt;p&gt;This page contains various HTML components to demonstrate structure and formatting.&lt;/p&gt;\n    &lt;p&gt;This paragraph contains &lt;a href=\"https://example.com\"&gt;a link to Example.com&lt;/a&gt;.&lt;/p&gt;\n\n    &lt;h2&gt;Sample Image&lt;/h2&gt;\n    &lt;img src=\"https://via.placeholder.com/300\" alt=\"Placeholder Image\" /&gt;\n\n    &lt;h2&gt;Key Features&lt;/h2&gt;\n    &lt;ul&gt;\n        &lt;li&gt;Easy to use&lt;/li&gt;\n        &lt;li&gt;Highly customizable&lt;/li&gt;\n        &lt;li&gt;Supports multiple components&lt;/li&gt;\n    &lt;/ul&gt;\n\n    &lt;h2&gt;Sample Data Table&lt;/h2&gt;\n    &lt;table border=\"1\"&gt;\n        &lt;tr&gt;\n            &lt;th&gt;Name&lt;/th&gt;\n            &lt;th&gt;Age&lt;/th&gt;\n            &lt;th&gt;City&lt;/th&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Alice&lt;/td&gt;\n            &lt;td&gt;30&lt;/td&gt;\n            &lt;td&gt;New York&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Bob&lt;/td&gt;\n            &lt;td&gt;25&lt;/td&gt;\n            &lt;td&gt;Los Angeles&lt;/td&gt;\n        &lt;/tr&gt;\n        &lt;tr&gt;\n            &lt;td&gt;Charlie&lt;/td&gt;\n            &lt;td&gt;35&lt;/td&gt;\n            &lt;td&gt;Chicago&lt;/td&gt;\n        &lt;/tr&gt;\n    &lt;/table&gt;\n\n    &lt;h2&gt;Contact Us&lt;/h2&gt;\n    &lt;form action=\"/submit\" method=\"POST\"&gt;\n        &lt;label for=\"name\"&gt;Name:&lt;/label&gt;\n        &lt;input type=\"text\" id=\"name\" name=\"name\" required&gt;&lt;br&gt;&lt;br&gt;\n        &lt;label for=\"email\"&gt;Email:&lt;/label&gt;\n        &lt;input type=\"email\" id=\"email\" name=\"email\" required&gt;&lt;br&gt;&lt;br&gt;\n        &lt;input type=\"submit\" value=\"Submit\"&gt;\n    &lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"transforms/language/html2parquet/#sample-output-using-default-parameters","title":"Sample Output (Using Default Parameters)","text":"<pre><code># Welcome to My Sample HTML Page\n\n## Overview\n\nThis page contains various HTML components to demonstrate structure and formatting.\n\nThis paragraph contains [a link to Example.com](https://example.com).\n\n## Sample Image\n\n\n## Key Features\n\n- Easy to use\n- Highly customizable\n- Supports multiple components\n\n## Getting Started\n\n- Download the HTML file\n- Open it in your browser\n- Explore the content\n\n## Sample Data Table\n\nName |\nAge |\nCity |\n\nAlice |\n30 |\nNew York |\n\nBob |\n25 |\nLos Angeles |\n\nCharlie |\n35 |\nChicago |\n\n\n## Contact Us\n</code></pre>"},{"location":"transforms/language/html2parquet/#usage","title":"Usage","text":""},{"location":"transforms/language/html2parquet/#command-line-interface-cli","title":"Command-Line Interface (CLI)","text":"<p>Run the transform with the following command:</p> <pre><code>python ./dpk_html2parquet/transform_python.py \\\n  --data_local_config \"{'input_folder': './test-data/input', 'output_folder': './test-data/expected'}\" \\\n  --data_files_to_use '[\".html\", \".zip\"]'\n</code></pre> <ul> <li>When invoking the CLI, use the following syntax for these parameters: <code>--html2parquet_&lt;parameter_name&gt;</code>. For example: <code>--html2parquet_output_format='markdown'</code>.</li> </ul>"},{"location":"transforms/language/html2parquet/#further-resources","title":"Further Resources","text":"<ul> <li>Trafilatura.</li> </ul>"},{"location":"transforms/language/html2parquet/#html2parquet-ray-transform","title":"html2parquet Ray Transform","text":"<p>This module implements the ray version of the html2parquet transform.</p> <p>The HTML conversion is using the Trafilatura.</p>"},{"location":"transforms/language/html2parquet/#prometheus-metrics","title":"Prometheus metrics","text":"<p>The transform will produce the following statsd metrics:</p> metric name Description worker_html_doc_count Number of HTML documents converted by the worker worker_html_pages_count Number of HTML pages converted by the worker worker_html_page_avg_convert_time Average time for converting a single HTML page on each worker worker_html_convert_time Time spent converting a single document"},{"location":"transforms/language/lang_id/","title":"Language Identification Transform","text":"<p>The Language Identification transforms serves as a simple exemplar to demonstrate the development of a simple 1:1 transform. Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/lang_id/#summary","title":"Summary","text":"<p>This transform will identify language of each text with confidence score with fasttext language identification model. ref</p>"},{"location":"transforms/language/lang_id/#contributors","title":"Contributors","text":"<ul> <li>Daiki Tsuzuku (dtsuzuku@jp.ibm.com)</li> <li>Maroun Touma (touma@us.ibm.com)</li> </ul>"},{"location":"transforms/language/lang_id/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding LangIdentificationTransform  configuration for values are as follows:</p> Key name Default Description lang_id_model_credential unset specifies the credential you use to get model. This will be huggingface token. Guide to get huggingface token lang_id_model_kind unset specifies what kind of model you want to use for language identification. Currently, only <code>fasttext</code> is available. lang_id_model_url unset specifies url that model locates. For fasttext, this will be repo nme of the model, like <code>facebook/fasttext-language-identification</code> lang_id_content_column_name <code>contents</code> specifies name of the column containing documents lang_id_output_lang_column_name <code>lang</code> specifies name of the output column to hold predicted language code lang_id_output_score_column_name <code>score</code> specifies name of the output column to hold score of prediction"},{"location":"transforms/language/lang_id/#running","title":"Running","text":""},{"location":"transforms/language/lang_id/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the launcher. <pre><code>  --lang_id_model_credential LANG_ID_MODEL_CREDENTIAL   the credential you use to get model. This will be huggingface token.\n  --lang_id_model_kind LANG_ID_MODEL_KIND   what kind of model you want to use for language identification. Currently, only `fasttext` is available.\n  --lang_id_model_url LANG_ID_MODEL_URL   url that model locates. For fasttext, this will be repo name of the model, like `facebook/fasttext-language-identification`\n  --lang_id_content_column_name LANG_ID_CONTENT_COLUMN_NAME   A name of the column containing documents\n  --lang_id_output_lang_column_name LANG_ID_OUTPUT_LANG_COLUMN_NAME   Column name to store identified language\n  --lang_id_output_score_column_name LANG_ID_OUTPUT_SCORE_COLUMN_NAME   Column name to store the score of language identification\n</code></pre> These correspond to the configuration keys described above.</p>"},{"location":"transforms/language/lang_id/#code-example","title":"Code example","text":"<p>Here is a sample notebook</p>"},{"location":"transforms/language/lang_id/#troubleshooting-guide","title":"Troubleshooting guide","text":"<p>For M1 Mac user, if you see following error during make command, <code>error: command '/usr/bin/clang' failed with exit code 1</code>, you should follow this step</p>"},{"location":"transforms/language/lang_id/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/lang_id/#language-identification-ray-transform","title":"Language Identification Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/lang_id/#summary_1","title":"Summary","text":"<p>This project wraps the language identification transform with a Ray runtime.</p>"},{"location":"transforms/language/lang_id/#configuration-and-command-line-options_1","title":"Configuration and command line Options","text":"<p>Language Identification configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/lang_id/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined here, the set of  launcher options are available.</p>"},{"location":"transforms/language/lang_id/#transforming-data-using-the-transform-image_1","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/lang_id/#supported-languages","title":"Supported Languages","text":"Afrikaans Albanian Alemannic Amharic Arabic Aragonese Armenian Asaro'o Assamese Asturian Avaric Azerbaijani Bashkir Basque Bavarian Belarusian Bengali Bihari languages Bishnupriya Manipuri Bosnian Breton Bulgarian Buriat Burmese Catalan Cebuano Central Bikol Central Khmer Central Kurdish Chavacano Chechen Chinese Chuvash Cornish Corsican Croatian Czech Danish Dimli Divehi Doteli Dutch Eastern Mari Egyptian Arabic Emilian-Romagnol English Esperanto Estonian Fiji Hindi Finnish French Galician Georgian German Goan Konkani Greek Guarani Gujarati Haitian Creole Hebrew Hill Mari Hindi Hungarian Icelandic Ido Ilokano Indonesian Interlingua Interlingue Irish Italian Japanese Javanese Kalmyk Kannada Kapampangan Karachay-Balkar Kazakh Komi Korean Kurdish Kyrgyz Lao Latin Latvian Lezgian Limburgish Lithuanian Lojban Lombard Low German Lower Sorbian Luxembourgish Macedonian Maithili Malagasy Malay Malayalam Maltese Manx Marathi Mazandarani Min Mirandese Mongolian Nahuatl Neapolitan Nepal Bhasa Nepali Northern Frisian Northern Luri Norwegian Norwegian Nynorsk Occitan Odia Ossetian Palatine Pashto Persian Piedmontese Polish Portuguese Punjabi Quechua Romanian Romansh Russian Rusyn Sanskrit Sardinian Scots Scottish Gaelic Serbian Serbo-Croatian Sicilian Sindhi Sinhala Slovak Slovenian Somali South Azerbaijani Spanish Sundanese Swahili Swedish Tagalog Tajik Tamil Tatar Telugu Thai Tibetan Turkish Turkmen Tuvan Uighur Ukrainian Upper Sorbian Urdu Uzbek Venetian Veps Vietnamese Volap\u00fck Walloon Waray Welsh West Flemish Western Frisian Western Punjabi Wu Chinese Yakut Yiddish Yoruba Yue"},{"location":"transforms/language/lang_id/kfp_ray/","title":"language Identification Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/lang_id/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/lang_id/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions first export your HugginFact token as follows: <pre><code>export HF_READ_ACCESS_TOKEN=&lt;token&gt;\n</code></pre></p> <p>Then run: <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/pdf2parquet/","title":"Pdf2Parquet Transform","text":"<p>The Pdf2Parquet transform iterates through PDF, Docx, Pptx, Images files or zip of files and generates parquet files containing the converted document in Markdown or JSON format.</p> <p>The conversion is using the Docling package.</p> <p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/pdf2parquet/#contributors","title":"Contributors","text":"<ul> <li>Michele Dolfi (dol@zurich.ibm.com)</li> </ul>"},{"location":"transforms/language/pdf2parquet/#input-files","title":"Input files","text":"<p>This transform supports the following input formats:</p> <ul> <li>PDF documents</li> <li>DOCX documents</li> <li>PPTX presentations</li> <li>Image files (png, jpeg, etc)</li> <li>HTML pages</li> <li>Markdown documents</li> <li>ASCII Docs documents</li> </ul> <p>The input documents can be provided in a folder structure, or as a zip archive. Please see the configuration section for specifying the input files.</p>"},{"location":"transforms/language/pdf2parquet/#output-format","title":"Output format","text":"<p>The output table will contain following columns</p> output column name data type description source_filename string the basename of the source archive or file filename string the basename of the PDF file contents string the content of the PDF document_id string the document id, a random uuid4 document_hash string the document hash of the input content ext string the detected file extension hash string the hash of the <code>contents</code> column size string the size of <code>contents</code> date_acquired date the date when the transform was executing num_pages number number of pages in the PDF num_tables number number of tables in the PDF num_doc_elements number number of document elements in the PDF pdf_convert_time float time taken to convert the document in seconds"},{"location":"transforms/language/pdf2parquet/#configuration","title":"Configuration","text":"<p>The transform can be initialized with the following parameters.</p> Parameter Default Description <code>data_files_to_use</code> - The files extensions to be considered when running the transform. Example value <code>['.pdf','.docx','.pptx','.zip']</code>. For all the supported input formats, see the section above. <code>batch_size</code> -1 Number of documents to be saved in the same result table. A value of -1 will generate one result file for each input file. <code>artifacts_path</code> Path where to Docling models artifacts are located, if unset they will be downloaded and fetched from the HF_HUB_CACHE folder. <code>contents_type</code> <code>text/markdown</code> The output type for the <code>contents</code> column. Valid types are <code>text/markdown</code>, <code>text/plain</code> and <code>application/json</code>. <code>do_table_structure</code> <code>True</code> If true, detected tables will be processed with the table structure model. <code>do_ocr</code> <code>True</code> If true, optical character recognition (OCR) will be used to read the content of bitmap parts of the document. <code>ocr_engine</code> <code>easyocr</code> The OCR engine to use. Valid values are <code>easyocr</code>, <code>tesseract</code>, <code>tesseract_cli</code>. <code>bitmap_area_threshold</code> <code>0.05</code> Threshold for running OCR on bitmap figures embedded in document. The threshold is computed as the fraction of the area covered by the bitmap, compared to the whole page area. <code>pdf_backend</code> <code>dlparse_v2</code> The PDF backend to use. Valid values are <code>dlparse_v2</code>, <code>dlparse_v1</code>, <code>pypdfium2</code>. <code>double_precision</code> <code>8</code> If set, all floating points (e.g. bounding boxes) are rounded to this precision. For tests it is advised to use 0. <p>Example</p> <pre><code>{\n    \"data_files_to_use\": ast.literal_eval(\"['.pdf','.docx','.pptx','.zip']\"),\n    \"contents_type\": \"application/json\",\n    \"do_ocr\": True,\n}\n</code></pre>"},{"location":"transforms/language/pdf2parquet/#usage","title":"Usage","text":""},{"location":"transforms/language/pdf2parquet/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When invoking the CLI, the parameters must be set as <code>--pdf2parquet_&lt;name&gt;</code>, e.g., <code>--pdf2parquet_do_ocr=true</code>.</p>"},{"location":"transforms/language/pdf2parquet/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target</p> <ul> <li><code>run-cli-sample</code> - runs dpk_pdf2parquet/transform_python.py using command line args</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/pdf2parquet/#code-example","title":"Code example","text":"<p>See the sample notebook for an example</p>"},{"location":"transforms/language/pdf2parquet/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/pdf2parquet/#testing","title":"Testing","text":"<p>Following the testing strategy of data-processing-lib</p> <p>Currently we have: - Unit test - Integration test</p>"},{"location":"transforms/language/pdf2parquet/#pdf2parquet-ray-transform","title":"Pdf2parquet Ray Transform","text":"<p>This module implements the ray version of the pdf2parquet transform.</p>"},{"location":"transforms/language/pdf2parquet/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Ingest PDF to Parquet configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/pdf2parquet/#running","title":"Running","text":""},{"location":"transforms/language/pdf2parquet/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e., TransformLauncher), in addition to those available to the transform for the Python version in this file, the set of  launcher options are available.</p>"},{"location":"transforms/language/pdf2parquet/#transforming-data-using-the-transform-image_1","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/pdf2parquet/#prometheus-metrics","title":"Prometheus metrics","text":"<p>The transform will produce the following statsd metrics:</p> metric name Description worker_pdf_doc_count Number of PDF documents converted by the worker worker_pdf_pages_count Number of PDF pages converted by the worker worker_pdf_page_avg_convert_time Average time for converting a single PDF page on each worker worker_pdf_convert_time Time spent converting a single document"},{"location":"transforms/language/pdf2parquet/#credits","title":"Credits","text":"<p>The PDF document conversion is developed by the AI for Knowledge group in IBM Research Zurich. The main package is Docling.</p>"},{"location":"transforms/language/pdf2parquet/kfp_ray/","title":"PDF2PARQUET Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/pdf2parquet/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the pdf2parquet Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/pdf2parquet/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/pii_redactor/","title":"PII Redactor Transform","text":"<p>This transform redacts Personally Identifiable Information (PII) from the input data.</p> <p>The transform leverages the Microsoft Presidio SDK for PII detection and uses the Flair recognizer for entity recognition.</p>"},{"location":"transforms/language/pii_redactor/#contributors","title":"Contributors","text":"<ul> <li>Sowmya.L.R (lrsowmya@gmail.com)</li> </ul>"},{"location":"transforms/language/pii_redactor/#supported-entities","title":"Supported Entities","text":"<p>The transform detects the following PII entities by default: - PERSON: Names of individuals - EMAIL_ADDRESS: Email addresses - ORGANIZATION: Names of organizations - DATE_TIME: Dates and times - PHONE_NUMBER: Phone number - CREDIT_CARD: Credit card numbers</p> <p>You can configure the entities to detect by passing the required entities as argument param ( --pii_redactor_entities ). To know more about different entity types supported - Entities</p>"},{"location":"transforms/language/pii_redactor/#redaction-techniques","title":"Redaction Techniques","text":"<p>Two redaction techniques are supported: - replace: Replaces detected PII with a placeholder (default) - redact: Removes the detected PII from the text</p> <p>You can choose the redaction technique by passing it as an argument parameter (--pii_redactor_operator).</p>"},{"location":"transforms/language/pii_redactor/#input-and-output","title":"Input and Output","text":""},{"location":"transforms/language/pii_redactor/#input","title":"Input","text":"<p>The input data should be a <code>py.Table</code> with a column containing the text where PII detection and redaction will be applied. By default, this column is named <code>contents</code>.</p> <p>Example Input Table Structure: Table 1: Sample input to the pii redactor transform</p> contents doc_id My name is John Doe doc001 I work at apple doc002"},{"location":"transforms/language/pii_redactor/#output","title":"Output","text":"<p>The output table will include the original columns plus an additional column <code>new_contents</code> which is configurable with redacted text and <code>detected_pii</code>  column consisting the type of PII entities detected in that document for replace operator.</p> <p>Example Output Table Structure for replace operator:</p> contents doc_id new_contents detected_pii My name is John Doe doc001 My name is <code>&lt;PERSON&gt;</code> <code>[PERSON]</code> I work at apple doc002 I work at <code>&lt;ORGANIZATION&gt;</code> <code>[ORGANIZATION]</code> <p>When <code>redact</code> operator is chosen the output will look like below</p> <p>Example Output Table Structure for redact operator</p> contents doc_id new_contents detected_pii My name is John Doe doc001 My name is <code>[PERSON]</code> I work at apple doc002 I work at <code>[ORGANIZATION]</code>"},{"location":"transforms/language/pii_redactor/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the launcher.</p> <pre><code>  --pii_redactor_entities PII_ENTITIES\n                        list of PII entities to be captured for example: [\"PERSON\", \"EMAIL\"]\n  --pii_redactor_operator REDACTOR_OPERATOR\n                        Two redaction techniques are supported - replace(default), redact \n  --pii_redactor_transformed_contents PII_TRANSFORMED_CONTENT_COLUMN_NAME\n                        Mention the column name in which transformed contents will be added. This is required argument. \n  --pii_redactor_score_threshold SCORE_THRESHOLD\n                        The score_threshold is a parameter that sets the minimum confidence score required for an entity to be considered a match.\n                        Provide a value above 0.6\n</code></pre>"},{"location":"transforms/language/pii_redactor/#pii-redactor-ray-transform","title":"PII Redactor Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/pii_redactor/#summary","title":"Summary","text":"<p>This project wraps the pii redactor transform with a Ray runtime.</p>"},{"location":"transforms/language/pii_redactor/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined here, the set of  launcher options are available.</p>"},{"location":"transforms/language/pii_redactor/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/pii_redactor/kfp_ray/","title":"PII Redactor Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/pii_redactor/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the pii redactor ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/pii_redactor/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/language/readability/","title":"Readability Scores Annotator","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/readability/#contributors","title":"Contributors","text":"<ul> <li>Hajar Emami Gohari (Hajar.Emami@ibm.com)</li> <li>Constantin Adam (cmadam@us.ibm.com)</li> </ul>"},{"location":"transforms/language/readability/#summary","title":"Summary","text":"<p>This transform annotates documents of a parquet file with various Readability Scores (originally defined in the textstat github page]) that can later be used in Quality Filtering. The transform annotates the following readability scores:</p> <ul> <li><code>flesch_ease_textstat</code> - while the maximum score is 121.22, there is no limit on how low the score can be. A negative score is valid. the values for this score are explained in the table below:</li> </ul> Score School level (US) Notes 100.00\u201390.00 5th grade Very easy to read. Easily understood by an average 11-year-old student. 90.0\u201380.0 6th grade Easy to read. Conversational English for consumers. 80.0\u201370.0 7th grade Fairly easy to read. 70.0\u201360.0 8th &amp; 9th grade Plain English. Easily understood by 13- to 15-year-old students. 60.0\u201350.0 10th to 12th grade Fairly difficult to read. 50.0\u201330.0 College Difficult to read. 30.0\u201310.0 College graduate Very difficult to read. Best understood by university graduates. 10.0\u20130.0 Professional Extremely difficult to read. Best understood by university graduates. <ul> <li><code>flesch_kincaid_textstat</code> - this is a grade formula in that a score of 9.3 means that a ninth grader    would be able to read the document.</li> <li><code>gunning_fog_textstat</code> - this is a grade formula in that a score of 9.3 means that a ninth grader    would be able to read the document.</li> <li><code>smog_index_textstat</code> -  the SMOG index of the given text. This is a grade formula in that a score    of 9.3 means that a ninth grader would be able to read the document. Texts of fewer than 30     sentences are statistically invalid, because the SMOG formula was normed on 30-sentence samples.     Textstat requires at least 3 sentences for a result. </li> <li><code>coleman_liau_index_textstat</code> - the grade level of the text using the Coleman-Liau Formula. This is     a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document. </li> <li><code>automated_readability_index_textstat</code> - the ARI (Automated Readability Index) which outputs a    number that approximates the grade level needed to comprehend the text. For example if the ARI is    6.5, then the grade level to comprehend the text is 6th to 7th grade.</li> <li><code>dale_chall_readability_score_textstat</code> - different from other tests, since it uses a lookup table    of the most commonly used 3000 English words. Thus it returns the grade level using the New    Dale-Chall Formula. Further reading on https://en.wikipedia.org/wiki/Dale\u2013Chall_readability_formula    The table below further explains the values assigned by this score:</li> </ul> Score Understood by 4.9 or lower average 4th-grade student or lower 5.0\u20135.9 average 5th or 6th-grade student 6.0\u20136.9 average 7th or 8th-grade student 7.0\u20137.9 average 9th or 10th-grade student 8.0\u20138.9 average 11th or 12th-grade student 9.0\u20139.9 average 13th to 15th-grade (college) student <ul> <li><code>linsear_write_formula_textstat</code> - the grade level using the Linsear Write Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document. Further reading on Wikipedia https://en.wikipedia.org/wiki/Linsear_Write</li> <li><code>text_standard_textstat</code> - based upon all the above tests, returns the estimated school grade level required to understand the text. Optional float_output allows the score to be returned as a float. Defaults to False.</li> <li><code>spache_readability_textstat</code> - the grade level of english text. Intended for text written for children up to grade four.</li> <li><code>mcalpine_eflaw_textstat</code> - a score for the readability of an english text for a foreign learner or English, focusing on the number of miniwords and length of sentences. </li> <li><code>reading_time_textstat</code> - the reading time of the given text. Assumes 14.69ms per character. </li> <li><code>avg_grade_level</code> - average of grade-level scores <code>flesch_kincaid</code>, <code>gunning_fog</code>, and  <code>automated_readability_index</code></li> </ul>"},{"location":"transforms/language/readability/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding ReadabilityTransform configuration for values are as follows:</p> <ul> <li>readability_contents_column_name - specifies the name of the column holding the document text. The default is <code>text</code>.</li> <li>readability_score_list - list of readability scores to be computed by the transform;   valid values: <code>coleman_liau_index_textstat</code>, <code>flesch_kincaid_textstat</code>,   <code>difficult_words_textstat</code>, <code>spache_readability_textstat</code>, <code>smog_index_textstat</code>,   <code>reading_time_textstat</code>, <code>dale_chall_readability_score_textstat</code>, <code>text_standard_textstat</code>,   <code>automated_readability_index_textstat</code>, <code>gunning_fog_textstat</code>, <code>flesch_ease_textstat</code>,   <code>mcalpine_eflaw_textstat</code>, <code>linsear_write_formula_textstat</code>.</li> </ul> <p>Additionally, a set of data access-specific arguments are provided that enable the specification of the location of domain list files, so that these files could be stored in the local file system or in S3 storage, for example. The arguments are as follows (and generally match the TransformLauncher's  data access arguments but with the `readability_' prefix).</p> <ul> <li>readability_local_config - specifies the input and output folders.</li> <li>readability_s3_config - specifies the input and output paths in s3.</li> <li>readability_s3_credentials - provides credentials to access the s3 storage.</li> </ul> <p>See the Command Line options below for specifics on these.</p>"},{"location":"transforms/language/readability/#running","title":"Running","text":"<p>You can run the readability module to transform the <code>readability-test.parquet</code> file in test input data  to an <code>output</code> directory.  The output directory will contain both the new annotated <code>readability-test.parquet</code> file and the <code>metadata.json</code> file.</p> <pre>\ncma:readability$ make venv PYTHON=python3.11\ncma:readability$ source venv/bin/activate\n(venv) cma:readability$ python -m dpk_readability.runtime --data_local_config \"{ 'input_folder': 'test-data/input', 'output_folder': 'output' }\" --readability_score_list \"['reading_time_textstat','spache_readability_textstat','text_standard_textstat']\"\n13:07:23 INFO - Launching Readability transform\n13:07:23 INFO - Readability parameters are : {'readability_contents_column_name': 'contents', 'readability_score_list': ['reading_time_textstat', 'spache_readability_textstat', 'text_standard_textstat']}\n13:07:23 INFO - pipeline id pipeline_id\n13:07:23 INFO - code location None\n13:07:23 INFO - data factory data_ is using local data access: input_folder - test-data/input output_folder - output\n13:07:23 INFO - data factory data_ max_files -1, n_sample -1\n13:07:23 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n13:07:23 INFO - orchestrator readability started at 2025-02-07 13:07:23\n13:07:23 INFO - Number of files is 1, source profile {'max_file_size': 0.014194488525390625, 'min_file_size': 0.014194488525390625, 'total_file_size': 0.014194488525390625}\n13:07:24 INFO - Completed 1 files (100.0%) in 0.002 min\n13:07:24 INFO - Done processing 1 files, waiting for flush() completion.\n13:07:24 INFO - done flushing in 0.0 sec\n13:07:24 INFO - Completed execution in 0.002 min, execution result 0\n(venv) cma:readability$ deactivate\n</pre>"},{"location":"transforms/language/readability/#building-the-docker-images","title":"Building the Docker Images","text":"<p><pre><code>(venv) cma:readability$ make image \n...\n(venv) cma:readability$ podman images\nREPOSITORY                                                  TAG       IMAGE ID       CREATED              SIZE\nreadability-ray                                             latest    eb753e168d29   49 seconds ago       2.66GB\nquay.io/dataprep1/data-prep-kit/readability-ray             latest    eb753e168d29   49 seconds ago       2.66GB\nreadability-python                                          latest    7f15c1d5d63d   About a minute ago   651MB\nquay.io/dataprep1/data-prep-kit/readability-python          latest    7f15c1d5d63d   About a minute ago   651MB\n````\nIn addition, there are some useful `make` targets (see conventions above)\nor use `make help` to see a list of available targets.\n\n### Launched Command Line Options \nWhen running the transform with the Ray launcher (i.e. TransformLauncher),\nthe following command line arguments are available in addition to \n[the options provided by the launcher](../../../data-processing-lib/doc/launcher-options.md).\n</code></pre> options:   -h, --help            show this help message and exit   --readability_contents_column_name READABILITY_CONTENTS_COLUMN_NAME                         contents column name for input parquet table to transform   --readability_score_list READABILITY_SCORE_LIST                         list of readability scores to be computed by the transform; valid values: {'flesch_ease_textstat', 'reading_time_textstat', 'flesch_kincaid_textstat', 'automated_readability_index_textstat', 'linsear_write_formula_textstat', 'text_standard_textstat', 'smog_index_textstat', 'difficult_words_textstat', 'spache_readability_textstat', 'dale_chall_readability_score_textstat', 'mcalpine_eflaw_textstat', 'gunning_fog_textstat', 'coleman_liau_index_textstat'}   --data_s3_cred DATA_S3_CRED                         AST string of options for s3 credentials. Only required for S3 data access.                         access_key: access key help text                         secret_key: secret key help text                         url: optional s3 url                         region: optional s3 region                         Example: { 'access_key': 'access', 'secret_key': 'secret',                          'url': 'https://s3.us-east.cloud-object-storage.appdomain.cloud',                          'region': 'us-east-1' }   --data_s3_config DATA_S3_CONFIG                         AST string containing input/output paths.                         input_folder: Path to input folder of files to be processed                         output_folder: Path to output folder of processed files                         Example: { 'input_folder': 's3-path/your-input-bucket',                          'output_folder': 's3-path/your-output-bucket' }   --data_local_config DATA_LOCAL_CONFIG                         ast string containing input/output folders using local fs.                         input_folder: Path to input folder of files to be processed                         output_folder: Path to output folder of processed files                         Example: { 'input_folder': './input', 'output_folder': '/tmp/output' }   --data_max_files DATA_MAX_FILES                         Max amount of files to process   --data_checkpointing DATA_CHECKPOINTING                         checkpointing flag   --data_files_to_checkpoint DATA_FILES_TO_CHECKPOINT                         list of file extensions to choose for checkpointing.   --data_data_sets DATA_DATA_SETS                         List of sub-directories of input directory to use for input. For example, ['dir1', 'dir2']   --data_files_to_use DATA_FILES_TO_USE                         list of file extensions to choose for input.   --data_num_samples DATA_NUM_SAMPLES                         number of random input files to process   --runtime_num_processors RUNTIME_NUM_PROCESSORS                         size of multiprocessing pool   --runtime_pipeline_id RUNTIME_PIPELINE_ID                         pipeline id   --runtime_job_id RUNTIME_JOB_ID                         job id   --runtime_code_location RUNTIME_CODE_LOCATION                         AST string containing code location                         github: Github repository URL.                         commit_hash: github commit hash                         path: Path within the repository                         Example: { 'github': 'https://github.com/somerepo', 'commit_hash': '1324',                          'path': 'transforms/universal/code' }</p>"},{"location":"transforms/language/similarity/","title":"Similarity Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration,  testing and IDE set up.</p>"},{"location":"transforms/language/similarity/#contributors","title":"Contributors","text":"<ul> <li>Chad DeLuca (delucac@us.ibm.com)</li> <li>Anna Lisa Gentile (annalisa.gentile@ibm.com)</li> </ul>"},{"location":"transforms/language/similarity/#summary","title":"Summary","text":"<p>The similarity transforms annotates each input document with potential matches found in a document collection. The annotation consists of a json object proving the id of the matched document in the collection and  the specific sentenced deemed as \"similar\" by the tranform. The Similarity Transform relies on a running ElasticSearch Index. We assume (and provide) a functioning endpoint, but you can spin up your own service (details in ElasticSearch Configuration).</p>"},{"location":"transforms/language/similarity/#is-this-transform-for-me","title":"Is this transform for me?","text":"<p>In it current implementation, this tranform helps identifying if any input text is nearly-verbatim reproducing content in a target collection. The main purpouse is \"Text Attribution\", i.e. vetting text against proprietary content - to identify data leakages or potential copyright violations.</p> <p>This is particularly useful in the contest of synthetically generated text. One of the many concerns about using LLMs is the inadvertent incorporation of proprietary content that may violate copyright laws.  Our assumption is that we are in possession of the data we want to check against - this is the data collection that we want to protect, and we look for verbatim or semi-verbatim reuse of any of its content. To perform search and similarity against our reference data, the data needs to be indexed (how to index your own data).</p>"},{"location":"transforms/language/similarity/#i-am-curious-how-is-it-implemented-optional-read","title":"I am curious: how is it implemented? [optional read]","text":"<p>We then take the input text to be vetted, and generate multimple shingle queries to efficiently find matches against the ElasticSearch index.</p> <p>For example, take the following text to check for similarity: </p> <p><code>Now is the winter of our discontent</code></p> <p>We can set the shingle size to 3 with a skip value (how far the shingle window slides) of 1 to get the following shingles: <pre><code>now is the\nis the winter\nthe winter of\nwinter of our\nof our discontent\n</code></pre></p> <p>For this implementation we have a level of flexibility for matching text in cases where a near-exact match is suitable. In Elasticsearch, this primarily manifests as a value for \u201cslop\u201d, which allows for words to appear in a different order as well as slight variations. The allowed variations increase in correlation with the slop value. Calculating this value at runtime allows for longer shingles to still find matches, even without a 100% exact match.</p>"},{"location":"transforms/language/similarity/#configuration","title":"Configuration","text":""},{"location":"transforms/language/similarity/#input-files","title":"Input files","text":"<p>This transform supports the input of parquet files that contain a single column, called \"contents\", where each row is a a string that will be searched for in a target document collection. Your contents column may for example contain a collection of texts generated by a LLM, or a collection of student essays, i.e. any text of which you want to verify originality against your target corpus.</p> <p>The target corpus (your elasticserach index) is specified with configuration parameters. You can index your document collection using a procedure we provide. To start, you can use our provided ElasticSearch instance, which contains some pre-indexed news articles.</p>"},{"location":"transforms/language/similarity/#output-format","title":"Output format","text":"<p>The output table will contain a single additional column:</p> output column name data type description contents string the original input text similarity_score json the annotations that describe in which document a potential match was found and which sentence in the document was the closest match <p> Note: similarity_score will be soon changed into similarity_annotiation. Within the annotation, the field score will be changed to rank. The current score numbe shouldn't be taken as an absolute value, rather a rank for the returned results. Higher ranking results are more closely similar to the input text. </p> <p>Example of single cell contents in the output column: <pre><code>I bet the company staffs want an increase in the wages\n</code></pre></p> <p>Example of single cell content in the similarity_score column:</p> <pre><code>  {\n      'contents': array(['I bet the company staffs want to have an increase in the wages.'], dtype=object), \n      'id': '123456789', \n      'index': 'myPrivateDocumentsIndex', \n      'score': 29.345\n  }\n</code></pre>"},{"location":"transforms/language/similarity/#initialization","title":"Initialization","text":"<p>The transform can be initialized with the following parameters.</p> Parameter Default Description <code>similarity_es_endpoint</code> - The URL for Elasticsearch <code>similarity_es_userid</code> - Elasticsearch user ID <code>similarity_es_pwd</code> - Elasticsearch password <code>similarity_es_index</code> - The Elasticsearch index to query <code>similarity_shingle_size</code> 8 Shingle size for query construction (default is 8) <code>similarity_result_size</code> 1 result size for matched sentences (default is 1) <code>similarity_annotation_column</code> similarity_score The column name that will contain the similarity annotations, in json format <p>Example</p> <pre><code>{\n      \"similarity_es_pwd\" :\"my password\",\n      \"similarity_es_userid\":\"myElasticsearchID\",\n      \"similarity_es_endpoint\":\"https://thisIsWhere.MyElasticIsRunning.com\",\n      \"similarity_es_index\" :\"myPrivateDocumentsIndex\"\n}\n</code></pre>"},{"location":"transforms/language/similarity/#running","title":"Running","text":""},{"location":"transforms/language/similarity/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the launcher. <pre><code>  --similarity_es_endpoint SIMILARITY_ES_ENDPOINT\n                        The URL for Elasticsearch\n  --similarity_es_userid SIMILARITY_ES_USERID\n                        Elasticsearch user ID\n  --similarity_es_pwd SIMILARITY_ES_PWD\n                        Elasticsearch password\n  --similarity_es_index SIMILARITY_ES_INDEX\n                        The Elasticsearch index to query\n  --similarity_shingle_size SIMILARITY_SHINGLE_SIZE\n                        Shingle size for query construction (default is 8)\n  --similarity_result_size SIMILARITY_RESULT_SIZE\n                        result size for matched sentences (default is 1)\n  --similarity_annotation_column SIMILARITY_ANNOTATION_COLUMN\n                        The column name that will contain the similarity score\n  --similarity_doc_text_column SIMILARITY_DOC_TEXT_COLUMN\n                        The column name that contains the document text\n</code></pre> These correspond to the configuration keys described above.</p>"},{"location":"transforms/language/similarity/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>When invoking the CLI, the parameters must be set as <code>--similarity_&lt;name&gt;</code>, e.g. <code>--similarity_es_pwd=pass</code>.</p>"},{"location":"transforms/language/similarity/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> targets</p> <ul> <li><code>run-cli-sample</code> - runs src/similarity_transform_python.py using command line args</li> <li><code>run-local-sample</code> - runs src/similarity_local.py</li> <li><code>run-local-python-sample</code> - runs src/similarity_local_python.py</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-local-python-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/language/similarity/#code-example","title":"Code example","text":"<p>See the sample notebook for an example</p>"},{"location":"transforms/language/similarity/tools/load_elasticsearch/","title":"Setup and Load Data into Elasticsearch","text":"<p>This script reads a CSV file containing documents, generates embeddings for a specified \"contents\" field using a Sentence Transformers model, and indexes the documents into an Elasticsearch index.</p>"},{"location":"transforms/language/similarity/tools/load_elasticsearch/#features","title":"Features","text":"<ul> <li>.env Configuration: Optionally reads Elasticsearch host, credentials, index name, and CSV path from a <code>.env</code> file.</li> <li>Index Management: Can optionally create a new index using a default mapping file if <code>CREATE_INDEX</code> is set to <code>True</code>. If <code>CREATE_INDEX</code> is <code>False</code>, the script verifies that the index exists.</li> <li>CSV Ingestion: Reads documents from a CSV file and verifies the existence of a <code>contents</code> column. If the column is not found, the script exits.</li> <li>Embeddings Generation: Uses a <code>SentenceTransformer</code> model (<code>paraphrase-MiniLM-L6-v2</code>) to generate 384-dimensional embeddings for each document\u2019s contents.</li> </ul>"},{"location":"transforms/language/similarity/tools/load_elasticsearch/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li><code>requests</code></li> <li><code>python-dotenv</code></li> <li><code>sentence-transformers</code></li> <li>A running instance of Elasticsearch (e.g., <code>Elasticsearch 7.x+</code> or <code>Elasticsearch 8.x+</code>), accessible at the specified <code>ELASTIC_HOST</code>.</li> </ul>"},{"location":"transforms/language/similarity/tools/load_elasticsearch/#setup","title":"Setup","text":"<ol> <li>Install Dependencies    Install Python dependencies using:    ```bash    pip install -r requirements.txt</li> </ol>"},{"location":"transforms/language/similarity/tools/load_elasticsearch/#running-the-script","title":"Running the script","text":"<p>```bash    python set_up_elasticsearch.py</p>"},{"location":"transforms/language/text_encoder/","title":"Text Encoder Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/text_encoder/#contributors","title":"Contributors","text":"<ul> <li>Michele Dolfi (dol@zurich.ibm.com)</li> </ul>"},{"location":"transforms/language/text_encoder/#description","title":"Description","text":"<p>This transform is using sentence encoder models to create embedding vectors of the text in each row of the input .parquet table.</p> <p>The embeddings vectors generated by the transform are useful for tasks like sentence similarity, features extraction, etc which are also at the core of retrieval-augmented generation (RAG) applications.</p>"},{"location":"transforms/language/text_encoder/#input","title":"Input","text":"input column name data type description the one specified in content_column_name configuration string the content used in this transform"},{"location":"transforms/language/text_encoder/#output-columns","title":"Output columns","text":"output column name data type description the one specified in output_embeddings_column_name configuration <code>array[float]</code> the embeddings vectors of the content"},{"location":"transforms/language/text_encoder/#configuration","title":"Configuration","text":"<p>The transform can be tuned with the following parameters.</p> Parameter Default Description <code>model_name</code> <code>BAAI/bge-small-en-v1.5</code> The HF model to use for encoding the text. <code>content_column_name</code> <code>contents</code> Name of the column containing the text to be encoded. <code>output_embeddings_column_name</code> <code>embeddings</code> Column name to store the embeddings in the output table."},{"location":"transforms/language/text_encoder/#usage","title":"Usage","text":""},{"location":"transforms/language/text_encoder/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When invoking the CLI, the parameters must be set as <code>--text_encoder_&lt;name&gt;</code>, e.g. <code>--text_encoder_column_name_key=myoutput</code>.</p>"},{"location":"transforms/language/text_encoder/#code-example","title":"Code example","text":"<p>Here is a sample notebook</p>"},{"location":"transforms/language/text_encoder/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/text_encoder/#testing","title":"Testing","text":"<p>Following the testing strategy of data-processing-lib</p> <p>Currently we have: - Unit test</p>"},{"location":"transforms/language/text_encoder/#textencoder-ray-transform","title":"TextEncoder Ray Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/language/text_encoder/#summary","title":"Summary","text":"<p>This project wraps the text_encoder transform with a Ray runtime.</p>"},{"location":"transforms/language/text_encoder/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Text Encoder configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/language/text_encoder/#code-example_1","title":"Code example","text":"<p>Here is a sample notebook</p>"},{"location":"transforms/language/text_encoder/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined here, launcher options are available.</p>"},{"location":"transforms/language/text_encoder/#transforming-data-using-the-transform-image_1","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/language/text_encoder/kfp_ray/","title":"TextEncoder Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/language/text_encoder/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the text_encoder Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/language/text_encoder/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/bloom/","title":"Bloom Annotation","text":""},{"location":"transforms/universal/bloom/#summary","title":"Summary","text":"<p>Recently, IBM has introduced GneissWeb; a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. The models trained using GneissWeb dataset outperform those trained on FineWeb by 2.14 percentage points in terms of average score computed on a set of 11 commonly used benchmarks.</p> <p>The Bloom Annotator transform assigns a label of 1 if the document is present in the GneissWeb Bloom filter; otherwise, it assigns 0. This approach provides a clear understanding of which documents in FineWeb are also present in GneissWeb and which are not. The GneissWeb Bloom filter is just one use case; the Bloom Annotator transform can work with any Bloom filter. </p> <p>Bloom annotator transform maps a non-empty input table to an output table with an added is_in_GneissWeb column. Each row in the table corresponds to a UUID and its associated document. The Bloom annotator transform verifies whether the document's UUID exists in the GneissWeb Bloom filter.</p> <p>In order to be able to reproduce GneissWeb, we provide here a Bloom filter representing all the document ids of FineWeb 1.1.0 whose documents are part of GneissWeb. It is of size 28GB and is of the rbloom family of Bloom filters. It is to be probed with the id column of FineWeb 1.1.0 or of Common Crawl. Due to the large size of the model, a machine with larger than 16GB of memory is needed to run the transform on the local machine. </p>"},{"location":"transforms/universal/bloom/#contributor","title":"Contributor","text":"<ul> <li>Yang Zhao (yangzhao@ibm.com)</li> </ul>"},{"location":"transforms/universal/bloom/#general-information","title":"General Information","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/bloom/#prerequisite","title":"Prerequisite","text":"<p>Please refer to <code>requirements.txt</code> to install the necessary packages.</p>"},{"location":"transforms/universal/bloom/#input-format","title":"input format","text":"<p>The input is in .parquet format and contains the following columns:</p> id contents &lt;urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7&gt; Viewing Single Post From: Spoilers for the We... &lt;urn:uuid:ba819eb7-e6e6-415a-87f4-0347b6a4f017&gt; sigh Fundamentalist community, let me pass o..."},{"location":"transforms/universal/bloom/#output-format","title":"output format","text":"<p>The output is in .parquet format and includes an additional column, in addition to those in the input:</p> id contents is_in_GneissWeb &lt;urn:uuid:39147604-bfbe-4ed5-b19c-54105f8ae8a7&gt; Viewing Single Post From: Spoilers for the We... 0 &lt;urn:uuid:ba819eb7-e6e6-415a-87f4-0347b6a4f017&gt; sigh Fundamentalist community, let me pass o... 1"},{"location":"transforms/universal/bloom/#configuration","title":"Configuration","text":"<p>The set of dictionary keys holding BLOOMTransformConfiguration  configuration for values are as follows:</p> <ul> <li>--model_name_or_path - specify the GneissWeb Bloom filter model, which should be sourced from HuggingFace. </li> <li>--batch_size - modify it based on the infrastructure capacity. Defaults to <code>1000</code>.</li> <li>--doc_text_column - the column name containing the document text in the input .parquet file. Defaults to <code>contents</code>.</li> <li>--annotation_column - the column name containing binary score in the output .parquet file. Defaults to <code>is_in_GneissWeb</code>.</li> </ul>"},{"location":"transforms/universal/bloom/#usage","title":"Usage","text":"<p>Place your input Parquet file in the <code>test-data/input/</code> directory. A sample file, <code>test1.parquet</code>, is available in this directory. Once done, run the script.</p> <pre><code>python local_python.py\n</code></pre> <p>You will obtain the output file <code>test1.parquet</code> in the output directory.</p>"},{"location":"transforms/universal/bloom/#code-example","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/universal/bloom/#testing","title":"Testing","text":"<p>Currently we have: - bloom test</p>"},{"location":"transforms/universal/doc_id/","title":"Document ID Python Annotator","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/doc_id/#contributors","title":"Contributors","text":"<ul> <li>Boris Lublinsky (blublinsk@ibm.com)</li> </ul>"},{"location":"transforms/universal/doc_id/#description","title":"Description","text":"<p>This transform assigns unique identifiers to the documents in a dataset and supports the following annotations to the original data: * Adding a Document Hash to each document. The unique hash-based ID is generated using <code>hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()</code>. To store this hash in the data specify the desired column name using the <code>hash_column</code> parameter. * Adding an Integer Document ID: to each document. The integer ID is unique across all rows and tables processed by the <code>transform()</code> method. To store this ID in the data, specify the desired column name using the <code>int_id_column</code> parameter.</p> <p>Document IDs are essential for tracking annotations linked to specific documents. They are also required for processes like fuzzy deduplication, which depend on the presence of integer IDs. If your dataset lacks document ID columns, this transform can be used to generate them.</p>"},{"location":"transforms/universal/doc_id/#input-columns-used-by-this-transform","title":"Input Columns Used by This Transform","text":"Input Column Name Data Type Description Column specified by the contents_column configuration argument str Column that stores document text"},{"location":"transforms/universal/doc_id/#output-columns-annotated-by-this-transform","title":"Output Columns Annotated by This Transform","text":"Output Column Name Data Type Description hash_column str Unique hash assigned to each document int_id_column uint64 Unique integer ID assigned to each document"},{"location":"transforms/universal/doc_id/#configuration-and-command-line-options","title":"Configuration and Command Line Options","text":"<p>The set of dictionary keys defined in DocIDTransform configuration for values are as follows:</p> <ul> <li>doc_column - specifies name of the column containing the document (required for ID generation)</li> <li>hash_column - specifies name of the column created to hold the string document id, if None, id is not generated</li> <li>int_id_column - specifies name of the column created to hold the integer document id, if None, id is not generated</li> <li>start_id - an id from which ID generator starts () </li> </ul> <p>At least one of hash_column or int_id_column must be specified.</p>"},{"location":"transforms/universal/doc_id/#usage","title":"Usage","text":""},{"location":"transforms/universal/doc_id/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e. TransformLauncher), the following command line arguments are available in addition to  the options provided by the launcher. <pre><code>  --doc_id_doc_column DOC_ID_DOC_COLUMN\n                        doc column name\n  --doc_id_hash_column DOC_ID_HASH_COLUMN\n                        Compute document hash and place in the given named column\n  --doc_id_int_column DOC_ID_INT_COLUMN\n                        Compute unique integer id and place in the given named column\n  --doc_id_start_id DOC_ID_START_ID\n                        starting integer id\n</code></pre> These correspond to the configuration keys described above.</p>"},{"location":"transforms/universal/doc_id/#code-example","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/universal/doc_id/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/doc_id/#testing","title":"Testing","text":"<p>Following the testing strategy of data-processing-lib</p> <p>Currently we have: - Unit test - Integration test</p>"},{"location":"transforms/universal/doc_id/#document-id-ray-annotator","title":"Document ID Ray Annotator","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/doc_id/#ray-summary","title":"Ray Summary","text":"<p>This project wraps the Document ID transform with a Ray runtime.</p>"},{"location":"transforms/universal/doc_id/#configuration-and-command-line-options_1","title":"Configuration and command line Options","text":"<p>Document ID configuration and command line options are the same as for the base python transform.</p>"},{"location":"transforms/universal/doc_id/#building","title":"Building","text":"<p>A docker file that can be used for building docker the ray image. You can use</p> <pre><code>make build \n</code></pre>"},{"location":"transforms/universal/doc_id/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e., RayTransformLauncher), in addition to Python  command line options,  there are options provided by the launcher.</p> <p>To use the transform image to transform your data, please refer to the running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/doc_id/#code-example_1","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/universal/doc_id/#document-id-spark-annotator","title":"Document ID Spark Annotator","text":""},{"location":"transforms/universal/doc_id/#summary","title":"Summary","text":"<p>This transform assigns a unique integer ID to each row in a Spark DataFrame. It relies on the monotonically_increasing_id pyspark function to generate the unique integer IDs. As described in the documentation of this function:</p> <p>The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive. </p>"},{"location":"transforms/universal/doc_id/#configuration-and-command-line-options_2","title":"Configuration and command line Options","text":"<p>Document ID configuration and command line options are the same as for the base python transform.</p>"},{"location":"transforms/universal/doc_id/#running","title":"Running","text":"<p>You can run the doc_id_local.py (spark-based implementation) to transform the <code>test1.parquet</code> file in test input data to an <code>output</code> directory.  The directory will contain both the new annotated <code>test1.parquet</code> file and the <code>metadata.json</code> file.</p>"},{"location":"transforms/universal/doc_id/#launched-command-line-options_2","title":"Launched Command Line Options","text":"<p>When running the transform with the Spark launcher (i.e., SparkTransformLauncher), the following command line arguments are available in addition to the options provided by the launcher.</p> <pre><code>  --doc_id_column_name DOC_ID_COLUMN_NAME\n                        name of the column that holds the generated document ids\n</code></pre>"},{"location":"transforms/universal/doc_id/#running-as-spark-based-application","title":"Running as spark-based application","text":"<pre><code>(venv) cma:src$ python doc_id_local.py\n18:32:13 INFO - data factory data_ is using local data access: input_folder - /home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/test-data/input output_folder - /home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/output at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:185\"\n18:32:13 INFO - data factory data_ max_files -1, n_sample -1 at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:201\"\n18:32:13 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'] at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:214\"\n18:32:13 INFO - pipeline id pipeline_id at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/runtime/execution_configuration.py:80\"\n18:32:13 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'} at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/runtime/execution_configuration.py:83\"\n18:32:13 INFO - spark execution config : {'spark_local_config_filepath': '/home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/config/spark_profile_local.yml', 'spark_kube_config_filepath': 'config/spark_profile_kube.yml'} at \"/home/cma/de/data-prep-kit/data-processing-lib/spark/src/data_processing_spark/runtime/spark/spark_execution_config.py:42\"\n24/05/26 18:32:14 WARN Utils: Your hostname, li-7aed0a4c-2d51-11b2-a85c-dfad31db696b.ibm.com resolves to a loopback address: 127.0.0.1; using 192.168.1.223 instead (on interface wlp0s20f3)\n24/05/26 18:32:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/05/26 18:32:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18:32:17 INFO - files = ['/home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/test-data/input/test_doc_id_1.parquet', '/home/cma/de/data-prep-kit/transforms/universal/doc_id/spark/test-data/input/test_doc_id_2.parquet'] at \"/home/cma/de/data-prep-kit/data-processing-lib/spark/src/data_processing_spark/runtime/spark/spark_launcher.py:184\"\n24/05/26 18:32:23 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n</code></pre>"},{"location":"transforms/universal/doc_id/#doc-id-statistics","title":"Doc ID Statistics","text":"<p>The metadata generated by the Spark <code>doc_id</code> transform contains the following statistics:   * <code>total_docs_count</code>, <code>total_columns_count</code>: total number of documents (rows), and columns in the input table, before the <code>doc_id</code> transform ran     * <code>docs_after_doc_id</code>, <code>columns_after_doc_id</code>: total number of documents (rows), and columns in the output table, after the <code>doc_id</code> transform ran  </p>"},{"location":"transforms/universal/doc_id/#transforming-data-using-the-transform-image_1","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/doc_id/kfp_ray/","title":"Document Identification Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/doc_id/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/doc_id/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/ededup/","title":"Exact Deduplication Transform","text":"<p>Exact deduplication transform identifies and removes identical documents in a dataset by comparing them hash-for-hash to ensure exact matching. Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/ededup/#contributors","title":"Contributors","text":"<ul> <li>Boris Lublinsky (blublinsk@ibm.com)</li> </ul>"},{"location":"transforms/universal/ededup/#description","title":"Description","text":"<p>This Python implementation of the exact deduplication transform uses \"streaming\" deduplication based on a central hash. As shown below, it relies on a distributed hash cache and data processors that read documents, generate hashes, coordinate with the cache to remove duplicates, and store unique documents in the data plane.</p> <p></p> <p>Mapping this model to the transform model is complicated by the need for a hash cache, which the transform model does not recognize. The solution is to have the transform runtime create the hash cache and pass it as a parameter to the transforms. The transform runtime handles hash cache creation and enhances statistics with details about cache size and utilization.</p>"},{"location":"transforms/universal/ededup/#incremental-execution-and-snapshotting","title":"Incremental Execution and Snapshotting","text":"<p>The current implementation includes snapshotting, where the hash cache is saved to storage (local disk or S3) at the end of execution. This enables incremental deduplication: you can run deduplication on existing files, save the hash cache, and later load the snapshot to deduplicate only new files, avoiding reprocessing the entire dataset.</p>"},{"location":"transforms/universal/ededup/#input-columns-used-by-this-transform","title":"Input Columns Used by This Transform","text":"Input Column Name Data Type Description Column specified by the contents_column configuration argument str Column that stores document text Column specified by the document_id_column configuration argument int64 Column that stores document ID"},{"location":"transforms/universal/ededup/#output-columns-annotated-by-this-transform","title":"Output Columns Annotated by This Transform","text":"<p>This transform does not perform any annotations; it only filters out the documents that are marked as duplicates.</p>"},{"location":"transforms/universal/ededup/#configuration","title":"Configuration","text":"<p>The set of dictionary keys holding EdedupTransform configuration for values (common for Python and Ray) are as follows:</p> <ul> <li>doc_column - specifies name of the column containing documents</li> <li>doc_id_column - specifies the name of the column containing a document id</li> <li>use_snapshot - specifies that ededup execution starts with a set of pre-existing hashes, enabling incremental execution</li> <li>snapshot_directory - specifies the directory for reading snapshots. If not provided, the default is <code>output_folder/snapshot</code></li> </ul>"},{"location":"transforms/universal/ededup/#usage","title":"Usage","text":"<p>The following command line arguments (corresponding to the configuration keys described above) are available in addition to the options provided by the launcher. <pre><code>  --ededup_doc_column EDEDUP_DOC_COLUMN\n                        name of the column containing document\n  --ededup_doc_id_column EDEDUP_DOC_ID_COLUMN\n                        name of the column containing document id\n  --ededup_use_snapshot EDEDUP_USE_SNAPSHOT\n                        flag to continue from snapshot\n  --ededup_snapshot_directory EDEDUP_SNAPSHOT_DIRECTORY\n                        location of snapshot files  \n</code></pre></p>"},{"location":"transforms/universal/ededup/#code-example","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/universal/ededup/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/ededup/#testing","title":"Testing","text":"<p>Following the testing strategy of data-processing-lib</p> <p>Currently we have: - Unit test - Integration test</p>"},{"location":"transforms/universal/ededup/#exact-dedup-ray-annotator","title":"Exact Dedup Ray Annotator","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/ededup/#additional-parameters","title":"Additional parameters","text":"<p>In addition to common ededup parameters, Ray implementation provides two additional ones</p> <ul> <li>hash_cpu - specifies amount of CPU per hash actor</li> <li>num_hashes - specifies number of hash actors</li> </ul>"},{"location":"transforms/universal/ededup/#additional-support","title":"Additional support","text":"<p>We also provide an estimate to roughly determine cluster size for running transformer.</p>"},{"location":"transforms/universal/ededup/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target</p> <ul> <li><code>run-cli-sample</code> - runs dpk_ededup/ray/transform.py using command line args</li> </ul> <p>This target will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/ededup/#code-example_1","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/universal/ededup/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e., RayTransformLauncher), these additional command line arguments are available  the options provided by the launcher.</p> <p><code>--ededup_hash_cpu EDEDUP_HASH_CPU                         number of CPUs per hash   --ededup_num_hashes EDEDUP_NUM_HASHES                         number of hash actors to use   --ededup_doc_column EDEDUP_DOC_COLUMN                         name of the column containing document   --ededup_doc_id_column EDEDUP_DOC_ID_COLUMN                         name of the column containing document id   --ededup_use_snapshot EDEDUP_USE_SNAPSHOT                         flag to continue from snapshot   --ededup_snapshot_directory EDEDUP_SNAPSHOT_DIRECTORY                         location of snapshot files</code></p> <p>These correspond to the configuration keys described above.</p>"},{"location":"transforms/universal/ededup/kfp_ray/","title":"Exact Deduplication Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/ededup/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/ededup/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/fdedup/","title":"Fuzzy Dedup","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/fdedup/#contributors","title":"Contributors","text":"<ul> <li>Nelson Bore (kibnelson@gmail.com)</li> <li>Constantin Adam (cmadam@us.ibm.com)</li> </ul>"},{"location":"transforms/universal/fdedup/#description","title":"Description","text":"<p>The fdedup transform eliminates documents that are highly similar to each other (but not necessarily identical) from a set of Parquet files. This ensures that the resulting dataset contains only unique or sufficiently distinct entries.</p> <p>Fuzzy dedup is a complex process made up of a pipeline that performs four main steps:</p> <ol> <li>Signature Calculation: creates a set of minhashes for each document, and uses them to create band signatures for the document.</li> <li>Cluster Analysis: groups documents into clusters based on matching band signatures. Within each cluster, it retains only the documents that have a Jaccard similarity above a specified threshold, and it identifies which documents to keep as unique and which ones to mark as duplicates.</li> <li>Duplicate List Generation: combines the similarity clusters identified in each band to create a single, unified list of duplicate documents.</li> <li>Data Cleaning: processes the documents by either filtering out duplicates or adding annotations to distinguish duplicates from non-duplicates.</li> </ol> <p>Each one of these steps is described in more detail below.</p>"},{"location":"transforms/universal/fdedup/#signature-calculation","title":"Signature Calculation","text":"<p>This transform computes <code>num_permutations</code> minhashes and <code>num_bands</code> signatures for each document in the dataset, by following these processing steps: 1. Shingle Generation: create a set of character or word shingles, using a specified window length. Character shingles are more effective at detecting similar documents, but require more computational resources compared to word shingles. 2. Minhash Calculation: using the shingles as input, compute <code>num_permutations</code> minhashes for each document. 3. Band Signature Calculation: divide the minhashes into <code>num_bands</code>, where each band contains <code>num_minhashes_per_band</code> minhashes. For each document, generate a unique signature for every band.</p> <p>The values for <code>num_bands</code> and <code>num_minhashes_per_band</code> determine the likelihood that documents with a certain Jaccard similarity will be marked as duplicates. A Jupyter notebook in the utils folder generates a graph of this probability function, helping users explore how different settings for <code>num_bands</code> and <code>num_minhashes_per_band</code> impact the deduplication process.</p> <p>To help distribute the workload and speed up processing of the next steps, the hash space of each band is divided into <code>num_segments</code> segments. The band signatures, the minhashes, the document ids, and lengths are stored in an organized output folder structure <code>bands/band=b/segment=s</code>, where <code>b</code> is the band number and <code>s</code> is the segment number.</p>"},{"location":"transforms/universal/fdedup/#cluster-analysis","title":"Cluster Analysis","text":"<p>This transform leverages segmented processing to analyze the data generated by the Signature Calculation step efficiently and in parallel. Each worker processes a specific segment <code>s</code> of a band <code>b</code> by loading and analyzing all Parquet files from the folder <code>bands/band=b/segment=s</code>. Each row in the Parquet files contains, for a document: * <code>band_hash</code>, the document's band signature, and  * <code>data</code>, a structure with three fields: the unique <code>document_id</code>, document's <code>minhashes</code>, and <code>document_size</code>.</p> <p>The transform runs the following processing steps: 1. Data Loading: combine into a single dataframe all Parquet files in <code>bands/band=b/segment=s</code>. 2. Clustering: run a <code>group_by</code> operation on the <code>band_hash</code> column that will group documents with the same band signature into clusters. 3. Similarity Analysis: for each cluster, calculate Jaccard similarity between pairs of documents using their minhashes, and move documents below the specified Jaccard similarity threshold into new clusters. 4. Duplicate Identification: in clusters with more than one document remaining, retain the largest document with the smallest document id, and mark as duplicates all other documents in the cluster. 5. Persist Results: save the duplicate clusters in a file.</p>"},{"location":"transforms/universal/fdedup/#duplicate-list-generation","title":"Duplicate List Generation","text":"<p>The Cluster Analysis step identifies duplicates across multiple bands, meaning a document can be marked as a duplicate in one or more bands (e.g., if two documents are identical, one will be marked as a duplicate in all bands). This transform consolidates all duplicate information from each band segment into a single file, providing a unified record of duplicates detected across the dataset.</p>"},{"location":"transforms/universal/fdedup/#data-cleaning","title":"Data Cleaning","text":"<p>This transform processes the original dataset using the list of duplicate documents generated by the Duplicate List Generation step. It imports each file in the original dataset into a table and produces a new dataset. The directory structure of the input dataset is preserved, but the contents of the output files depend on the selected operating mode: 1. Annotate - add a new <code>duplicate</code> column to the dataset, that contains a <code>d</code> for documents marked as duplicates, and is empty for non-duplicates 2. Filter duplicates - removes all documents identified as duplicates from the dataset. 3. Filter non-duplicates - removes from the dataset all documents that were not marked as duplicates, leaving only the duplicates.</p> <p>The output dataset reflects the selected mode, providing flexibility for downstream processing.</p>"},{"location":"transforms/universal/fdedup/#input-columns-used-by-this-transform","title":"Input Columns Used by This Transform","text":"Input Column Name Data Type Description Column specified by the contents_column configuration argument str Column that stores document text Column specified by the document_id_column configuration argument int64 Column that stores document ID"},{"location":"transforms/universal/fdedup/#output-columns-annotated-by-this-transform","title":"Output Columns Annotated by This Transform","text":"Output Column Name Data Type Description duplicate str Column added if fuzzy dedup runs in 'annotate' mode. Value is 'd' for duplicate documents, empty for non-duplicates"},{"location":"transforms/universal/fdedup/#configuration-and-usage","title":"Configuration and Usage","text":""},{"location":"transforms/universal/fdedup/#fuzzy-deduplication-transform","title":"Fuzzy Deduplication Transform","text":"<p>The set of dictionary keys holding Fuzzy Dedup configuration for values are as follows: <pre><code>--input_folder INPUT_FOLDER\n                    Input folder path\n--output_folder OUTPUT_FOLDER\n                    Output folder path\n--operation_mode {filter_duplicates,filter_non_duplicates,annotate}\n                    operation mode for data cleanup: filter out duplicates/non-duplicates, or annotate duplicate documents\n--contents_column CONTENTS_COLUMN\n                    name of the column that stores document text\n--document_id_column DOCUMENT_ID_COLUMN\n                    name of the column that stores document ID\n--seed SEED         seed of the random number generator\n--num_permutations NUM_PERMUTATIONS\n                    number of permutations to use for minhash calculation\n--num_bands NUM_BANDS\n                    number of bands to use for band hash calculation\n--num_minhashes_per_band NUM_MINHASHES_PER_BAND\n                    number of minhashes to use in each band\n--word_shingle_size WORD_SHINGLE_SIZE\n                    number of words included in one shingle\n--jaccard_similarity_threshold JACCARD_SIMILARITY_THRESHOLD\n                    jaccard similarity threshold above which two documents are similar\n--num_segments NUM_SEGMENTS\n                    the number of segments dividing the hashing space for each band (for scalability)\n--duplicate_list_location DUPLICATE_LIST_LOCATION\n                    path to the file with all the duplicate document ids\n--services SERVICES   Comma-separated list of services to run (e.g., SignatureCalculation,ClusterAnalysis,GetDuplicateList,DataCleaning)\n--use_s3 USE_S3       use s3\n--s3_cred S3_CRED     ast string of options for s3 credentials\n--shingle_option SHINGLE_OPTION\n                    Option used for shingling\n</code></pre></p>"},{"location":"transforms/universal/fdedup/#signature-calculation-transform","title":"Signature Calculation Transform","text":"<p>The set of dictionary keys holding SignatureCalcTransform configuration for values are as follows: <pre><code>--minhash_document_id_column MINHASH_DOCUMENT_ID_COLUMN\n                    name of the column storing the unique ID assigned to each document\n--minhash_contents_column MINHASH_CONTENTS_COLUMN\n                    name of the column storing the contents of each document\n--minhash_seed MINHASH_SEED\n                    the seed used to instantiate the random number generator\n--minhash_num_permutations MINHASH_NUM_PERMUTATIONS\n                    number of permutations (minhashes) calculated for each document\n--minhash_word_shingle_size MINHASH_WORD_SHINGLE_SIZE\n                    the size of the word shingles calculated for each document\n--minhash_num_bands MINHASH_NUM_BANDS\n                    the number of bands to use in the banding technique\n--minhash_num_minhashes_per_band MINHASH_NUM_MINHASHES_PER_BAND\n                    the number of minhashes to use in each band\n--minhash_num_segments MINHASH_NUM_SEGMENTS\n                    the number of segments across which we divide the hashing space for each band\n--minhash_shingle_option MINHASH_SHINGLE_OPTION\n                    Shingling option ('word' or 'char')\n</code></pre></p>"},{"location":"transforms/universal/fdedup/#cluster-analysis-transform","title":"Cluster Analysis Transform","text":"<p>The set of dictionary keys holding ClusterAnalysisTransform configuration for values are as follows: <pre><code>--cluster_jaccard_similarity_threshold CLUSTER_JACCARD_SIMILARITY_THRESHOLD\n                      Jaccard similarity threshold above which two documents are duplicates\n--cluster_num_bands CLUSTER_NUM_BANDS\n                      The number of bands used in the banding technique\n--cluster_num_segments CLUSTER_NUM_SEGMENTS\n                      The number of segments dividing the hashing space for each band\n</code></pre></p>"},{"location":"transforms/universal/fdedup/#get-duplicates-list-transform","title":"Get Duplicates List Transform","text":"<p>This transform currently has no configuration parameters.</p>"},{"location":"transforms/universal/fdedup/#data-cleaning-transform","title":"Data Cleaning Transform","text":"<p>The set of dictionary keys holding DataCleaningTransform configuration for values are as follows: <pre><code>  --fdclean_document_id_column FDCLEAN_DOCUMENT_ID_COLUMN\n                        name of the column storing the unique ID assigned to each document\n  --fdclean_operation_mode {filter_duplicates,filter_non_duplicates,annotate}\n                        operation mode: filter out duplicates/non-duplicates, or annotate duplicate documents\n</code></pre></p>"},{"location":"transforms/universal/fdedup/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target to create a virtual environment:</p> <p><pre><code>make venv\n</code></pre> Subsequently, the main orchestration program can run with: <pre><code>source venv/bin/activate\npython -m dpk_fdedup.transform_python\n</code></pre> Alternatively the transforms included in fuzzy dedup can be launched independently: <pre><code>source venv/bin/activate\npython -m dpk_fdedup.signature_calc.local_python\npython -m dpk_fdedup.cluster_analysis.local_python\npython -m dpk_fdedup.get_duplicate_list.transform_local_python\npython -m dpk_fdedup.data_cleaning.local_python\n</code></pre> After running the transforms, execute: <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/fdedup/#code-example","title":"Code example","text":"<p>This is a sample notebook that shows how to invoke the python fuzzy dedup transform.</p>"},{"location":"transforms/universal/fdedup/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/fdedup/#testing","title":"Testing","text":"<p>For testing fuzzy deduplication in a pure python runtime, use the following <code>make</code> targets. To launch integration tests for all the component transforms of fuzzy dedup (signature calculation, cluster analysis, get duplicate list and data cleaning) use:  <pre><code>make test-src\n</code></pre></p> <p>To test the creation of the Docker image for fuzzy dedup transform and the capability to run a local program inside that image, use: <pre><code>make test-image\n</code></pre></p>"},{"location":"transforms/universal/fdedup/#fuzzy-dedup-ray-implementation","title":"Fuzzy Dedup - Ray implementation","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration,  testing and IDE set up.</p>"},{"location":"transforms/universal/fdedup/#summary","title":"Summary","text":"<p>This project wraps the Fuzzy Dedup transform with a Ray runtime.</p>"},{"location":"transforms/universal/fdedup/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Fuzzy Dedup configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/fdedup/#running","title":"Running","text":""},{"location":"transforms/universal/fdedup/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e., TransformLauncher), in addition to those available to the transform as defined in here, the set of  launcher options are available.</p>"},{"location":"transforms/universal/fdedup/#running-the-samples_1","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target to create a virtual environment:</p> <p><pre><code>make venv\n</code></pre> Subsequently, the main orchestration program can run with: <pre><code>source venv/bin/activate\npython -m dpk_fdedup.ray.transform\n</code></pre> Alternatively the transforms included in fuzzy dedup can be launched independently: <pre><code>source venv/bin/activate\npython -m dpk_fdedup.signature_calc.ray.local\npython -m dpk_fdedup.cluster_analysis.ray.local\npython -m dpk_fdedup.get_duplicate_list.ray.tarnsform\npython -m dpk_fdedup.data_cleaning.ray.local\n</code></pre> After running the transforms, execute: <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/fdedup/#transforming-data-using-the-transform-image_1","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/fdedup/#code-example_1","title":"Code Example","text":"<p>This is a sample notebook that shows how to invoke the ray fuzzy dedup transform.</p>"},{"location":"transforms/universal/fdedup/#testing_1","title":"Testing","text":"<p>For testing fuzzy deduplication in a ray runtime, use the following <code>make</code> targets. To launch integration tests for all the component transforms of fuzzy dedup (signature calculation, cluster analysis, get duplicate list and data cleaning) use:  <pre><code>make test-src\n</code></pre></p> <p>To test the creation of the Docker image for fuzzy dedup transform and the capability to run a local program inside that image, use: <pre><code>make test-image\n</code></pre></p>"},{"location":"transforms/universal/fdedup/#fuzzy-dedup-spark","title":"Fuzzy Dedup -- Spark","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration,  testing and IDE set up.</p>"},{"location":"transforms/universal/fdedup/#summary_1","title":"Summary","text":"<p>This project wraps the Fuzzy Dedup transform with a Spark runtime.</p>"},{"location":"transforms/universal/fdedup/#configuration-and-command-line-options_1","title":"Configuration and command line Options","text":"<p>Fuzzy Dedup configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/fdedup/#running_1","title":"Running","text":""},{"location":"transforms/universal/fdedup/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>When running the transform with the Spark launcher (i.e., TransformLauncher), in addition to those available to the transform as defined in here, the set of  spark launcher options are available.</p>"},{"location":"transforms/universal/fdedup/#running-the-samples_2","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target to create a virtual environment:</p> <p><pre><code>make venv\n</code></pre> Subsequently, the main orchestration program can run with: <pre><code>source venv/bin/activate\npython -m dpk_fdedup.spark.transform\n</code></pre> Alternatively the transforms included in fuzzy dedup can be launched independently: <pre><code>source venv/bin/activate\npython -m dpk_fdedup.signature_calc.spark.local\npython -m dpk_fdedup.cluster_analysis.spark.local\npython -m dpk_fdedup.get_duplicate_list.transform\npython -m dpk_fdedup.data_cleaning.spark.local\n</code></pre> After running the transforms, execute: <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/fdedup/#transforming-data-using-the-transform-image_2","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/fdedup/#code-example_2","title":"Code Example","text":"<p>This is a sample notebook that shows how to invoke the spark fuzzy dedup transform.</p>"},{"location":"transforms/universal/fdedup/#testing_2","title":"Testing","text":"<p>For testing fuzzy deduplication in a spark runtime, use the following <code>make</code> targets. To launch integration tests for all the component transforms of fuzzy dedup (signature calculation, cluster analysis, get duplicate list and data cleaning) use:  <pre><code>make test-src\n</code></pre></p> <p>To test the creation of the Docker image for fuzzy dedup transform and the capability to run a local program inside that image, use: <pre><code>make test-image\n</code></pre></p>"},{"location":"transforms/universal/fdedup/#further-resources","title":"Further Resources","text":"<p>The following is a list of references to research articles and github repositories that inspired the module's design:</p> <ol> <li>Jure Leskovec, Anand Rajaraman, Jeff Ullman, Mining of Massive Datasets, Chapter 3: Finding Similar Items </li> <li>G Penedo et al., The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale</li> <li>Datatrove github repo</li> </ol>"},{"location":"transforms/universal/fdedup/kfp_ray/","title":"Fuzzy Deduplication Ray-based KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/fdedup/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the fuzzy dedup Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/fdedup/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p>"},{"location":"transforms/universal/fdedup/kfp_ray/#considerations","title":"Considerations","text":"<p>Currently, fuzzy dedup KFP pipeline definitions can be compiled and executed on KFPv1. KFPv2 is not supported currently, because of this issue: https://github.com/kubeflow/pipelines/issues/10914</p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/fdedup/spark/","title":"Fuzzy Dedup -- Spark","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration,  testing and IDE set up.</p>"},{"location":"transforms/universal/fdedup/spark/#summary","title":"Summary","text":"<p>This project wraps the Fuzzy Dedup transform with a Spark runtime.</p>"},{"location":"transforms/universal/fdedup/spark/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Fuzzy Dedup configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/fdedup/spark/#running","title":"Running","text":""},{"location":"transforms/universal/fdedup/spark/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Spark launcher (i.e. TransformLauncher), In addition to those available to the transform as defined in here, the set of  spark launcher are available.</p>"},{"location":"transforms/universal/fdedup/spark/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target to create a virtual environment:</p> <p><pre><code>make venv\n</code></pre> Subsequently, the main orchestration program can run with: <pre><code>source venv/bin/activate\ncd src\npython fdedup_transform_spark.py\n</code></pre> Alternatively the transforms included in fuzzy dedup can be launched independently: <pre><code>source venv/bin/activate\ncd src\npython signature_calc_local_spark.py\npython cluster_analysis_local_spark.py\npython get_duplicate_list_local_spark.py\npython data_cleaning_local_spark.py\n</code></pre> After running the transforms, execute: <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/fdedup/spark/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/fdedup/spark/#code-example","title":"Code Example","text":"<p>This is a sample notebook that shows how to invoke the spark fuzzy dedup transform.</p>"},{"location":"transforms/universal/fdedup/spark/#testing","title":"Testing","text":"<p>For testing fuzzy deduplication in a spark runtime, use the following <code>make</code> targets. To launch integration tests for all the component transforms of fuzzy dedup (signature calculation, cluster analysis, get duplicate list and data cleaning) use:  <pre><code>make test-src\n</code></pre></p> <p>To test the creation of the Docker image for fuzzy dedup transform and the capability to run a local program inside that image, use: <pre><code>make test-image\n</code></pre></p>"},{"location":"transforms/universal/filter/","title":"Filtering Transform for Python Runtime","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/filter/#summary","title":"Summary","text":"<p>Filtering cleans up data by:  * Removing the rows that do not meet a specific set of criteria.  * Dropping the columns that are no longer needed (e.g., annotation columns, used for filtering rows).</p>"},{"location":"transforms/universal/filter/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding FilterTransform  configuration for values are as follows:</p> <ul> <li>filter_criteria_list - specifies the list of row filter criteria (in SQL WHERE clause format). Each filter criterion is a string. The default value of this parameter is <code>[]</code> (an empty list, meaning that all the rows in the input table will be kept). </li> <li>filter_logical_operator - specifies the logical operator that joins filter criteria (<code>AND</code> or <code>OR</code>). The default value of this parameter is <code>AND</code>.</li> <li>filter_columns_to_drop - the list with the names of the columns to drop after row filtering is complete. The default value of this parameter is <code>[]</code> (an empty list, meaning that all the columns in the input table will be kept)</li> </ul>"},{"location":"transforms/universal/filter/#example","title":"Example","text":"<p>Consider a table with eight text documents, where each row has additional info about that document (date acquired, source URL, etc.), and a set of quality signals for that document.  </p> <pre><code>|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 document | title    | contents | date_acq | extra    | cluster  | ft_lang | ft_score | docq_tot | docq_mea | docq_pe \u2502\n\u2502 ---      | ---      | ---      | uired    | ---      | ---      | ---     | ---      | al_words | n_word_l | rplex_s \u2502\n\u2502 str      | str      | str      | ---      | struct[5 | i64      | str     | f64      | ---      | en       | core    \u2502\n\u2502          |          |          | datetime | ]        |          |         |          | i64      | ---      | ---     \u2502\n\u2502          |          |          | [ns]     |          |          |         |          |          | f64      | f64     \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 CC-MAIN- | https:// | BACKGROU | 2023-07- | {\"applic | -1       | en      | 1.0      | 77       | 5.662338 | 226.5   \u2502\n\u2502 20190221 | www.sema | ND       | 05       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 132217-2 | nticscho | The      | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01902211 | lar.org/ | Rhinitis |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Control  |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502          |          | \u2026        |          |          |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Travel + | 2023-07- | {\"applic | -1       | en      | 1.0      | 321      | 5.05919  | 245.0   \u2502\n\u2502 20200528 | www.torn | Leisure: | 27       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 232803-2 | osnews.g | The 5    | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 02005290 | r/en/tou | best     |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | res\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Stourbri | 2023-07- | {\"applic | -1       | en      | 1.0      | 646      | 5.27709  | 230.3   \u2502\n\u2502 20190617 | www.stou | dge      | 04       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 103006-2 | rbridgen | College  | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01906171 | ews.co.u | to close |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | BMe\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Our      | 2023-07- | {\"applic | -1       | en      | 1.0      | 242      | 5.557851 | 407.2   \u2502\n\u2502 20180318 | www.univ | Guidance | 19       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 184945-2 | ariety.c | Philosop | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01803182 | om/app/s | hy       |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | High     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502          |          | sch\u2026     |          |          |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://h | Hukun    | 2023-07- | {\"applic | -1       | en      | 1.0      | 169      | 4.840237 | 240.5   \u2502\n\u2502 20180120 | urur.com | Hurur    | 18       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 083038-2 | /hukun-h | running  | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01801201 | urur-run | for Ward |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | 1 c\u2026     |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | https:// | Life's   | 2023-07- | {\"applic | -1       | en      | 1.0      | 61       | 4.786885 | 244.0   \u2502\n\u2502 20180522 | www.chap | Reverie  | 18       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 131652-2 | ters.ind | Kobo     | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01805221 | igo.ca/e | ebook |  |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Sept\u2026    |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://w | Kamis,   | 2023-07- | {\"applic | 18008253 | en      | 1.0      | 509      | 4.738703 | 224.6   \u2502\n\u2502 20181120 | ww.onedo | 10 Maret | 05       | ation/ht | 113      |         |          |          |          |         \u2502\n\u2502 130743-2 | llarfoll | 2016     | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01811201 | owers.co | Buy      |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Twitter\u2026 |          | \u2026        |          |         |          |          |          |         \u2502\n\u2502 CC-MAIN- | http://w | Rory     | 2023-07- | {\"applic | -1       | en      | 1.0      | 223      | 4.829596 | 167.5   \u2502\n\u2502 20171213 | ww.iron- | Fallon   | 09       | ation/ht |          |         |          |          |          |         \u2502\n\u2502 104259-2 | bru.co.u | joins    | 05:00:00 | tp; msgt |          |         |          |          |          |         \u2502\n\u2502 01712131 | k/rory-f | Bristol  |          | ype=resp |          |         |          |          |          |         \u2502\n\u2502 \u2026        | \u2026        | Rovers\u2026  |          | \u2026        |          |         |          |          |          |         \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n</code></pre>"},{"location":"transforms/universal/filter/#example-1-two-numerical-filtering-criteria-joined-by-and","title":"Example 1 - two numerical filtering criteria joined by AND","text":"<p>To filter this table and only keep the documents that have between 100 and 500 words and a perplexity score less than 230, and furthermore, drop the <code>extra</code> and <code>cluster</code> columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"docq_total_words &gt; 100 AND docq_total_words &lt; 500\", \"docq_perplex_score &lt; 230\"]\nfilter_logical_operator = \"AND\"\nfilter_columns_to_drop = [\"extra\", \"cluster\"]\n</code></pre> This filter operation applied on the table above will return the following result: <pre><code>|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 document    | title       | contents    | date_acquir | ft_lang | ft_score | docq_total_ | docq_mean_w | docq_perple \u2502\n\u2502 ---         | ---         | ---         | ed          | ---     | ---      | words       | ord_len     | x_score     \u2502\n\u2502 str         | str         | str         | ---         | str     | f64      | ---         | ---         | ---         \u2502\n\u2502             |             |             | datetime[ns |         |          | i64         | f64         | f64         \u2502\n\u2502             |             |             | ]           |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 CC-MAIN-201 | http://www. | Rory Fallon | 2023-07-09  | en      | 1.0      | 223         | 4.829596    | 167.5       \u2502\n\u2502 71213104259 | iron-bru.co | joins       | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201712131\u2026 | .uk/rory-f\u2026 | Bristol     |             |         |          |             |             |             \u2502\n\u2502             |             | Rovers\u2026     |             |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n</code></pre></p>"},{"location":"transforms/universal/filter/#example-2-two-numerical-filtering-criteria-joined-by-or","title":"Example 2 - two numerical filtering criteria joined by OR","text":"<p>To filter this table and only keep the documents that have between 100 and 500 words or a perplexity score less than 230, and furthermore, drop the <code>extra</code> and <code>cluster</code> columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"docq_total_words &gt; 100 AND docq_total_words &lt; 500\", \"docq_perplex_score &lt; 230\"]\nfilter_logical_operator = \"OR\"\nfilter_columns_to_drop = [\"extra\", \"cluster\"]\n</code></pre> This filter operation applied on the table above will return the following result: <pre><code>|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 document    | title       | contents    | date_acquir | ft_lang | ft_score | docq_total_ | docq_mean_w | docq_perple |\n\u2502 ---         | ---         | ---         | ed          | ---     | ---      | words       | ord_len     | x_score     \u2502\n\u2502 str         | str         | str         | ---         | str     | f64      | ---         | ---         | ---         \u2502\n\u2502             |             |             | datetime[ns |         |          | i64         | f64         | f64         \u2502\n\u2502             |             |             | ]           |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n\u2502 CC-MAIN-201 | https://www | BACKGROUND  | 2023-07-05  | en      | 1.0      | 77          | 5.662338    | 226.5       \u2502\n\u2502 90221132217 | .semanticsc | The         | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201902211\u2026 | holar.org/\u2026 | Rhinitis    |             |         |          |             |             |             \u2502\n\u2502             |             | Control \u2026   |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://www. | Kamis, 10   | 2023-07-05  | en      | 1.0      | 509         | 4.738703    | 224.6       \u2502\n\u2502 81120130743 | onedollarfo | Maret 2016  | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201811201\u2026 | llowers.co\u2026 | Buy         |             |         |          |             |             |             \u2502\n\u2502             |             | Twitter\u2026    |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://www. | Rory Fallon | 2023-07-09  | en      | 1.0      | 223         | 4.829596    | 167.5       \u2502\n\u2502 71213104259 | iron-bru.co | joins       | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201712131\u2026 | .uk/rory-f\u2026 | Bristol     |             |         |          |             |             |             \u2502\n\u2502             |             | Rovers\u2026     |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-202 | https://www | Travel +    | 2023-07-27  | en      | 1.0      | 321         | 5.05919     | 245.0       \u2502\n\u2502 00528232803 | .tornosnews | Leisure:    | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -202005290\u2026 | .gr/en/tou\u2026 | The 5 best  |             |         |          |             |             |             \u2502\n\u2502             |             | res\u2026        |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | https://www | Our         | 2023-07-19  | en      | 1.0      | 242         | 5.557851    | 407.2       \u2502\n\u2502 80318184945 | .univariety | Guidance    | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201803182\u2026 | .com/app/s\u2026 | Philosophy  |             |         |          |             |             |             \u2502\n\u2502             |             | High sch\u2026   |             |         |          |             |             |             \u2502\n\u2502 CC-MAIN-201 | http://huru | Hukun Hurur | 2023-07-18  | en      | 1.0      | 169         | 4.840237    | 240.5       \u2502\n\u2502 80120083038 | r.com/hukun | running for | 05:00:00    |         |          |             |             |             \u2502\n\u2502 -201801201\u2026 | -hurur-run\u2026 | Ward 1 c\u2026   |             |         |          |             |             |             \u2502\n|-------------|-------------|-------------|-------------|---------|----------|-------------|-------------|-------------|\n</code></pre></p>"},{"location":"transforms/universal/filter/#example-3-two-filtering-criteria-based-on-non-numerical-datetime-and-string-types","title":"Example 3 - two filtering criteria based on non-numerical (datetime and string) types","text":"<p>To filter this table and only keep the documents that were acquired between 2023-07-04 and 2023-07-08 and were downloaded using the <code>HTTPS</code> protocol, without dropping any columns, invoke filtering with the following parameters: <pre><code>filter_criteria_list = [\"date_acquired BETWEEN '2023-07-04' AND '2023-07-08'\", \"title LIKE 'https://%'\"]\nfilter_logical_operator = \"AND\"\nfilter_columns_to_drop = []\n</code></pre></p> <p>This filter operation applied on the table above will return the following result: <pre><code>|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 document | title    | contents | date_acq | extra    | cluster  | ft_lang | ft_score | docq_tot | docq_mea | docq_pe \u2502\n\u2502 ---      | ---      | ---      | uired    | ---      | ---      | ---     | ---      | al_words | n_word_l | rplex_s \u2502\n\u2502 str      | str      | str      | ---      | struct[5 | i64      | str     | f64      | ---      | en       | core    \u2502\n\u2502          |          |          | datetime | ]        |          |         |          | i64      | ---      | ---     \u2502\n\u2502          |          |          | [ns]     |          |          |         |          |          | f64      | f64     \u2502\n|----------|----------|----------|----------|----------|----------|---------|----------|----------|----------|---------|\n\u2502 CC-MAIN- | https:// | BACKGROU | 2023-07- | {\"applic | -1      | en      | 1.0      | 77       | 5.662338 | 226.5    \u2502\n\u2502 20190221 | www.sema | ND       | 05       | ation/ht |         |         |          |          |          |          \u2502\n\u2502 132217-2 | nticscho | The      | 05:00:00 | tp; msgt |         |         |          |          |          |          \u2502\n\u2502 01902211 | lar.org/ | Rhinitis |          | ype=resp |         |         |          |          |          |          \u2502\n\u2502 \u2026        | \u2026        | Control  |          | \u2026        |         |         |          |          |          |          \u2502\n\u2502          |          | \u2026        |          |          |         |         |          |          |          |          \u2502\n\u2502 CC-MAIN- | https:// | Stourbri | 2023-07- | {\"applic | -1      | en      | 1.0      | 646      | 5.27709  | 230.3    \u2502\n\u2502 20190617 | www.stou | dge      | 04       | ation/ht |         |         |          |          |          |          \u2502\n\u2502 103006-2 | rbridgen | College  | 05:00:00 | tp; msgt |         |         |          |          |          |          \u2502\n\u2502 01906171 | ews.co.u | to close |          | ype=resp |         |         |          |          |          |          \u2502\n\u2502 \u2026        | \u2026        | BMe\u2026     |          | \u2026        |         |         |          |          |          |          \u2502\n|----------|----------|----------|----------|----------|---------|---------|----------|----------|----------|----------|\n</code></pre></p>"},{"location":"transforms/universal/filter/#running","title":"Running","text":"<p>You can run the python-only implementation or the ray-based implementation or the spark-based implementation to transform the <code>test1.parquet</code> file in test input data to an <code>output</code> directory.  The directory will contain both the new annotated <code>test1.parquet</code> file and the <code>metadata.json</code> file.</p>"},{"location":"transforms/universal/filter/#running-as-ray-based-application","title":"Running as ray-based application","text":"<pre><code>make venv\nsource venv/bin/activate\n(venv) cma:src$ python -m dpk_filter.ray.transform\n12:48:01 INFO - Running locally\n12:48:01 INFO - Using local configuration with: input_folder - /home/cma/de/data-prep-kit/transforms/universal/filter/test-data/input output_folder - /home/cma/de/data-prep-kit/transforms/universal/filter/output\n12:48:01 INFO - Not using data sets, checkpointing False, max files -1\n12:48:01 INFO - number of workers 5 worker options {'num_cpus': 0.8}\n12:48:01 INFO - pipeline id pipeline_id; number workers 5\n12:48:01 INFO - job details {'job category': 'preprocessing', 'job name': 'filter', 'job type': 'ray', 'job id': 'job_id'}\n12:48:01 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n12:48:01 INFO - actor creation delay 0\n2024-03-31 12:48:03,390 INFO worker.py:1715 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 \n(orchestrate pid=308034) 12:48:04 INFO - orchestrator started at 2024-03-31 12:48:04\n(orchestrate pid=308034) 12:48:04 INFO - Number of files is 1, source profile {'max_file_size': 0.15915775299072266, 'min_file_size': 0.15915775299072266, 'total_file_size': 0.15915775299072266}\n(orchestrate pid=308034) 12:48:04 INFO - Cluster resources: {'cpus': 20, 'gpus': 0, 'memory': 31.60095291212201, 'object_store': 15.800476455129683}\n(orchestrate pid=308034) 12:48:04 INFO - Number of workers - 5 with {'num_cpus': 0.8} each\n(orchestrate pid=308034) 12:48:04 INFO - Completed 0 files in 5.312760670979818e-06 min. Waiting for completion\n(orchestrate pid=308034) 12:48:06 INFO - Completed processing in 0.022701112429300944 min\n12:48:16 INFO - Completed execution in 0.24697633584340414 min, execution result 0\n</code></pre>"},{"location":"transforms/universal/filter/#running-as-pure-python-application","title":"Running as pure python application","text":"<pre>\nmake venv\nsource venv/bin/activate\n(venv) % python -m dpk_filter.transform_python\ninput table has 100 rows\n\noutput table has 11 rows\noutput metadata : {'total_docs_count': 100, 'total_bytes_count': 478602, 'total_columns_count': 25, \"docs_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'\": 78, \"bytes_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'\": 429191, \"docs_filtered_by 'docq_perplex_score &lt; 230'\": 53, \"bytes_filtered_by 'docq_perplex_score &lt; 230'\": 275911, 'docs_after_filter': 11, 'bytes_after_filter': 24061, 'columns_after_filter': 23}\n(venv) % deactivate\n\nls output\nmetadata.json   test1.parquet\n\n</pre>"},{"location":"transforms/universal/filter/#passing-parameters-through-command-line-interface","title":"Passing parameters through command-line-interface","text":"<p>When running filtering on a local terminal, double quotes need to be escaped accordingly. For example, to find documents that are written in Java or Python programming languages, a SQL query using the <code>IN</code> keyword is needed in the <code>filter_criteria_list</code> argument. The example below shows how to properly pass this argument to the filter app: <pre><code>python -m dpk_filter.ray.transform --filter_criteria_list \"[\\\"language IN ('Java', 'Python')\\\"]\" ...\n</code></pre> When filter runs from the command line, it needs to include the entire <code>filter_criteria_list</code> parameter within double quotes (<code>\"</code>), so that the command line parser can determine where the parameter begins and ends. This, however, will conflict with the internal double quotes that are used to specify the conditions inside the list (<code>language IN ('Java', 'Python')</code>). To resolve this problem, the internal double quotes need to be escaped, as in the \\\"language IN ('Java', 'Python')\\\" notation.</p>"},{"location":"transforms/universal/filter/#filter-statistics","title":"Filter Statistics","text":"<p>As shown in the output of the local run of filtering, the metadata contains several statistics: * Global statistics:   * <code>total_docs_count</code>, <code>total_bytes_count</code>, <code>total_columns_count</code>: total number of documents (rows), bytes, and columns that were present in the input table, before filtering took place   * <code>docs_after_filter</code>, <code>bytes_after_filter</code>, <code>columns_after_filter</code>: total number of documents (rows), bytes, and columns that were present in the output table, after filtering took place * Per-criteria statistics: these statistics show the impact of each filtering criteria - number of documents and bytes that it filters out, when applied by itself. We ran the local filter with two filtering criteria, and these are the stats for each of them:   * <code>docs_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'</code>, <code>bytes_filtered_by 'docq_total_words &gt; 100 AND docq_total_words &lt; 200'</code> - the number of documents and bytes filtered out by the <code>docq_total_words &gt; 100 AND docq_total_words &lt; 200</code> filtering condition   * <code>docs_filtered_by 'docq_perplex_score &lt; 230'</code>, <code>bytes_filtered_by 'docq_perplex_score &lt; 230'</code>  - the number of documents and bytes filtered out by the <code>docq_perplex_score &lt; 230</code> filtering condition  </p>"},{"location":"transforms/universal/filter/#additional-launched-command-line-options","title":"Additional Launched Command Line Options","text":"<p>The following command line arguments are available in addition to  the options provided by  the launcher.</p> <pre><code>  --filter_criteria_list FILTER_CRITERIA_LIST\n                        list of filter criteria (in SQL WHERE clause format), for example: [\n                          \"docq_total_words &gt; 100 AND docq_total_words &lt; 200\",\n                          \"docq_perplex_score &lt; 230\",\n                          \"date_acquired BETWEEN '2023-07-04' AND '2023-07-08'\",\n                          \"title LIKE 'https://%'\",\n                          \"document_id IN ('doc-id-1', 'doc-id-2', 'doc-id-3')\"\n                        ]\n  --filter_columns_to_drop FILTER_COLUMNS_TO_DROP\n                        list of columns to drop after filtering, for example: [\"column1\", \"column2\"]\n  --filter_logical_operator {AND,OR}\n                        logical operator (AND or OR) that joins filter criteria\n</code></pre>"},{"location":"transforms/universal/filter/#code-example","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/universal/filter/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/filter/#filtering-transform-for-ray","title":"Filtering Transform for Ray","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/filter/#summary_1","title":"Summary","text":"<p>This project wraps the filter transform with a Ray runtime.</p>"},{"location":"transforms/universal/filter/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined here, the set of  launcher options are available.</p>"},{"location":"transforms/universal/filter/#code-example-ray","title":"Code example (Ray)","text":"<p>notebook</p>"},{"location":"transforms/universal/filter/#filtering-transform-for-spark","title":"Filtering Transform for Spark","text":"<p>You can run the Spark filter transform local.py to filter the <code>test1.parquet</code> file in test input data to an <code>output</code> directory.  The directory will contain one or several filtered parquet files and the <code>metadata.json</code> file.</p>"},{"location":"transforms/universal/filter/#running-as-spark-based-application","title":"Running as Spark-based application","text":"<pre><code>make venv\nsource venv/bin/activate\n(venv) python -m dpk_filter.spark.transform\n18:57:46 INFO - data factory data_ is using local data access: input_folder - /home/cma/de/data-prep-kit/transforms/universal/filter/spark/test-data/input output_folder - /home/cma/de/data-prep-kit/transforms/universal/filter/spark/output at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:185\"\n18:57:46 INFO - data factory data_ max_files -1, n_sample -1 at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:201\"\n18:57:46 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'] at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/data_access/data_access_factory.py:214\"\n18:57:46 INFO - pipeline id pipeline_id at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/runtime/execution_configuration.py:80\"\n18:57:46 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'} at \"/home/cma/de/data-prep-kit/data-processing-lib/ray/src/data_processing/runtime/execution_configuration.py:83\"\n18:57:46 INFO - spark execution config : {'spark_local_config_filepath': '/home/cma/de/data-prep-kit/transforms/universal/filter/spark/config/spark_profile_local.yml', 'spark_kube_config_filepath': 'config/spark_profile_kube.yml'} at \"/home/cma/de/data-prep-kit/data-processing-lib/spark/src/data_processing_spark/runtime/spark/spark_execution_config.py:42\"\n24/05/26 18:57:47 WARN Utils: Your hostname, li-7aed0a4c-2d51-11b2-a85c-dfad31db696b.ibm.com resolves to a loopback address: 127.0.0.1; using 192.168.1.223 instead (on interface wlp0s20f3)\n24/05/26 18:57:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/05/26 18:57:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18:57:50 INFO - files = ['/home/cma/de/data-prep-kit/transforms/universal/filter/spark/test-data/input/test1.parquet'] at \"/home/cma/de/data-prep-kit/data-processing-lib/spark/src/data_processing_spark/runtime/spark/spark_launcher.py:188\"\n</code></pre> <p>Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/filter/#passing-parameters-through-command-line-interface_1","title":"Passing parameters through command-line-interface","text":"<p>When running filtering on a local terminal, double quotes need to be escaped accordingly. For example, to find documents that are written in Java or Python programming languages, a SQL query using the <code>IN</code> keyword is needed in the <code>filter_criteria_list</code> argument. The example below shows how to properly pass this argument to the filter app: <pre><code>python -m dpk_filter.spark.transform --filter_criteria_list \"[\\\"language IN ('Java', 'Python')\\\"]\" ...\n</code></pre> When filter runs from the command line, it needs to include the entire <code>filter_criteria_list</code> parameter within double quotes (<code>\"</code>), so that the command line parser can determine where the parameter begins and ends. This, however, will conflict with the internal double quotes that are used to specify the conditions inside the list (<code>language IN ('Java', 'Python')</code>). To resolve this problem, the internal double quotes need to be escaped, as in the \\\"language IN ('Java', 'Python')\\\" notation.</p>"},{"location":"transforms/universal/filter/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>When running the transform with the Spark launcher (i.e., SparkTransformLauncher), the following command line arguments are available in addition to  the options provided by the spark launcher.</p> <pre><code>  --filter_criteria_list FILTER_CRITERIA_LIST\n                        list of filter criteria (in SQL WHERE clause format), for example: [\n                          \"docq_total_words &gt; 100 AND docq_total_words &lt; 200\",\n                          \"docq_perplex_score &lt; 230\",\n                          \"date_acquired BETWEEN '2023-07-04' AND '2023-07-08'\",\n                          \"title LIKE 'https://%'\",\n                          \"document_id IN ('doc-id-1', 'doc-id-2', 'doc-id-3')\"\n                        ]\n  --filter_columns_to_drop FILTER_COLUMNS_TO_DROP\n                        list of columns to drop after filtering, for example: [\"column1\", \"column2\"]\n  --filter_logical_operator {AND,OR}\n                        logical operator (AND or OR) that joins filter criteria\n</code></pre>"},{"location":"transforms/universal/filter/kfp_ray/","title":"Filter Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/filter/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/filter/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/hap/","title":"Hate, Abuse, and Profanity (HAP) Annotation","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/hap/#contributor","title":"Contributor","text":"<ul> <li>Yang Zhao (yangzhao@ibm.com)</li> </ul>"},{"location":"transforms/universal/hap/#description","title":"Description","text":""},{"location":"transforms/universal/hap/#prerequisite","title":"Prerequisite","text":"<p>This repository needs NLTK and please refer to <code>requirements.txt</code>.</p>"},{"location":"transforms/universal/hap/#overview","title":"Overview","text":"<p>The hap transform maps a non-empty input table to an output table with an added <code>hap_score</code> column. Each row in the table represents a document, and the hap transform performs the following three steps to calculate the hap score for each document:</p> <ul> <li>Sentence spliting: we use NLTK to split the document into sentence pieces.</li> <li>hap annotation: each sentence is assigned a hap score between 0 and 1, where 1 represents hap and 0 represents non-hap.</li> <li>Aggregation: the document hap score is determined by selecting the maximum hap score among its sentences.</li> </ul>"},{"location":"transforms/universal/hap/#input-format","title":"input format","text":"<p>The input is in .parquet format and contains the following columns:</p> doc_id contents 1 GSC is very much a little Swiss Army knife for... 2 Here are only a few examples. And no, I'm not ..."},{"location":"transforms/universal/hap/#output-format","title":"output format","text":"<p>The output is in .parquet format and includes an additional column, in addition to those in the input:</p> doc_id contents hap_score 1 GSC is very much a little Swiss Army knife for... 0.002463 2 Here are only a few examples. And no, I'm not ... 0.989713"},{"location":"transforms/universal/hap/#configuration","title":"Configuration","text":"<p>The set of dictionary keys holding HAPTransformConfiguration  configuration for values are as follows:</p> <ul> <li>--model_name_or_path - specify the HAP model, which should be compatible with HuggingFace's AutoModelForSequenceClassification. Defaults to IBM's open-source toxicity classifier <code>ibm-granite/granite-guardian-hap-38m</code>.</li> <li>--batch_size - modify it based on the infrastructure capacity. Defaults to <code>128</code>.</li> <li>--max_length - the maximum length for the tokenizer. Defaults to <code>512</code>.</li> <li>--doc_text_column - the column name containing the document text in the input .parquet file. Defaults to <code>contents</code>.</li> <li>--annotation_column - the column name containing hap (toxicity) score in the output .parquet file. Defaults to <code>hap_score</code>.</li> </ul>"},{"location":"transforms/universal/hap/#usage","title":"Usage","text":"<p>Place your input Parquet file in the <code>test-data/input/</code> directory. A sample file, <code>test1.parquet</code>, is available in this directory. Once done, run the script.</p> <pre><code>python local_python.py\n</code></pre> <p>You will obtain the output file <code>test1.parquet</code> in the output directory.</p>"},{"location":"transforms/universal/hap/#code-example","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/universal/hap/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/hap/#testing","title":"Testing","text":"<p>Currently we have: - hap test</p>"},{"location":"transforms/universal/hap/#throughput","title":"Throughput","text":"<p>The table below shows the throughput (tokens per second) of the HAP transform module, which primarily includes sentence splitting, HAP annotation, and HAP score aggregation. We herein compare two models:</p> <ul> <li>4-layer lightweight toxicity classifier ibm-granite/granite-guardian-hap-38m</li> <li>12-layer toxicity classifier ibm-granite/granite-guardian-hap-125m</li> </ul> <p>We processed 6,000 documents (12 MB in Parquet file size) using the HAP transform module and reported the average CPU throughput over three trials.</p> Model used in HAP transform module throughput (tokens per second) granite-guardian-hap-38m 6.16 k granite-guardian-hap-125m 1.14 k"},{"location":"transforms/universal/hap/#credits","title":"Credits","text":"<p>The HAP transform is jointly developed by IBM Research - Tokyo and Yorktown.</p>"},{"location":"transforms/universal/hap/#hate-abuse-and-profanity-hap-annotation_1","title":"Hate, Abuse, and Profanity (HAP) Annotation","text":""},{"location":"transforms/universal/hap/#hap-transform-for-ray","title":"HAP Transform for Ray","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/hap/#summary","title":"Summary","text":"<p>This project wraps the hap transform with a Ray runtime.</p>"},{"location":"transforms/universal/hap/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/hap/#running","title":"Running","text":""},{"location":"transforms/universal/hap/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined here, the set of  launcher options are available.</p>"},{"location":"transforms/universal/noop/","title":"NOOP Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/noop/#summary","title":"Summary","text":"<p>This transform serves as a template for transform writers as it does not perform any transformations on the input (i.e., a no-operation transform). As such, it simply copies the input parquet files to the output directory. It shows the basics of creating a simple 1:1 table transform. It also implements a single configuration value to show how configuration of the transform is implemented.</p>"},{"location":"transforms/universal/noop/#output-format","title":"Output Format","text":"<p>The noop transform simply copies the input, so the output format is the same as the input.</p> Output column name Data type Description same as input same as input same as input ... ... ..."},{"location":"transforms/universal/noop/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The transform can be initialized with the following parameters found in NOOPTransform </p> Parameter Default Description <code>noop_sleep_sec</code> 1 Number of seconds to sleep while inside the transform() method.  This may be useful to simulate transform timeings and as a way to limit I/O bandwidth use. <code>noop_pwd</code> None specifies a dummy password not included in metadata. Provided as an example of metadata that we want to not include in logging. <p>When running the transform with a launcher (i.e. TransformLauncher), the above are available as command line options in addition to the options provided by the launcher.</p>"},{"location":"transforms/universal/noop/#usage","title":"Usage","text":""},{"location":"transforms/universal/noop/#command-line-launched","title":"Command Line-Launched","text":""},{"location":"transforms/universal/noop/#creating-the-virtual-environment","title":"Creating the Virtual Environment","text":"<p>First we need a python environment containing the Noop transform. We create the virtual environment in the project: <pre><code>make venv\nsource venv/bin/activate\n</code></pre> or by installing the DPK transform wheel <pre><code>python -m venv venv\nsource venv/bin/activate\npip install data-prep-transforms\n</code></pre> Now that we have a virtual environment containing the transform, we invoke the transform from the CLI using the runtime parameters and those from the transform itself (i.e. the table above). For example, to run the transform in the python runtime,  <pre><code>make venv\nsource venv/bin/activate\npython -m dpk_noop.runtime --noop_sleep_sec 10 \\\n    --data_local '{ \"input_folder\": \"test-data/input\", \"output_folder\": \"output\" }'\ndeactivate\n</code></pre> or in the Ray runtime using a local Ray cluster,  <pre><code>...\npython -m dpk_noop.ray.runtime --run_locally True --noop_sleep_sec 10 \\\n    --data_local '{ \"input_folder\": \"test-data/input\", \"output_folder\": \"output\" }'\n...\n</code></pre> or in the spark runtime,  <pre><code>...\npython -m dpk_noop.spark.runtime --noop_sleep_sec 10 \\\n    --data_local '{ \"input_folder\": \"test-data/input\", \"output_folder\": \"output\" }'\n...\n</code></pre></p> <p><pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/noop/#image-launched","title":"Image-Launched","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/noop/kfp_ray/","title":"NOOP Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/noop/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/noop/kfp_ray/#pipeline-file-generation","title":"Pipeline file generation","text":"<p>In order to generate a pipeline python file run <pre><code>make workflow-generate\n</code></pre> This will use the pipeline_definitions.yaml to generate the python file of the pipeline. It uses the pipeline generator directory.</p>"},{"location":"transforms/universal/noop/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/profiler/","title":"Profiler Transform","text":""},{"location":"transforms/universal/profiler/#summary","title":"Summary","text":"<p>Profiler implement a word count. Typical implementation of the word count is done using map reduce. * It\u2019s O(N2) complexity * shuffling with lots of data movement</p> <p>Implementation here is using \u201cstreaming\u201d aggregation, based on central cache:</p> <ul> <li>At the heart of the implementation is a cache of partial word counts, implemented as a set of Ray actors and containing  word counts processed so far.</li> <li>Individual data processors are responsible for:</li> <li>Reading data from data plane</li> <li>tokenizing documents (we use pluggable tokenizer)</li> <li>Coordinating with distributed cache to collect overall word counts</li> </ul> <p>The complication of mapping this model to transform model is the fact that implementation requires an aggregators cache,  that transform mode knows nothing about. The solution here is to use transform runtime to create cache and pass it as a parameter to transforms.</p>"},{"location":"transforms/universal/profiler/#transform-runtime","title":"Transform runtime","text":"<p>Transform runtime is responsible for creating cache actors and sending their  handles to the transforms themselves. Additionally, it writes created word counts to the data storage (as .csv files) and enhances statistics information with the information about cache size and utilization</p>"},{"location":"transforms/universal/profiler/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding ProfilerTransform configuration for values are as follows:</p> <ul> <li>doc_column - specifies name of the column containing documents</li> </ul>"},{"location":"transforms/universal/profiler/#running","title":"Running","text":""},{"location":"transforms/universal/profiler/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Python launcher (i.e., TransformLauncher), the following command line arguments are available in addition to the options provided by the launcher.</p> <pre><code>  --profiler_doc_column PROFILER_DOC_COLUMN\n                        key for accessing data\n ```\n\nThese correspond to the configuration keys described above.\n\n### Running the samples\nTo run the samples, run the following command from the transform folder transform/universal/profiler\n\nFor example, \n```shell\nmake venv &amp;&amp; source venv/bin/activate\npython -m dpk_profiler.local\n</code></pre>"},{"location":"transforms/universal/profiler/#code-example","title":"Code example","text":"<p>notebook</p>"},{"location":"transforms/universal/profiler/#transform-ray-runtime","title":"Transform ray runtime","text":"<p>Transform ray runtime is responsible for creating cache actors and sending their  handles to the transforms themselves.</p>"},{"location":"transforms/universal/profiler/#configuration-and-command-line-options_1","title":"Configuration and command line Options","text":"<p>In addition to the configuration parameters defined above,  Ray version adds the following parameters:</p> <ul> <li>aggregator_cpu - specifies an amount of CPUs per aggregator actor</li> <li>num_aggregators - specifies number of aggregator actors</li> </ul>"},{"location":"transforms/universal/profiler/#launched-command-line-options_1","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e., TransformLauncher), the following command line arguments are available in addition to the options provided by the launcher:</p> <pre><code>  --profiler_aggregator_cpu PROFILER_AGGREGATOR_CPU\n                        number of CPUs per aggrigator\n  --profiler_num_aggregators PROFILER_NUM_AGGREGATORS\n                        number of agregator actors to use\n  --profiler_doc_column PROFILER_DOC_COLUMN\n                        key for accessing data\n ```\n\nThese correspond to the configuration keys described above.\n\n### Running the samples\nTo run the samples, run the following command from the transform folder transform/universal/profiler\n\nFor example, \n```shell\nmake venv &amp;&amp; source venv/bin/activate\npython -m dpk_profiler.ray.local\n</code></pre>"},{"location":"transforms/universal/profiler/#code-example-ray-runtime","title":"Code example (Ray runtime)","text":"<p>notebook</p>"},{"location":"transforms/universal/profiler/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/profiler/kfp_ray/","title":"Profiler Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/profiler/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/profiler/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/profiler/kfp_ray/src/","title":"Profiler Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/profiler/kfp_ray/src/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/profiler/kfp_ray/src/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/rep_removal/","title":"Repetition Removal Transform","text":"<p>This tranforms performs text repetition removal to remove sequences that frequently occur at documents within a single parquet file level.</p> <p>The work is adopted from https://github.com/google-research/deduplicate-text-datasets to identify and remove all substrings of a given length that are repeated more than some threshold number of times.</p> <p>Several enhancements have been made to run at scale at single-parquet file level:</p> <pre><code>- Run in the same python space\n\n- Avoid resource conflict having two processes handling two different parquet files (in making suffix_array and other steps)\n\n- Cleanup (to prevent data fill up in container environment)\n\n- Avoid I/O processes (file read/write operations)\n\n- Eliminate encoding and decoding procedures when working with related files\n\n- Optimize data loading (loading the data once)\n\n- Optimize data encoding/decoding for saving the generated output\n\n- Save the tokenizer in a local path and load it from local path to speed up the process\n\n- Copy the empty parquet file to target folder without processing\n</code></pre> <p>These enhancements speed up the task, minimize the disk storage utilization, and improve parallelism on each node.</p> <p>The original code from https://github.com/google-research/deduplicate-text-datasets removes all copies of the duplicates.  Another modification has been made to retain the first copy of each duplicate cluster. </p> <p>This repetition removal task can be fine-tuned by adjusting the length_threshold(repeated text sequence length) and frequency_threshold. </p>"},{"location":"transforms/universal/rep_removal/#contributors","title":"Contributors","text":"<ul> <li>Shalisha Witherspoon (shalisha.witherspoon@ibm.com)</li> <li>Hajar Emami Gohari (Hajar.Emami@ibm.com)</li> </ul>"},{"location":"transforms/universal/rep_removal/#requirements","title":"Requirements","text":"<p>To run the repetition removal transform, Rust is required to be installed on the machine.  You can install rust following instructions here.</p> <p>gcc is also required to be present on the machine. Run <code>gcc -v</code> to see if already installed. Otherwise,  you can find information for installing here.</p>"},{"location":"transforms/universal/rep_removal/#running-on-m1-mac","title":"Running on M1 Mac","text":"<p>To run the Transform on an M1 mac, there are a few prerequisites you must run to get started. </p> <p>A) Install a compatible psutils library in the environment (uninstall if already present): <pre><code>pip uninstall psutil\npip install --no-binary :all: psutil\n</code></pre></p>"},{"location":"transforms/universal/rep_removal/#input-parameters","title":"Input Parameters","text":"<p>The transform can be initialized with the following parameters:</p> Parameter Default Description <code>rep_removal_contents_column_name</code> <code>contents</code> Name of the column holding the document contents <code>rep_removal_dedup_level_name</code> <code>parquet</code> Name of the type of file to process <code>rep_remova_length_thresh</code> <code>50</code> Length threshold for processing <code>rep_removal_frequency_threshold</code> <code>1</code> Frequency threshold for processing <code>rep_removal_retain_first_copy</code> <code>True</code> Boolean value for whether to retain first copy <code>rep_removal_tokenize</code> <code>True</code> Boolean value for whether to tokenize <code>rep_removal_num_threads</code> <code>psutils.cpu_count(logical=False)</code> Value for number of threads to use for processing"},{"location":"transforms/universal/rep_removal/#output-format","title":"Output Format","text":"<p>The output format will be a new parquet file with the repeated sequence(s) removed, for example: <pre><code>orig parquet:\n0 A staffer sells cars via livestream at a deale... ...           0.012263\n1 The May 1st submission deadline may feel like ... ...           0.000067\n2 Yes! Cinnamon Oil is a great way to deter mice... ...           0.021643\n3 Rosemary Oil can be used to deter cockroaches.... ...           0.005885\n4 A cat might have discovered an insect crawling... ...           0.881134\n5 A staffer sells cars via livestream at a deale... ...           0.012263\n6 The May 1st submission deadline may feel like ... ...           0.000067\n7 Yes! Cinnamon Oil is a great way to deter mice... ...           0.021643\n8 Rosemary Oil can be used to deter cockroaches.... ...           0.005885\n9 A cat might have discovered an insect crawling... ...           0.881134\n</code></pre></p> <pre><code>dedup output:\n0 A staffer sells cars via livestream at a deale... ...           0.012263\n1 The May 1st submission deadline may feel like ... ...           0.000067\n2 Yes! Cinnamon Oil is a great way to deter mice... ...           0.021643\n3 Rosemary Oil can be used to deter cockroaches.... ...           0.005885\n4 A cat might have discovered an insect crawling... ...           0.881134\n5                                                   ...           0.012263\n6                                                   ...           0.000067\n7                                                   ...           0.021643\n8                                                   ...           0.005885\n9                                                   ...           0.881134\n</code></pre>"},{"location":"transforms/universal/rep_removal/#basic-usage","title":"Basic Usage","text":""},{"location":"transforms/universal/rep_removal/#run-via-command-line","title":"Run via Command Line","text":"<p>You can invoke the transform via command line, as shown in sample make command <code>make run-cli-sample</code>: <pre><code>python -m dpk_rep_removal.runtime \\\n                --data_local_config \"{ 'input_folder' : 'test-data/input', 'output_folder' : 'output'}\" \\\n                --rep_removal_contents_column_name 'text' \n</code></pre></p>"},{"location":"transforms/universal/rep_removal/#run-in-a-notebook","title":"Run in a notebook","text":"<p>A sample notebook here shows how to run the code with python.</p>"},{"location":"transforms/universal/rep_removal/#run-in-a-container","title":"Run in a container","text":"<p>There are docker files available for building an image to run the code with pure python, or with ray.</p> <p>For details on building and running the image, please refer to the running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/resize/","title":"Resize","text":"<p>The resize transforms allows to change the sizes of input files (both split larger ones and combine smaller). Per the set of transform project conventions the following runtimes are available:</p> <ul> <li>python - provides the base python-based transformation   implementation.</li> <li>ray - enables the running of the base python transformation   in a Ray runtime</li> <li>kfp - enables running the ray docker image   in a kubernetes cluster using a generated <code>yaml</code> file.</li> </ul> <p>Also, please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/resize/#summary","title":"Summary","text":"<p>This is a simple transformer that is resizing the input tables to a specified size.  * resizing based on in-memory size of the tables. * resizing based on the number of rows in the tables. </p> <p>Tables can be either split into smaller sizes or aggregated into larger sizes.</p>"},{"location":"transforms/universal/resize/#building","title":"Building","text":"<p>A docker file that can be used for building docker image. You can use</p> <pre><code>make build \n</code></pre>"},{"location":"transforms/universal/resize/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>The set of dictionary keys holding ResizeTransform configuration for values are as follows:</p> <ul> <li>max_rows_per_table - specifies max documents per table</li> <li>max_mbytes_per_table - specifies max size of table, according to the _size_type value.</li> <li>size_type - indicates how table size is measured. Can be one of<ul> <li>memory - table size is measure by the in-process memory used by the table</li> <li>disk - table size is estimated as the on-disk size of the parquet files.  This is an estimate only     as files are generally compressed on disk and so may not be exact due varying compression ratios.     This is the default.</li> </ul> </li> </ul> <p>Only one of the max_rows_per_table and max_mbytes_per_table may be used.</p>"},{"location":"transforms/universal/resize/#running","title":"Running","text":""},{"location":"transforms/universal/resize/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>When running the transform with the Ray launcher (i.e., TransformLauncher), the following command line arguments are additionally available  the options provided by the launcher and map to the configuration keys above.</p> <pre><code>  --resize_max_rows_per_table RESIZE_MAX_ROWS_PER_TABLE\n                        Max number of rows per table\n  --resize_max_mbytes_per_table RESIZE_MAX_MBYTES_PER_TABLE\n                        Max table size (MB). Size is measured according to the --resize_size_type parameter\n  --resize_size_type {disk,memory}\n                        Determines how memory is measured when using the --resize_max_mbytes_per_table option.\n                        'memory' measures the in-process memory footprint and \n                        'disk' makes an estimate of the resulting parquet file size.\n</code></pre>"},{"location":"transforms/universal/resize/#command-line-launched-example","title":"Command Line-Launched example","text":""},{"location":"transforms/universal/resize/#creating-the-virtual-environment","title":"Creating the Virtual Environment","text":"<p>First we need a python environment containing the transform. We create the virtual environment in the transform folder (transforms/universal/resize): <pre><code>make venv\nsource venv/bin/activate\n</code></pre> Run the transform from the command line using <pre><code>python -m dpk_noop.runtime --resize_max_rows_per_table= 125 \\\n    --data_local '{ \"input_folder\": \"test-data/input\", \"output_folder\": \"output\" }'\n</code></pre> Exit the virtual environment and list parquet files produced by the transform <pre><code>deactivate\nls output\n</code></pre></p>"},{"location":"transforms/universal/resize/#code-examples","title":"Code examples","text":"<p>notebook (python runtime)</p> <p>notebook(ray runtime)</p>"},{"location":"transforms/universal/resize/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate. </p> <p>We also provide several demos of the transform usage for different data storage options, including local file system and s3.</p>"},{"location":"transforms/universal/resize/kfp_ray/","title":"Document Identification Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/resize/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/resize/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/tokenization/","title":"Index","text":"<p> Distributed tokenization module for data sets using any Hugging Face compatible tokenizer.      </p>"},{"location":"transforms/universal/tokenization/#contributors","title":"Contributors","text":"<ul> <li>Xuan-Hong Dang (xuan-hong.dang@ibm.com)</li> </ul>"},{"location":"transforms/universal/tokenization/#data-tokenization","title":"Data Tokenization","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/tokenization/#summary","title":"Summary","text":"<p>The data tokenization transform operates by converting a (non-empty) input table into an output table  using a pre-trained tokenizer. The input table is required to have a minimum of two columns,  named <code>document_id</code> and <code>contents</code> by default. However, alternate column names can be specified using  <code>--tkn_doc_id_column</code> for the document id and <code>--tkn_doc_content_column</code> for the document contents. It is essential for the values within the <code>document_id</code> column to be unique across the dataset,  while the <code>contents</code> column stores their respective document content. To execute example demonstrations within this directory,  a machine with <code>64GiB</code> of RAM is recommended.</p> <p>To specify a pre-trained tokenizer, utilize the <code>--tkn_tokenizer</code> parameter.  This parameter accepts the name of a tokenizer ready for download from Hugging Face,  such as <code>hf-internal-testing/llama-tokenizer, bigcode/starcoder</code>, or any other tokenizer compatible  with the Hugging Face AutoTokenizer library. Additionally, you can employ the <code>--tkn_tokenizer_args</code> parameter  to include extra arguments specific to the chosen tokenizer.  For instance, when loading a Hugging Face tokenizer like <code>bigcode/starcoder</code>, which necessitate an access token,  you can specify <code>use_auth_token=&lt;your token&gt;</code> in <code>--tkn_tokenizer</code>. </p> <p>The tokenization transformer utilizes the specified tokenizer to tokenize each row,  assuming each row represents a document, in the input table and save it to a corresponding row in the output table.  The output table generally consists of four columns: <code>tokens, document_id, document_length</code>, and <code>token_count</code>.</p> <p>The <code>tokens</code> stores the sequence of token IDs generated by the tokenizer during the document tokenization process.  The <code>document_id</code> (or the designated name specified in <code>--tkn_doc_id_column</code>) contains the document ID,  while <code>document_length</code> and <code>token_count</code> respectively record the length of the document and the total count of generated tokens. During tokenization, the tokenizer will disregard empty documents (rows) in the input table,  as well as documents that yield no tokens or encounter failure during tokenization.  The count of such documents will be stored in the <code>num_empty_rows</code> field of the <code>metadata</code> file.</p> <p>In certain cases, the tokenization process of some tokenizers may be sluggish,  particularly when handling lengthy documents containing millions of characters.  To address this, you can employ the <code>--tkn_chunk_size</code> parameter to define the length of chunks to tokenize at a given time. For English text (<code>en</code>), it is recommended to set the chunk size to <code>20,000</code>, roughly equivalent to <code>15</code> pages of text.  The tokenizer will then tokenize each chunk separately and combine their resulting token IDs. By default, the value of <code>--tkn_chunk_size</code> is <code>0</code>, indicating that each document is tokenized as a whole, regardless of its length.</p>"},{"location":"transforms/universal/tokenization/#running","title":"Running","text":""},{"location":"transforms/universal/tokenization/#cli-options","title":"CLI Options","text":"<p>The following command line arguments are available in addition to  the options provided by the launcher. <pre><code>  --tkn_tokenizer TKN_TOKENIZER\n                        Tokenizer used for tokenization. It also can be a path to a pre-trained tokenizer. By defaut, `hf-internal-testing/llama-tokenizer` from HuggingFace is used\n  --tkn_tokenizer_args TKN_TOKENIZER_ARGS\n                        Arguments for tokenizer. For example, `cache_dir=/tmp/hf,use_auth_token=Your_HF_authentication_token` could be arguments for tokenizer `bigcode/starcoder` from HuggingFace\n  --tkn_doc_id_column TKN_DOC_ID_COLUMN\n                        Column contains document id which values should be unique across dataset\n  --tkn_doc_content_column TKN_DOC_CONTENT_COLUMN\n                        Column contains document content\n  --tkn_text_lang TKN_TEXT_LANG\n                        Specify language used in the text content for better text splitting if needed\n  --tkn_chunk_size TKN_CHUNK_SIZE\n                        Specify &gt;0 value to tokenize each row/doc in chunks of characters (rounded in words)\n</code></pre></p>"},{"location":"transforms/universal/tokenization/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use the following <code>make</code> target</p> <ul> <li><code>run-cli-sample</code> - runs dpk_tokenization/transform_python.py using command line args</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/tokenization/#code-example","title":"Code example","text":"<p>Here is a sample notebook</p>"},{"location":"transforms/universal/tokenization/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/tokenization/#tokenization-transform-for-ray","title":"Tokenization Transform for Ray","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/tokenization/#summary_1","title":"Summary","text":"<p>This project wraps the tokenization transform with a Ray runtime.</p>"},{"location":"transforms/universal/tokenization/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/tokenization/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  launcher options are available.</p>"},{"location":"transforms/universal/tokenization/kfp_ray/","title":"Tokenization Ray-base KubeFlow Pipeline Transformation","text":""},{"location":"transforms/universal/tokenization/kfp_ray/#summary","title":"Summary","text":"<p>This project allows execution of the noop Ray transform as a  KubeFlow Pipeline</p> <p>The detail pipeline is presented in the Simplest Transform pipeline tutorial </p>"},{"location":"transforms/universal/tokenization/kfp_ray/#compilation","title":"Compilation","text":"<p>In order to compile pipeline definitions run <pre><code>make workflow-build\n</code></pre> from the directory. It creates a virtual environment (make workflow-venv) and after that compiles the pipeline  definitions in the folder. The virtual environment is created once for all transformers. </p> <p>Note: the pipelines definitions can be compiled and executed on KFPv1 and KFPv2. Meantime, KFPv1 is our default. If you prefer KFPv2, please do the following: <pre><code>make clean\nexport KFPv2=1\nmake workflow-build\n</code></pre></p> <p>The next steps are described in Deploying a pipeline and Executing pipeline and watching execution results</p>"},{"location":"transforms/universal/tokenization2arrow/","title":"Tokenization2Arrow Transform","text":"<p>Please see the set of transform project conventions for details on general project conventions, transform configuration, testing and IDE set up.</p>"},{"location":"transforms/universal/tokenization2arrow/#contributors","title":"Contributors","text":"<ul> <li>Santosh Borse (ssborse@us.ibm.com)</li> </ul>"},{"location":"transforms/universal/tokenization2arrow/#summary","title":"Summary","text":"<p> Distributed tokenization module for data sets using any Hugging Face compatible tokenizer. This Tokenizer is built upon existing [DPK Tokenizer](https://github.com/IBM/data-prep-kit/tree/dev/transforms/universal/tokenization)       For every input .parquet file it generates .arrow and 2 metadata files ( in meta folder), - .arrow file - contains actual tokens - .docs file -  contains 1 line summary of file with content,          [full file path], documents: [total document count], tokens: [total token count]  - .doc.ids - contains details of token count for every document of file, contents looks like,          [document id], [document's token counts]  </p> <p>The data tokenization transform operates by converting a (non-empty) input table into an output table  using a pre-trained tokenizer. The input table is required to have a minimum of two columns,  named <code>document_id</code> and <code>contents</code> by default. However, alternate column names can be specified using  <code>--tkn_doc_id_column</code> for the document id and <code>--tkn_doc_content_column</code> for the document contents. It is essential for the values within the <code>document_id</code> column to be unique across the dataset,  while the <code>contents</code> column stores their respective document content. To execute example demonstrations within this directory,  a machine with <code>64GB</code> of RAM is recommended.</p> <p>To specify a pre-trained tokenizer, utilize the <code>--tkn_tokenizer</code> parameter.  This parameter accepts the name of a tokenizer ready for download from Hugging Face,  such as <code>hf-internal-testing/llama-tokenizer, bigcode/starcoder</code>, or any other tokenizer compatible  with the Hugging Face AutoTokenizer library. Additionally, you can employ the <code>--tkn_tokenizer_args</code> parameter  to include extra arguments specific to the chosen tokenizer.  For instance, when loading a Hugging Face tokenizer like <code>bigcode/starcoder</code>, which necessitate an access token,  you can specify <code>use_auth_token=&lt;your token&gt;</code> in <code>--tkn_tokenizer</code>. </p> <p>The tokenization transformer utilizes the specified tokenizer to tokenize each row,  assuming each row represents a document, in the input table and save it to a corresponding row in the output table.  The output table generally consists of four columns: <code>tokens, document_id, document_length</code>, and <code>token_count</code>.</p> <p>The <code>tokens</code> stores the sequence of token IDs generated by the tokenizer during the document tokenization process.  The <code>document_id</code> (or the designated name specified in <code>--tkn_doc_id_column</code>) contains the document ID,  while <code>document_length</code> and <code>token_count</code> respectively record the length of the document and the total count of generated tokens. During tokenization, the tokenizer will disregard empty documents (rows) in the input table,  as well as documents that yield no tokens or encounter failure during tokenization.  The count of such documents will be stored in the <code>num_empty_rows</code> field of the <code>metadata</code> file.</p> <p>In certain cases, the tokenization process of some tokenizers may be sluggish,  particularly when handling lengthy documents containing millions of characters.  To address this, you can employ the <code>--tkn_chunk_size</code> parameter to define the length of chunks to tokenize at a given time. For English text (<code>en</code>), it is recommended to set the chunk size to <code>20,000</code>, roughly equivalent to <code>15</code> pages of text.  The tokenizer will then tokenize each chunk separately and combine their resulting token IDs. By default, the value of <code>--tkn_chunk_size</code> is <code>0</code>, indicating that each document is tokenized as a whole, regardless of its length.</p>"},{"location":"transforms/universal/tokenization2arrow/#running","title":"Running","text":""},{"location":"transforms/universal/tokenization2arrow/#cli-options","title":"CLI Options","text":"<p>The following command line arguments are available in addition to  the options provided by the launcher. <pre><code>  --tkn_tokenizer TKN_TOKENIZER\n                        Tokenizer used for tokenization. It also can be a path to a pre-trained tokenizer. By defaut, `hf-internal-testing/llama-tokenizer` from HuggingFace is used\n  --tkn_tokenizer_args TKN_TOKENIZER_ARGS\n                        Arguments for tokenizer. For example, `cache_dir=/tmp/hf,use_auth_token=Your_HF_authentication_token` could be arguments for tokenizer `bigcode/starcoder` from HuggingFace\n  --tkn_doc_id_column TKN_DOC_ID_COLUMN\n                        Column contains document id which values should be unique across dataset\n  --tkn_doc_content_column TKN_DOC_CONTENT_COLUMN\n                        Column contains document content\n  --tkn_text_lang TKN_TEXT_LANG\n                        Specify language used in the text content for better text splitting if needed\n  --tkn_chunk_size TKN_CHUNK_SIZE\n                        Specify &gt;0 value to tokenize each row/doc in chunks of characters (rounded in words)\n</code></pre></p>"},{"location":"transforms/universal/tokenization2arrow/#running-the-samples","title":"Running the samples","text":"<p>To run the samples, use one of the following <code>make</code> targets:</p> <ul> <li><code>run-cli-sample-python</code> - runs dpk_tokenization2arrow using python runtime</li> </ul> <p>or </p> <ul> <li><code>run-cli-sample-ray</code> - runs dpk_tokenization2arrow using ray runtime</li> </ul> <p>These targets will activate the virtual environment and set up any configuration needed. Use the <code>-n</code> option of <code>make</code> to see the detail of what is done to run the sample.</p> <p>For example,  <pre><code>make run-cli-sample-python\n...\n</code></pre> Then  <pre><code>ls output\n</code></pre> To see results of the transform.</p>"},{"location":"transforms/universal/tokenization2arrow/#code-example","title":"Code example","text":"<p>Here is a sample notebook</p>"},{"location":"transforms/universal/tokenization2arrow/#transforming-data-using-the-transform-image","title":"Transforming data using the transform image","text":"<p>To use the transform image to transform your data, please refer to the  running images quickstart, substituting the name of this transform image and runtime as appropriate.</p>"},{"location":"transforms/universal/tokenization2arrow/#tokenization2arrow-transform-for-ray","title":"Tokenization2Arrow Transform for Ray","text":""},{"location":"transforms/universal/tokenization2arrow/#summary_1","title":"Summary","text":"<p>This project wraps the tokenization2arrow transform with a Ray runtime.</p>"},{"location":"transforms/universal/tokenization2arrow/#configuration-and-command-line-options","title":"Configuration and command line Options","text":"<p>Configuration and command line options are the same as for the base python transform. </p>"},{"location":"transforms/universal/tokenization2arrow/#launched-command-line-options","title":"Launched Command Line Options","text":"<p>In addition to those available to the transform as defined in here, the set of  launcher options are available.</p>"},{"location":"transforms/universal/tokenization2arrow/#code-example_1","title":"Code example","text":"<p>Here is a sample notebook that uses ray runtime. </p>"},{"location":"transforms/universal/web2parquet/","title":"Web Crawler to Parquet","text":"<p>This tranform crawls the web and downloads files in real-time.</p> <p>This first release of the transform, only accepts the following 4 parameters. Additional releases will extend the functionality to allow the user to specify additional constraints such as mime-type, domain-focus, etc.</p>"},{"location":"transforms/universal/web2parquet/#parameters","title":"Parameters","text":"<p>For configuring the crawl, users need to specify the following parameters:</p> parameter:type Description urls:list list of seed URLs (i.e., ['https://thealliance.ai'] or ['https://www.apache.org/projects','https://www.apache.org/foundation']). The list can include any number of valid URLS that are not configured to block web crawlers depth:int control crawling depth downloads:int number of downloads that are stored to the download folder. Since the crawler operations happen asynchronously, the process can result in any 10 of the visited URLs being retrieved (i.e. consecutive runs can result in different files being downloaded) folder:str folder where downloaded files are stored. If the folder is not empty, new files are  added or replace the existing ones with the same URLs"},{"location":"transforms/universal/web2parquet/#install-the-transform","title":"Install the transform","text":"<p>The transform can be installed directly from pypi and has a dependency on the data-prep-toolkit and the data-prep-connector</p> <p>Set up the local environment to run Jupyter notebook: <pre><code>python -v venv venv\nsource venv/bin/activate\npip install jupyter lab\n</code></pre> Install pre-requisites:</p> <pre><code>pip install data-prep-connector\npip install data-prep-toolkit&gt;=0.2.2.dev2\npip install 'data-prep-toolkit-transforms[web2parquet]&gt;=0.2.2.dev3'\n</code></pre> <p>If working from a fork in the git repo, from the root folder of the git repo, do the following:</p> <pre><code>cd transforms/universal/web2parquet\nmake venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"transforms/universal/web2parquet/#invoking-the-transform-from-a-notebook","title":"Invoking the transform from a notebook","text":"<p>In order to invoke the transfrom from a notebook, users must enable nested asynchronous ( https://pypi.org/project/nest-asyncio/ ), import the transform class and call the <code>transform()</code>function as shown in the example below:</p> <p>``` import nest_asyncio nest_asyncio.apply() from dpk_web2parquet.transform import Web2Parquet Web2Parquet(urls= ['https://thealliance.ai/'],                     depth=2,                      downloads=10,                     folder='downloads').transform() ````</p>"}]}