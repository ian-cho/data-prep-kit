{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate Data-Prep-kit transforms as LangChain tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is based on Data Prep Kit Demo\n",
    "link: https://github.com/IBM/data-prep-kit/blob/dev/examples/notebooks/intro/dpk_intro_1_ray.ipynb\n",
    "\n",
    "![](https://raw.githubusercontent.com/IBM/data-prep-kit/dev/examples/notebooks/intro/images/data-prep-kit-3-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies. This can take some time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ibm-cos-sdk-core 2.13.6 requires requests<2.32.3,>=2.32.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq -r dpk-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the input task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True to execute the transforms on the local Ray cluster; otherwise, the Python implementation is used.\n",
    "run_with_local_ray=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_text=\"\"\n",
    "if run_with_local_ray:\n",
    "    ray_text=\"on a local ray cluster \"\n",
    "\n",
    "task=f\"Execute pdf2parquet, doc_chunk, doc_id, ededup, text_encoder transforms {ray_text} one after the other where the input to a transform is the output of the previous transform run.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set input/output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleared /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output directory\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "output_base_path = f\"{cwd}/output\"\n",
    "\n",
    "input_folder = f\"{cwd}/input/test-input/\"\n",
    "output_folder =  f\"{output_base_path}/final_1/\"\n",
    "\n",
    "shutil.rmtree(output_base_path, ignore_errors=True)\n",
    "print (f\"‚úÖ Cleared {output_base_path} directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set transforms parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def prepare_params(params: dict):\n",
    "    params_json=json.dumps(params)\n",
    "    # trim clurly braces\n",
    "    return params_json[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "pdf2parquet_params_dict={\"data_files_to_use\": \"['.pdf']\", \"input_folder\":input_folder,  \"pdf2parquet_contents_type\": \"application/json\"}\n",
    "doc_chunk_params_dict={}\n",
    "doc_id_params_dict={\"doc_id_hash_column\": \"chunk_hash\", \"doc_id_int_column\": \"chunk_id\"}\n",
    "ededup_params_dict={\"ededup_doc_column\": \"contents\", \"ededup_doc_id_column\": \"chunk_hash\"}\n",
    "text_encoder_params_dict={\"text_encoder_model_name\": \"sentence-transformers/all-MiniLM-L6-v2\", \"output_folder\":output_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_with_local_ray:\n",
    "    worker_options_str=ParamsUtils.convert_to_ast({\"num_cpus\" : 0.8, \"memory\": 2 * GB})\n",
    "    ededup_params_dict=ededup_params_dict|{\"ededup_hash_cpu\": 0.5, \n",
    "                    \"ededup_num_hashes\": 2,\n",
    "                    \"runtime_worker_options\": worker_options_str,\n",
    "                    \"runtime_num_workers\": 2}\n",
    "    \n",
    "pdf2parquet_params=prepare_params(pdf2parquet_params_dict)\n",
    "doc_chunk_params=prepare_params(doc_chunk_params_dict)\n",
    "doc_id_params=prepare_params(doc_id_params_dict)\n",
    "ededup_params=prepare_params(ededup_params_dict)\n",
    "text_encoder_params=prepare_params(text_encoder_params_dict)\n",
    "\n",
    "params=f\"for pdf2parquet params use {pdf2parquet_params}. for doc_id use params {doc_id_params}. for ededup use params {ededup_params}. for text_encoder use params {text_encoder_params}\"\n",
    "input=f\"{task} {params}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print input task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><span style='color:blue; font-weight:bold; font-size:14.0pt;'>TASK: Execute pdf2parquet, doc_chunk, doc_id, ededup, text_encoder transforms on a local ray cluster  one after the other where the input to a transform is the output of the previous transform run.</span></p><p><span style='white-space: pre-wrap;color:green; ;font-size:10pt;'>PDF2PARQUET Params: \"data_files_to_use\": \"['.pdf']\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/input/test-input/\", \"pdf2parquet_contents_type\": \"application/json\"</span></p> <p><span style='white-space: pre-wrap;color:green; ;font-size:10pt;'>DOC CHUNKS Params: </span></p> <p><span style='white-space: pre-wrap;color:green; ;font-size:10pt;'>DOC_ID Params: \"doc_id_hash_column\": \"chunk_hash\", \"doc_id_int_column\": \"chunk_id\"</span> </p><p><span style='color:green; ;font-size:10pt;'>EDEDUP Params: \"ededup_doc_column\": \"contents\", \"ededup_doc_id_column\": \"chunk_hash\", \"ededup_hash_cpu\": 0.5, \"ededup_num_hashes\": 2, \"runtime_worker_options\": \"{'num_cpus': 0.8, 'memory': 2147483648}\", \"runtime_num_workers\": 2</span></p> <p><span style='color:green; ;font-size:10pt;'>TEXT_ENCODER Params: \"text_encoder_model_name\": \"sentence-transformers/all-MiniLM-L6-v2\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\"</span></p> </span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "print_task=f\"<p><span style='color:blue; font-weight:bold; font-size:14.0pt;'>TASK: {task}</span></p>\"\n",
    "print_pdf2parquet=f\"<p><span style='white-space: pre-wrap;color:green; ;font-size:10pt;'>PDF2PARQUET Params: {pdf2parquet_params}</span></p> \"\n",
    "print_doc_chunks=f\"<p><span style='white-space: pre-wrap;color:green; ;font-size:10pt;'>DOC CHUNKS Params: {doc_chunk_params}</span></p> \"\n",
    "print_doc_id_params=f\"<p><span style='white-space: pre-wrap;color:green; ;font-size:10pt;'>DOC_ID Params: {doc_id_params}</span> </p>\"\n",
    "print_ededup_params=f\"<p><span style='color:green; ;font-size:10pt;'>EDEDUP Params: {ededup_params}</span></p> \"\n",
    "print_text_encoder_params=f\"<p><span style='color:green; ;font-size:10pt;'>TEXT_ENCODER Params: {text_encoder_params}</span></p> \"\n",
    "\n",
    "HTML(f\"{print_task}{print_pdf2parquet}{print_doc_chunks}{print_doc_id_params}{print_ededup_params}{print_text_encoder_params}</span>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and tools¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "config = dotenv_values(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from llm_utils.logging import prep_loggers\n",
    "\n",
    "os.environ[\"LLM_LOG_PATH\"] = \"./logs/llm_log.txt\"\n",
    "os.environ[\"TOOL_CALLING_LOG_PATH\"] = \"./logs/tool_log.txt\"\n",
    "prep_loggers(\"llm=INFO,tool_calling=INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils.models import getChatLLM\n",
    "\n",
    "model_id1 = \"ibm/granite-3-8b-instruct\"\n",
    "model_id2 = \"meta-llama/llama-3-3-70b-instruct\"\n",
    "model_id3 = \"mistralai/mixtral-8x7b-instruct-v01\"\n",
    "llm = getChatLLM(\"watsonx\", model_id2, config);\n",
    "#model_ollama = \"llama3.1:70b\"\n",
    "#llm = getChatLLM(\"ollama\", model_ollama);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List DPK transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DPK tools: --\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FdedupTransform(),\n",
       " EdedupTransform(),\n",
       " FilterTransform(),\n",
       " ResizeTransform(),\n",
       " TokenizationTransform(),\n",
       " DocIDTransform(),\n",
       " Pdf2parquetTransform(),\n",
       " CodeQualityTransform(),\n",
       " ProgLangSelectTransform(),\n",
       " DocChunkTransform(),\n",
       " DocQualityTransform(),\n",
       " Code2ParquetTransform(),\n",
       " LangIdentificationTransform(),\n",
       " TextEncoderTransform(),\n",
       " PIIRedactorTransform()]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_utils.dpk.langchain_tools.agent_toolkit.toolkit import DataPrepKitToolkit\n",
    "\n",
    "toolkit = DataPrepKitToolkit()  \n",
    "tools = toolkit.get_tools()\n",
    "print(\"-- DPK tools: --\")\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from typing import Union, List\n",
    "\n",
    "def find_tool_by_name(tools: List[Tool], tool_name: str) -> Tool:\n",
    "    for tool in tools:\n",
    "        if tool.name == tool_name:\n",
    "            return tool\n",
    "    raise ValueError(f\"Tool with name {tool_name} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Print tools descriptions --\n",
      "fdedup - Apply Fdedup transform on files in input folder\n",
      "ededup - Apply Ededup transform on files in input folder\n",
      "filter - Apply filter transform on files in input folder\n",
      "resize - Apply resize transform on files in input folder\n",
      "tokenization - Apply Tokenization transform on files in input folder\n",
      "doc_id - Apply doc_id transform on files in input folder\n",
      "pdf2parquet - Apply pdf2parquet transform on files in input folder\n",
      "code_quality - Apply code_quality transform on files in input folder\n",
      "proglang_select - Apply proglang_select transform on files in input folder\n",
      "doc_chunk - Apply DocChunk transform on files in input folder\n",
      "doc_quality - Apply DocQuality transform on files in input folder\n",
      "code2parquet - Apply code2parquet transform on files in input folder\n",
      "lang_id - Apply LangIdentification transform on files in input folder\n",
      "text_encoder - Apply text_encoder transform on files in input folder\n",
      "pii_redactor - Apply PIIRedactor transform on files in input folder\n",
      "('fdedup, ededup, filter, resize, tokenization, doc_id, pdf2parquet, code_quality, proglang_select, doc_chunk, doc_quality, code2parquet, lang_id, text_encoder, pii_redactor',)\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools.render import render_text_description\n",
    "\n",
    "tools_str=render_text_description(tools)\n",
    "print(\"-- Print tools descriptions --\")\n",
    "print(tools_str)\n",
    "tool_names=\", \".join([t.name for t in tools]),\n",
    "print(tool_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools.render import render_text_description\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template( \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "    {tools}\n",
    "    \n",
    "    Use the following format:\n",
    "    \n",
    "    Question: the input question you must answer\n",
    "    Thought: you should always think about what to do\n",
    "    Action: the action to take, should be one of [{tool_names}]\n",
    "    Action Input: the input to the action\n",
    "    Observation: the result of the action\n",
    "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "    Thought: I now know the final answer\n",
    "    Final Answer: the final answer to the original input question\n",
    "\n",
    "    Final Answer or Action should appear in the answer but not both.\n",
    "    Follow the exact Action Input format provided in the examples when crafting your response.\n",
    "\n",
    "    Here's an example.\n",
    "\n",
    "    For example, If the required task was to execute ededup , doc_id transforms one after the other. \n",
    "    The output directory of a transform is the input for the next transform in the transform order. \n",
    "    for ededup params use: 'input_folder':'/home/user/input/ededup'\n",
    "    for doc_id params use : 'output_folder':'/home/user/output/final'. \n",
    "    The output should be the following:\n",
    "      \n",
    "    Thought: I need to execute the ededup and doc_id one after the other.\n",
    "    \n",
    "    Action: ededup\n",
    "    Action Input: \"input_folder\":\"/home/user/input/ededup\", \"output_folder\":\"/home/user/output/ededup\"\n",
    "    Observation: The output of the ededup transform is stored in \"/home/user/output/ededup\".\n",
    "\n",
    "    Action: doc_id\n",
    "    Action Input: \"input_folder\":\"/home/user/output/ededup\", \"output_folder\":\"/home/user/output/final\"\n",
    "    Observation: The output of the doc_id transform is stored in \"/home/eres/output/final\".\n",
    "\n",
    "    Here's another example: \n",
    "\n",
    "    If the required task was to execute ededup , doc_id transforms on a local ray cluster one after the other. \n",
    "    The output directory of a transform is the input for the next transform in the transform order. \n",
    "    for ededup params use: 'input_folder':'/home/user/input/ededup'\n",
    "    for doc_id params use : 'output_folder':'/home/user/output/final'\n",
    "    The output should be the following:\n",
    "      \n",
    "    Thought: I need to execute the ededup and doc_id one after the other.\n",
    "    \n",
    "    Action: ededup\n",
    "    Action Input: \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\":\"/home/user/input/ededup\", \"output_folder\":\"/home/user/output/ededup\"\n",
    "    Observation: The output of the ededup transform is stored in \"/home/user/output/ededup\".\n",
    "\n",
    "    Action: doc_id\n",
    "    Action Input: \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\":\"/home/user/output/ededup\", \"output_folder\":\"/home/user/output/final\"\n",
    "    Observation: The output of the doc_id transform is stored in \"/home/user/output/final\".\n",
    "\n",
    "    \n",
    "    Begin!\n",
    "    \n",
    "    Question: {input}\n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the agent to create the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute pdf2parquet, doc_chunk, doc_id, ededup, text_encoder transforms on a local ray cluster  one after the other where the input to a transform is the output of the previous transform run. for pdf2parquet params use \"data_files_to_use\": \"['.pdf']\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/input/test-input/\", \"pdf2parquet_contents_type\": \"application/json\". for doc_id use params \"doc_id_hash_column\": \"chunk_hash\", \"doc_id_int_column\": \"chunk_id\". for ededup use params \"ededup_doc_column\": \"contents\", \"ededup_doc_id_column\": \"chunk_hash\", \"ededup_hash_cpu\": 0.5, \"ededup_num_hashes\": 2, \"runtime_worker_options\": \"{'num_cpus': 0.8, 'memory': 2147483648}\", \"runtime_num_workers\": 2. for text_encoder use params \"text_encoder_model_name\": \"sentence-transformers/all-MiniLM-L6-v2\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\"\n",
      "1. Thought: I need to execute the pdf2parquet, doc_chunk, doc_id, ededup, text_encoder transforms one after the other on a local ray cluster.\n",
      "    \n",
      "    Action: pdf2parquet\n",
      "    Action Input: \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"data_files_to_use\": \"['.pdf']\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/input/test-input/\", \"pdf2parquet_contents_type\": \"application/json\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet\"\n",
      "    Observation: The output of the pdf2parquet transform is stored in \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workwork/output/pdf2parquet\".\n",
      "\n",
      "    2. Thought: Now that I have the output of the pdf2parquet transform, I can use it as the input for the doc_chunk transform.\n",
      "\n",
      "    Action: doc_chunk\n",
      "    Action Input: \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk\"\n",
      "    Observation: The output of the doc_chunk transform is stored in \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk\".\n",
      "\n",
      "    3. Thought: Now that I have the output of the doc_chunk transform, I can use it as the input for the doc_id transform.\n",
      "\n",
      "    Action: doc_id\n",
      "    Action Input: \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk\", \"doc_id_hash_column\": \"chunk_hash\", \"doc_id_int_column\": \"chunk_id\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id\"\n",
      "    Observation: The output of the doc_id transform is stored in \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id\".\n",
      "\n",
      "    4. Thought: Now that I have the output of the doc_id transform, I can use it as the input for the ededup transform.\n",
      "\n",
      "    Action: ededup\n",
      "    Action Input: \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id\", \"ededup_doc_column\": \"contents\", \"ededup_doc_id_column\": \"chunk_hash\", \"ededup_hash_cpu\": 0.5, \"ededup_num_hashes\": 2, \"runtime_worker_options\": \"{'num_cpus': 0.8, 'memory': 2147483648}\", \"runtime_num_workers\": 2, \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup\"\n",
      "    Observation: The output of the ededup transform is stored in \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup\".\n",
      "\n",
      "    5. Thought: Now that I have the output of the ededup transform, I can use it as the input for the text_encoder transform.\n",
      "\n",
      "    Action: text_encoder\n",
      "    Action Input: \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup\", \"text_encoder_model_name\": \"sentence-transformers/all-MiniLM-L6-v2\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\"\n",
      "    Observation: The output of the text_encoder transform is stored in \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\".\n",
      "\n",
      "    Thought: I now know the final answer\n",
      "    Final Answer: The output of the text_encoder transform is stored in \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\".\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "print(input)\n",
    "\n",
    "agent = prompt_template | llm \n",
    "\n",
    "agent_step = \"\"\n",
    "agent_step = agent.invoke(\n",
    "            {\n",
    "                \"input\": input,\n",
    "                \"tool_names\": tool_names,\n",
    "                \"tools\": tools_str,\n",
    "            }\n",
    "        )\n",
    "   \n",
    "print(agent_step.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM result contain the following transforms:\n",
      "\n",
      "TRANSFORM NAME pdf2parquet\n",
      "TRANSFORM PARAMS \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"data_files_to_use\": \"['.pdf']\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/input/test-input/\", \"pdf2parquet_contents_type\": \"application/json\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet\"\n",
      "--------------------------------------\n",
      "TRANSFORM NAME doc_chunk\n",
      "TRANSFORM PARAMS \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk\"\n",
      "--------------------------------------\n",
      "TRANSFORM NAME doc_id\n",
      "TRANSFORM PARAMS \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk\", \"doc_id_hash_column\": \"chunk_hash\", \"doc_id_int_column\": \"chunk_id\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id\"\n",
      "--------------------------------------\n",
      "TRANSFORM NAME ededup\n",
      "TRANSFORM PARAMS \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id\", \"ededup_doc_column\": \"contents\", \"ededup_doc_id_column\": \"chunk_hash\", \"ededup_hash_cpu\": 0.5, \"ededup_num_hashes\": 2, \"runtime_worker_options\": \"{'num_cpus': 0.8, 'memory': 2147483648}\", \"runtime_num_workers\": 2, \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup\"\n",
      "--------------------------------------\n",
      "TRANSFORM NAME text_encoder\n",
      "TRANSFORM PARAMS \"runtime_type\": \"ray\", \"run_locally\": \"True\", \"input_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup\", \"text_encoder_model_name\": \"sentence-transformers/all-MiniLM-L6-v2\", \"output_folder\": \"/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\"\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex = (r\"Action\\s*\\d*\\s*:[\\s]*(.*?)[\\s]*Action\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\")\n",
    "matches = re.findall(regex, agent_step.content)\n",
    "\n",
    "print(\"LLM result contain the following transforms:\\n\")\n",
    "for match in matches:\n",
    "    print(f\"TRANSFORM NAME {match[0]}\")\n",
    "    print(f\"TRANSFORM PARAMS {match[1]}\")\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "def load_from_json(js: str) -> dict[str, Any]:\n",
    "        try:\n",
    "            return json.loads(js)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load parameters {js} with error {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the transfoms by calling their tool definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tool(match) -> str:\n",
    "    def contains_parquet_files(dir_path):\n",
    "      return any(file.endswith(\".parquet\") for file in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, file)))\n",
    "\n",
    "    tool_name = match[0]\n",
    "    tool_to_use = find_tool_by_name(tools, tool_name)\n",
    "    tool_name = match[0]\n",
    "    tool_input=\"{\"+match[1]+\"}\"\n",
    "    tool_input_dict = load_from_json(tool_input)\n",
    "    print(\"=======================================================\")\n",
    "    print (f\"üèÉüèº RUNNING {tool_name} with params: {tool_input_dict}\")\n",
    "    print(\"=======================================================\")\n",
    "    tool_result  = tool_to_use.run(tool_input_dict)\n",
    "    if not contains_parquet_files(tool_input_dict[\"output_folder\"]):\n",
    "        out_dir=tool_input_dict[\"output_folder\"]\n",
    "        raise Exception (f\"The {out_dir} directory is unexpectedly empty, indicating the job failed.\")\n",
    "    print (f\"‚úÖ {tool_result}\")\n",
    "    \n",
    "    return tool_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "üèÉüèº RUNNING pdf2parquet with params: {'runtime_type': 'ray', 'run_locally': 'True', 'data_files_to_use': \"['.pdf']\", 'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/input/test-input/', 'pdf2parquet_contents_type': 'application/json', 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet'}\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:54:44 INFO - pdf2parquet parameters are : {'batch_size': -1, 'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': True, 'ocr_engine': <pdf2parquet_ocr_engine.EASYOCR: 'easyocr'>, 'bitmap_area_threshold': 0.05, 'pdf_backend': <pdf2parquet_pdf_backend.DLPARSE_V2: 'dlparse_v2'>, 'double_precision': 8}\n",
      "INFO:dpk_pdf2parquet.transform:pdf2parquet parameters are : {'batch_size': -1, 'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': True, 'ocr_engine': <pdf2parquet_ocr_engine.EASYOCR: 'easyocr'>, 'bitmap_area_threshold': 0.05, 'pdf_backend': <pdf2parquet_pdf_backend.DLPARSE_V2: 'dlparse_v2'>, 'double_precision': 8}\n",
      "11:54:44 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "11:54:44 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "11:54:44 INFO - number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "11:54:44 INFO - actor creation delay 0\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:actor creation delay 0\n",
      "11:54:44 INFO - job details {'job category': 'preprocessing', 'job name': 'pdf2parquet', 'job type': 'ray', 'job id': 'job_id'}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:job details {'job category': 'preprocessing', 'job name': 'pdf2parquet', 'job type': 'ray', 'job id': 'job_id'}\n",
      "11:54:44 INFO - data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/input/test-input/ output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/input/test-input/ output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet\n",
      "11:54:44 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ max_files -1, n_sample -1\n",
      "11:54:44 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "11:54:44 INFO - Running locally\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Running locally\n",
      "2025-01-16 11:54:48,573\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=73711)\u001b[0m 11:54:52 INFO - orchestrator started at 2025-01-16 11:54:52\n",
      "\u001b[36m(orchestrate pid=73711)\u001b[0m 11:54:52 INFO - Number of files is 1, source profile {'max_file_size': 0.055823326110839844, 'min_file_size': 0.055823326110839844, 'total_file_size': 0.055823326110839844}\n",
      "\u001b[36m(orchestrate pid=73711)\u001b[0m 11:54:52 INFO - Cluster resources: {'cpus': 10, 'gpus': 0, 'memory': 34.536434937268496, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=73711)\u001b[0m 11:54:52 INFO - Number of workers - 1 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(RayTransformFileProcessor pid=73760)\u001b[0m 11:54:55 INFO - Initializing models\n",
      "Fetching 9 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 94846.07it/s]\n",
      "\u001b[36m(orchestrate pid=73711)\u001b[0m 11:55:38 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=73711)\u001b[0m 11:55:38 INFO - Completed processing 1 files in 0.016 min\n",
      "\u001b[36m(orchestrate pid=73711)\u001b[0m 11:55:38 INFO - done flushing in 0.001 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=73760)\u001b[0m /Users/alexey/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTransformFileProcessor pid=73760)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "11:55:48 INFO - Completed execution in 1.08 min, execution result 0\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Completed execution in 1.08 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ pdf2parquet transform successfully applied with input_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/input/test-input/ output_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet.\n",
      "=======================================================\n",
      "üèÉüèº RUNNING doc_chunk with params: {'runtime_type': 'ray', 'run_locally': 'True', 'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet', 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk'}\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:56:07 INFO - doc_chunk parameters are : {'chunking_type': <chunking_types.DL_JSON: 'dl_json'>, 'content_column_name': 'contents', 'doc_id_column_name': 'document_id', 'output_chunk_column_name': 'contents', 'output_source_doc_id_column_name': 'source_document_id', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox', 'chunk_size_tokens': 128, 'chunk_overlap_tokens': 30, 'dl_min_chunk_len': None}\n",
      "INFO:dpk_doc_chunk.transformcfg:doc_chunk parameters are : {'chunking_type': <chunking_types.DL_JSON: 'dl_json'>, 'content_column_name': 'contents', 'doc_id_column_name': 'document_id', 'output_chunk_column_name': 'contents', 'output_source_doc_id_column_name': 'source_document_id', 'output_jsonpath_column_name': 'doc_jsonpath', 'output_pageno_column_name': 'page_number', 'output_bbox_column_name': 'bbox', 'chunk_size_tokens': 128, 'chunk_overlap_tokens': 30, 'dl_min_chunk_len': None}\n",
      "11:56:07 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "11:56:07 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "11:56:07 INFO - number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "11:56:07 INFO - actor creation delay 0\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:actor creation delay 0\n",
      "11:56:07 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_chunk', 'job type': 'ray', 'job id': 'job_id'}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:job details {'job category': 'preprocessing', 'job name': 'doc_chunk', 'job type': 'ray', 'job id': 'job_id'}\n",
      "11:56:07 INFO - data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk\n",
      "11:56:07 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ max_files -1, n_sample -1\n",
      "11:56:07 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "11:56:07 INFO - Running locally\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Running locally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "launching transform with params: {'run_locally': 'True', 'data_local_config': \"{'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet', 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk'}\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:56:08,921\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=74013)\u001b[0m 11:56:11 INFO - orchestrator started at 2025-01-16 11:56:11\n",
      "\u001b[36m(orchestrate pid=74013)\u001b[0m 11:56:11 INFO - Number of files is 1, source profile {'max_file_size': 0.00682830810546875, 'min_file_size': 0.00682830810546875, 'total_file_size': 0.00682830810546875}\n",
      "\u001b[36m(orchestrate pid=74013)\u001b[0m 11:56:11 INFO - Cluster resources: {'cpus': 10, 'gpus': 0, 'memory': 36.21767272986472, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=74013)\u001b[0m 11:56:11 INFO - Number of workers - 1 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=74013)\u001b[0m 11:56:13 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=74013)\u001b[0m 11:56:14 INFO - Completed processing 1 files in 0.021 min\n",
      "\u001b[36m(orchestrate pid=74013)\u001b[0m 11:56:14 INFO - done flushing in 0.001 sec\n",
      "11:56:24 INFO - Completed execution in 0.281 min, execution result 0\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Completed execution in 0.281 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ doc_chunk transform successfully applied with input_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/pdf2parquet output_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:56:36 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'chunk_hash', 'int_column': 'chunk_id', 'start_id': 0}\n",
      "INFO:dpk_doc_id.transform:Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'chunk_hash', 'int_column': 'chunk_id', 'start_id': 0}\n",
      "11:56:36 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "11:56:36 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "11:56:36 INFO - number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "11:56:36 INFO - actor creation delay 0\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:actor creation delay 0\n",
      "11:56:36 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "11:56:36 INFO - data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id\n",
      "11:56:36 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ max_files -1, n_sample -1\n",
      "11:56:36 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "11:56:36 INFO - Running locally\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Running locally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "üèÉüèº RUNNING doc_id with params: {'runtime_type': 'ray', 'run_locally': 'True', 'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk', 'doc_id_hash_column': 'chunk_hash', 'doc_id_int_column': 'chunk_id', 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id'}\n",
      "=======================================================\n",
      "launching transform with params: {'run_locally': 'True', 'data_local_config': \"{'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk', 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id'}\", 'doc_id_hash_column': 'chunk_hash', 'doc_id_int_column': 'chunk_id'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:56:37,163\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=74137)\u001b[0m 11:56:38 INFO - orchestrator started at 2025-01-16 11:56:38\n",
      "\u001b[36m(orchestrate pid=74137)\u001b[0m 11:56:38 INFO - Number of files is 1, source profile {'max_file_size': 0.008722305297851562, 'min_file_size': 0.008722305297851562, 'total_file_size': 0.008722305297851562}\n",
      "\u001b[36m(orchestrate pid=74137)\u001b[0m 11:56:38 INFO - Cluster resources: {'cpus': 10, 'gpus': 0, 'memory': 35.934898376464844, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=74137)\u001b[0m 11:56:38 INFO - Number of workers - 1 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=74137)\u001b[0m 11:56:39 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=74137)\u001b[0m 11:56:39 INFO - Completed processing 1 files in 0.003 min\n",
      "\u001b[36m(orchestrate pid=74137)\u001b[0m 11:56:39 INFO - done flushing in 0.001 sec\n",
      "11:56:49 INFO - Completed execution in 0.22 min, execution result 0\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Completed execution in 0.22 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ doc_id transform successfully applied with input_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_chunk output_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:57:00 INFO - exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'chunk_hash', 'use_snapshot': False, 'snapshot_directory': None, 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "INFO:dpk_ededup.transform_base:exact dedup params are {'doc_column': 'contents', 'doc_id_column': 'chunk_hash', 'use_snapshot': False, 'snapshot_directory': None, 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "11:57:00 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "11:57:00 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "11:57:00 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'memory': 2147483648, 'max_restarts': -1}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:number of workers 2 worker options {'num_cpus': 0.8, 'memory': 2147483648, 'max_restarts': -1}\n",
      "11:57:00 INFO - actor creation delay 0\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:actor creation delay 0\n",
      "11:57:00 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "11:57:00 INFO - data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup\n",
      "11:57:00 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ max_files -1, n_sample -1\n",
      "11:57:00 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "11:57:00 INFO - Running locally\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Running locally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================\n",
      "üèÉüèº RUNNING ededup with params: {'runtime_type': 'ray', 'run_locally': 'True', 'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id', 'ededup_doc_column': 'contents', 'ededup_doc_id_column': 'chunk_hash', 'ededup_hash_cpu': 0.5, 'ededup_num_hashes': 2, 'runtime_worker_options': \"{'num_cpus': 0.8, 'memory': 2147483648}\", 'runtime_num_workers': 2, 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup'}\n",
      "=======================================================\n",
      "Warning: unexpected parameter provided for the transform\n",
      "dict_keys(['runtime_type', 'run_locally', 'runtime_num_workers', 'runtime_worker_options', 'ededup_doc_column', 'ededup_doc_id_column', 'ededup_num_hashes', 'ededup_hash_cpu'])\n",
      "launching transform with params: {'run_locally': 'True', 'runtime_num_workers': 2, 'runtime_worker_options': \"{'num_cpus': 0.8, 'memory': 2147483648}\", 'data_local_config': \"{'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id', 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup'}\", 'ededup_doc_column': 'contents', 'ededup_doc_id_column': 'chunk_hash', 'ededup_num_hashes': 2, 'ededup_hash_cpu': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:57:01,857\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=74254)\u001b[0m 11:57:02 INFO - orchestrator started at 2025-01-16 11:57:02\n",
      "\u001b[36m(orchestrate pid=74254)\u001b[0m 11:57:02 INFO - Number of files is 1, source profile {'max_file_size': 0.010000228881835938, 'min_file_size': 0.010000228881835938, 'total_file_size': 0.010000228881835938}\n",
      "\u001b[36m(orchestrate pid=74254)\u001b[0m 11:57:02 INFO - Cluster resources: {'cpus': 10, 'gpus': 0, 'memory': 35.87061462458223, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=74254)\u001b[0m 11:57:02 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'memory': 2147483648, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=74254)\u001b[0m 11:57:03 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=74254)\u001b[0m 11:57:04 INFO - Completed processing 1 files in 0.003 min\n",
      "\u001b[36m(orchestrate pid=74254)\u001b[0m 11:57:04 INFO - done flushing in 0.001 sec\n",
      "11:57:14 INFO - Completed execution in 0.219 min, execution result 0\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Completed execution in 0.219 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ededup transform successfully applied with input_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/doc_id output_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup.\n",
      "=======================================================\n",
      "üèÉüèº RUNNING text_encoder with params: {'runtime_type': 'ray', 'run_locally': 'True', 'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup', 'text_encoder_model_name': 'sentence-transformers/all-MiniLM-L6-v2', 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/'}\n",
      "=======================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:57:26 INFO - text_encoder parameters are : {'content_column_name': 'contents', 'output_embeddings_column_name': 'embeddings', 'model_name': 'sentence-transformers/all-MiniLM-L6-v2'}\n",
      "INFO:dpk_text_encoder.transformcfg:text_encoder parameters are : {'content_column_name': 'contents', 'output_embeddings_column_name': 'embeddings', 'model_name': 'sentence-transformers/all-MiniLM-L6-v2'}\n",
      "11:57:26 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "11:57:26 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "11:57:26 INFO - number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "11:57:26 INFO - actor creation delay 0\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:actor creation delay 0\n",
      "11:57:26 INFO - job details {'job category': 'preprocessing', 'job name': 'text_encoder', 'job type': 'ray', 'job id': 'job_id'}\n",
      "INFO:data_processing_ray.runtime.ray.execution_configuration:job details {'job category': 'preprocessing', 'job name': 'text_encoder', 'job type': 'ray', 'job id': 'job_id'}\n",
      "11:57:26 INFO - data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ is using local data access: input_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup output_folder - /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\n",
      "11:57:26 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ max_files -1, n_sample -1\n",
      "11:57:26 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_based155f7e5-6241-4f6f-9dcf-ba3cc16d61ab:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "11:57:26 INFO - Running locally\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Running locally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "launching transform with params: {'run_locally': 'True', 'data_local_config': \"{'input_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup', 'output_folder': '/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/'}\", 'text_encoder_model_name': 'sentence-transformers/all-MiniLM-L6-v2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 11:57:27,722\tINFO worker.py:1777 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=74345)\u001b[0m 11:57:31 INFO - orchestrator started at 2025-01-16 11:57:31\n",
      "\u001b[36m(orchestrate pid=74345)\u001b[0m 11:57:31 INFO - Number of files is 1, source profile {'max_file_size': 0.010349273681640625, 'min_file_size': 0.010349273681640625, 'total_file_size': 0.010349273681640625}\n",
      "\u001b[36m(orchestrate pid=74345)\u001b[0m 11:57:31 INFO - Cluster resources: {'cpus': 10, 'gpus': 0, 'memory': 35.83247833326459, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=74345)\u001b[0m 11:57:31 INFO - Number of workers - 1 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=74345)\u001b[0m 11:57:36 INFO - Completed 0 files (0.0%)  in 0.0 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=74345)\u001b[0m 11:57:37 INFO - Completed processing 1 files in 0.01 min\n",
      "\u001b[36m(orchestrate pid=74345)\u001b[0m 11:57:37 INFO - done flushing in 0.001 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=74358)\u001b[0m /Users/alexey/.pyenv/versions/3.12.8/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTransformFileProcessor pid=74358)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "11:57:47 INFO - Completed execution in 0.343 min, execution result 0\n",
      "INFO:data_processing_ray.runtime.ray.transform_launcher:Completed execution in 0.343 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ text_encoder transform successfully applied with input_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/ededup output_folder /Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/.\n",
      "=================================================\n",
      "‚úÖ Transforms execution completed successfully\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "error=False\n",
    "for match in matches:\n",
    "    try:\n",
    "        tool_result = run_tool(match)\n",
    "        time.sleep(10)\n",
    "    except Exception as e:\n",
    "            error=True\n",
    "            print(f\"‚ùå Error: \" + str(e))\n",
    "            break\n",
    "\n",
    "if not error:\n",
    "    print(\"=================================================\")\n",
    "    print (f\"‚úÖ Transforms execution completed successfully\")\n",
    "    print(\"=================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Generated Output File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see a column called embeddings added at the end. This the text content converted into vectors or embeddings. \n",
    "We used the model sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-16 11:57:59--  https://raw.githubusercontent.com/IBM/data-prep-kit/dev/examples/notebooks/intro/my_utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8002::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "...◊î◊ë◊ï◊©◊™ ◊™◊ú◊ë◊ß◊ú ◊ü◊ô◊™◊û◊û ,◊î◊ó◊ú◊©◊† HTTP ◊™◊ô◊ô◊†◊§ 200 OK\n",
      "Length: 1856 (1.8K) [text/plain]\n",
      "Saving to: ‚Äòmy_utils.py‚Äô\n",
      "\n",
      "my_utils.py         100%[===================>]   1.81K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2025-01-16 11:57:59 (2.00 MB/s) - ‚Äòmy_utils.py‚Äô saved [1856/1856]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "!wget -O 'my_utils.py'  'https://raw.githubusercontent.com/IBM/data-prep-kit/dev/examples/notebooks/intro/my_utils.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alexey/goWork/src/github.com/data-preprocessing/data-prep-lab/examples/agentic-workflow/output/final_1/\n",
      "Output data dimensions (rows x columns)=  (6, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>num_tables</th>\n",
       "      <th>num_doc_elements</th>\n",
       "      <th>document_hash</th>\n",
       "      <th>ext</th>\n",
       "      <th>hash</th>\n",
       "      <th>size</th>\n",
       "      <th>date_acquired</th>\n",
       "      <th>pdf_convert_time</th>\n",
       "      <th>...</th>\n",
       "      <th>source_document_id</th>\n",
       "      <th>contents</th>\n",
       "      <th>doc_jsonpath</th>\n",
       "      <th>page_number</th>\n",
       "      <th>bbox</th>\n",
       "      <th>document_id</th>\n",
       "      <th>chunk_hash</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>removed</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>earth.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>17915699055171962696</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fc2c603509e668f99cfeede83e0137fe7f711515765dc7...</td>\n",
       "      <td>4994</td>\n",
       "      <td>2025-01-16T11:55:38.984640</td>\n",
       "      <td>0.960712</td>\n",
       "      <td>...</td>\n",
       "      <td>fd2f14b5-8513-42ab-ab4b-e2f348af250f</td>\n",
       "      <td>Our solar system is a vast and fascinating exp...</td>\n",
       "      <td>#/texts/2</td>\n",
       "      <td>1</td>\n",
       "      <td>[132.66474915, 588.23297119, 479.40899658, 626...</td>\n",
       "      <td>4d2e06dbe1d7a07f3722947cb6e7f9e2487d4a536c73dc...</td>\n",
       "      <td>4d2e06dbe1d7a07f3722947cb6e7f9e2487d4a536c73dc...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.002112108, -0.010703988, 0.011550161, 0.020...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earth.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>17915699055171962696</td>\n",
       "      <td>pdf</td>\n",
       "      <td>fc2c603509e668f99cfeede83e0137fe7f711515765dc7...</td>\n",
       "      <td>4994</td>\n",
       "      <td>2025-01-16T11:55:38.984640</td>\n",
       "      <td>0.960712</td>\n",
       "      <td>...</td>\n",
       "      <td>fd2f14b5-8513-42ab-ab4b-e2f348af250f</td>\n",
       "      <td>For more details about our Solar system see Ch...</td>\n",
       "      <td>#/texts/3</td>\n",
       "      <td>1</td>\n",
       "      <td>[133.18649292, 570.30102539, 375.57901001, 584...</td>\n",
       "      <td>6b1799aadb8131a788033660ec73788c9f4165eda5e339...</td>\n",
       "      <td>6b1799aadb8131a788033660ec73788c9f4165eda5e339...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[-0.060079582, 0.014746683, 0.025538277, 0.048...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows √ó 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    filename  num_pages  num_tables  num_doc_elements         document_hash  \\\n",
       "0  earth.pdf          1           0                11  17915699055171962696   \n",
       "1  earth.pdf          1           0                11  17915699055171962696   \n",
       "\n",
       "   ext                                               hash  size  \\\n",
       "0  pdf  fc2c603509e668f99cfeede83e0137fe7f711515765dc7...  4994   \n",
       "1  pdf  fc2c603509e668f99cfeede83e0137fe7f711515765dc7...  4994   \n",
       "\n",
       "                date_acquired  pdf_convert_time  ...  \\\n",
       "0  2025-01-16T11:55:38.984640          0.960712  ...   \n",
       "1  2025-01-16T11:55:38.984640          0.960712  ...   \n",
       "\n",
       "                     source_document_id  \\\n",
       "0  fd2f14b5-8513-42ab-ab4b-e2f348af250f   \n",
       "1  fd2f14b5-8513-42ab-ab4b-e2f348af250f   \n",
       "\n",
       "                                            contents doc_jsonpath page_number  \\\n",
       "0  Our solar system is a vast and fascinating exp...    #/texts/2           1   \n",
       "1  For more details about our Solar system see Ch...    #/texts/3           1   \n",
       "\n",
       "                                                bbox  \\\n",
       "0  [132.66474915, 588.23297119, 479.40899658, 626...   \n",
       "1  [133.18649292, 570.30102539, 375.57901001, 584...   \n",
       "\n",
       "                                         document_id  \\\n",
       "0  4d2e06dbe1d7a07f3722947cb6e7f9e2487d4a536c73dc...   \n",
       "1  6b1799aadb8131a788033660ec73788c9f4165eda5e339...   \n",
       "\n",
       "                                          chunk_hash chunk_id  removed  \\\n",
       "0  4d2e06dbe1d7a07f3722947cb6e7f9e2487d4a536c73dc...        0       []   \n",
       "1  6b1799aadb8131a788033660ec73788c9f4165eda5e339...        1       []   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.002112108, -0.010703988, 0.011550161, 0.020...  \n",
       "1  [-0.060079582, 0.014746683, 0.025538277, 0.048...  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from my_utils import read_parquet_files_as_df\n",
    "\n",
    "# print the last transform output\n",
    "last_transform=matches[-1]\n",
    "tool_input=\"{\"+match[1]+\"}\"\n",
    "tool_input_dict = load_from_json(tool_input)\n",
    "dir=tool_input_dict[\"output_folder\"]\n",
    "print(dir)\n",
    "output_df = read_parquet_files_as_df(dir)\n",
    "\n",
    "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
    "\n",
    "output_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
