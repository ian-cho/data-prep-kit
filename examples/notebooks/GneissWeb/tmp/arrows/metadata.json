{
  "pipeline": "pipeline_id",
  "job details": {
    "job category": "preprocessing",
    "job name": "Tokenization2Arrow",
    "job type": "pure python",
    "job id": "job_id",
    "start_time": "2025-02-14 11:52:31",
    "end_time": "2025-02-14 11:52:31",
    "status": "success"
  },
  "code": null,
  "job_input_params": {
    "tokenizer": "bigcode/starcoder",
    "tokenizer_args": null,
    "doc_id_column": "id",
    "doc_content_column": "text",
    "text_lang": "en",
    "chunk_size": 20000,
    "checkpointing": false,
    "max_files": -1,
    "random_samples": -1,
    "files_to_use": [
      ".parquet"
    ],
    "num_processors": 0
  },
  "execution_stats": {
    "cpus": 62.0,
    "gpus": 0,
    "memory": 29.84,
    "object_store": 0,
    "execution time, min": 0.007
  },
  "job_output_stats": {
    "source_files": 1,
    "source_size": 36664,
    "result_files": 1,
    "result_size": 29914,
    "processing_time": 0.023,
    "num_files": 1,
    "num_rows": 15,
    "num_tokenized_rows": 10,
    "num_empty_rows": 5,
    "num_tokens": 6981,
    "num_chars": 27395,
    "result_doc_count": 0
  },
  "source": {
    "name": "/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty",
    "type": "path"
  },
  "target": {
    "name": "/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/arrows",
    "type": "path"
  }
}