{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40647db0-0509-4550-8b7b-bee114ca270e",
   "metadata": {},
   "source": [
    "# GneissWeb Recipe\n",
    "\n",
    "#### This notebook presents the GneissWeb recipe and applies the components in sequence to reproduce the GneissWeb processing pipeline using DPK transforms. \n",
    "#### ![](recipe3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed3dab-30af-41c6-a5c3-f665a7c49243",
   "metadata": {},
   "source": [
    "#### Pip installations\n",
    "\n",
    "##### These pip installs need to be adapted to use the appropriate release level. Alternatively, The venv running the jupyter lab could be pre-configured with a requirement file that includes the right release.\n",
    "\n",
    "##### Example for transform developers working from git clone:\n",
    "\n",
    "##### make venv\n",
    "\n",
    "##### source venv/bin/activate\n",
    "\n",
    "##### pip install jupyterlab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ab090-f34d-4707-8a16-f91bfd7e36c9",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Repetition Removal\n",
    "##### This component applies exact substring deduplication to remove any substring of predetermined length that repeats more than once within a single parquet file level by adapting the implementation from [deduplicate-text-datasets](https://github.com/google-research/deduplicate-text-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00652330-69bd-4652-81d1-5ffcca913742",
   "metadata": {},
   "source": [
    "\n",
    "#### Prerequisites\n",
    "\n",
    "##### To run the repetition removal transform, Rust is required to be installed on the machine. You can install rust following instructions [here](https://www.rust-lang.org/tools/install).\n",
    "\n",
    "##### Add Rust to $PATH\n",
    "\n",
    "##### If Rust is not added to your $PATH, run the below steps to add the rust installation location for proper execution.\n",
    "\n",
    "##### You can use the !whereis cargo command to find where rust is installed in your machine, and set the path there up to the /bin\n",
    "\n",
    "##### ex: whereis cargo produces: cargo: /Users/USERNAME/.cargo/bin/cargo\n",
    "\n",
    "##### set the $PATH to include /Users/USERNAME/.cargo/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a724d-6b68-41d7-87d1-4672475f42ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0fd19-dba5-4934-b510-c8c964b6ca00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c5ff67d-7ce9-4e7c-b283-bb75739ea876",
   "metadata": {},
   "source": [
    "### 2. Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1a2b7-bc06-4dba-ae7c-764444f79da4",
   "metadata": {},
   "source": [
    "### 2.1. Fasttext Quality Annotator\n",
    "##### This step annotates the documents using two FastText quality classifiers: (i) the fastText classifier from [DCLM](https://arxiv.org/pdf/2406.11794) and (ii) our own fastText classifier trained on a mix of high-quality synthetic data and data annotated by an LLM for high educational value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12962e6-44b4-45b8-9c63-0a33a4e5c0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ef14a-6970-4eb1-a6ac-b2f92259d894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e29c9a90-f0ba-4849-a45c-0a582f23ac2d",
   "metadata": {},
   "source": [
    "### 2.2. Readability Scores Quality Annotator\n",
    "##### This transform calculates the McAlpine-EFLAW readability score for each document in the output parquet file from the previous step and adds McAlpine-EFLAW readability score column to the data.\n",
    "\n",
    "##### McAlpine-EFLAW readability score of a document is a numerical score computed as a function of the number of words in a document plus the number of mini-words (consisting of â‰¤ 3 characters) divided by the number of sentences. Lower score means the document is easier to understand for a reader with English as a foreign language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869513e5-2ee3-4a41-bb3a-d48e2e970d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210915e6-9813-43d1-a61c-da50d7a3fdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e09597a-d8d9-4e88-b1ef-4502f54447de",
   "metadata": {},
   "source": [
    "### 2.3. Extreme-Tokenized Annotator\n",
    "##### This annotator retrieves the tokens generated for a set of documents. Then, it calculates, for each document, the size and the total number of characters. The number of tokens is divided by the size and by the number of characters, and the resulting values are stored in two columns ( tokens_per_doc_size and tokens_per_doc_num_chars).\n",
    "\n",
    "##### The annotator transform annotates the input table with 5 columns:\n",
    "\n",
    "###### 1. doc_num_tokens - number of tokens for each document\n",
    "###### 2. doc_size_kbs - document size in kb\n",
    "###### 3. doc_num_chars - number of characters in the document\n",
    "###### 4. tokens_per_doc_size - ratio between number of tokens and document size\n",
    "###### 5. tokens_per_doc_num_chars - ratio between number of tokens and number of characters in document\n",
    "##### Documents with extremely high or low number of tokens per character (or tokens per byte) are identified as extreme-tokenized documents and can be excluded in the filtering step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008247a0-568e-46f0-911e-93f8587e3156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8d302-86b6-494f-85e9-1d9056bb2473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b3ad219-944d-4592-81b1-84a1d26ae99c",
   "metadata": {},
   "source": [
    "### 2.4. Document Category Classifiers\n",
    "##### This step annotates the documents using four FastText category classifiers:\n",
    "######   1. Science\n",
    "######   2. Education\n",
    "######   3. Technology & computing\n",
    "######   4. Medical health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737ffd4-48f2-4106-80a9-72ef1efb21e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91107fa-772d-4939-800b-c9e0a0fa7e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eadce7d7-1e39-4c1b-8f0b-39b115d8223a",
   "metadata": {},
   "source": [
    "### 3. Ensemble Quality Filter\n",
    "##### This filtering step filters out low-quality documents from the input data using multiple quality annotators and by leveraging the category information of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2030da-40a7-4eca-8841-3b5e59165694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
