{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40647db0-0509-4550-8b7b-bee114ca270e",
   "metadata": {},
   "source": [
    "# GneissWeb Recipe\n",
    "\n",
    "#### This notebook presents the GneissWeb recipe and applies the components in sequence to reproduce the GneissWeb processing pipeline using DPK transforms. \n",
    "#### ![](recipe3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd3d5a9-2c03-4945-ba17-30d8170bd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"data-prep-toolkit-transforms[rep_removal]==1.0.1.dev1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ee4b2-d3ca-4d4f-9a37-e0c029a241c2",
   "metadata": {},
   "source": [
    "### 0. Read the input parquet file\n",
    "##### Download a parquet file from HF using the HF download API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a74dff-faee-44ae-9f0a-d7dfd8be2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# import pandas as pd\n",
    "\n",
    "# REPO_ID = \"HuggingFaceFW/fineweb\"\n",
    "# FILENAME = \"data/CC-MAIN-2013-20/000_00000.parquet\"\n",
    "\n",
    "# hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c412a75-c29b-4632-ac7c-115b415a2944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78871738-46dc-4ce1-ae3c-8b7c281e8258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tmp/input/test1.parquet', <http.client.HTTPMessage at 0x1110dada0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.os.makedirs(\"tmp/input\", exist_ok=True)\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/universal/rep_removal/test-data/input/test1.parquet\", \"tmp/input/test1.parquet\")\n",
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/language/extreme_tokenized/test-data/input/arrow/test1.arrow\", \"tmp/input/test1.arrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed3dab-30af-41c6-a5c3-f665a7c49243",
   "metadata": {},
   "source": [
    "#### Pip installations\n",
    "\n",
    "##### These pip installs need to be adapted to use the appropriate release level. Alternatively, The venv running the jupyter lab could be pre-configured with a requirement file that includes the right release.\n",
    "\n",
    "##### Example for transform developers working from git clone:\n",
    "\n",
    "##### make venv\n",
    "\n",
    "##### source venv/bin/activate\n",
    "\n",
    "##### pip install jupyterlab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ab090-f34d-4707-8a16-f91bfd7e36c9",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Repetition Removal\n",
    "##### This component applies exact substring deduplication to remove any substring of predetermined length that repeats more than once within a single parquet file level by adapting the implementation from [deduplicate-text-datasets](https://github.com/google-research/deduplicate-text-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00652330-69bd-4652-81d1-5ffcca913742",
   "metadata": {},
   "source": [
    "\n",
    "#### Prerequisites\n",
    "\n",
    "##### To run the repetition removal transform, Rust is required to be installed on the machine. You can install rust following instructions [here](https://www.rust-lang.org/tools/install).\n",
    "\n",
    "##### Add Rust to $PATH\n",
    "\n",
    "##### If Rust is not added to your $PATH, run the below steps to add the rust installation location for proper execution.\n",
    "\n",
    "##### You can use the !whereis cargo command to find where rust is installed in your machine, and set the path there up to the /bin\n",
    "\n",
    "##### ex: whereis cargo produces: cargo: /Users/USERNAME/.cargo/bin/cargo\n",
    "\n",
    "##### set the $PATH to include /Users/USERNAME/.cargo/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8083eb6-fb50-43ea-84e7-40ac635e54cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from dpk_rep_removal.runtime import RepRemoval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab0fd19-dba5-4934-b510-c8c964b6ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:38:28 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "00:38:28 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "00:38:28 INFO - data factory data_ is using local data access: input_folder - tmp/input output_folder - tmp/files-repremoval\n",
      "INFO:data_processing.data_access.data_access_factory_basea8443818-6c54-4be7-90d0-ff8a12bfa061:data factory data_ is using local data access: input_folder - tmp/input output_folder - tmp/files-repremoval\n",
      "00:38:28 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_basea8443818-6c54-4be7-90d0-ff8a12bfa061:data factory data_ max_files -1, n_sample -1\n",
      "00:38:28 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_basea8443818-6c54-4be7-90d0-ff8a12bfa061:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:38:28 INFO - orchestrator rep_removal started at 2025-02-06 00:38:28\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:orchestrator rep_removal started at 2025-02-06 00:38:28\n",
      "00:38:28 INFO - Number of files is 1, source profile {'max_file_size': 0.04273414611816406, 'min_file_size': 0.04273414611816406, 'total_file_size': 0.04273414611816406}\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Number of files is 1, source profile {'max_file_size': 0.04273414611816406, 'min_file_size': 0.04273414611816406, 'total_file_size': 0.04273414611816406}\n",
      "INFO:root:timeout is: 60.616790582403965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:running the merge\n",
      "INFO:root:merging complete\n",
      "\u001b[1m\u001b[32m    Updating\u001b[0m crates.io index\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m libc v0.2.169\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m version_check v0.9.5\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro2 v1.0.93\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m either v1.13.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-ident v1.0.16\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m shlex v1.3.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m glob v0.3.2\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m syn v1.0.109\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m autocfg v1.4.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd-safe v2.0.6+zstd.1.4.7\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m itertools v0.9.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m heck v0.4.1\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m os_str_bytes v6.6.1\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro-error-attr v1.0.4\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro-error v1.0.4\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m hashbrown v0.12.3\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m indexmap v1.9.3\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_lex v0.2.4\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m textwrap v0.16.1\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m bitflags v1.3.2\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m once_cell v1.20.2\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m termcolor v1.4.1\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m strsim v0.10.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m jobserver v0.1.32\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m atty v0.2.14\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m filebuffer v0.4.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam v0.3.2\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m cc v1.2.12\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m quote v1.0.38\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd-sys v1.4.18+zstd.1.4.7\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_derive v3.2.25\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m clap v3.2.25\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd v0.5.4+zstd.1.4.7\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m dedup_dataset v1.0.0 (/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/dpk_rep_removal/rust)\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 11.67s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmp3thz5le3/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmp3thz5le3/cache --num-threads 1 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 19909 \n",
      "Duplicates found: 7250\n",
      "Total time taken: 2ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:38:43 INFO - Completed 1 files (100.0%) in 0.248 min\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Completed 1 files (100.0%) in 0.248 min\n",
      "00:38:43 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Done processing 1 files, waiting for flush() completion.\n",
      "00:38:43 INFO - done flushing in 0.0 sec\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:done flushing in 0.0 sec\n",
      "00:38:43 INFO - Completed execution in 0.248 min, execution result 0\n",
      "INFO:data_processing.runtime.pure_python.transform_launcher:Completed execution in 0.248 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RepRemoval(input_folder= \"tmp/input\",\n",
    "            output_folder= \"tmp/files-repremoval\",\n",
    "            rep_removal_contents_column_name='text', \n",
    "            rep_removal_num_threads='1',\n",
    "            ).transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ff67d-7ce9-4e7c-b283-bb75739ea876",
   "metadata": {},
   "source": [
    "### 2. Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1a2b7-bc06-4dba-ae7c-764444f79da4",
   "metadata": {},
   "source": [
    "### 2.1. Fasttext Quality Annotator\n",
    "##### This step annotates the documents using two FastText quality classifiers: (i) the fastText classifier from [DCLM](https://arxiv.org/pdf/2406.11794) and (ii) our own fastText classifier trained on a mix of high-quality synthetic data and data annotated by an LLM for high educational value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12962e6-44b4-45b8-9c63-0a33a4e5c0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ef14a-6970-4eb1-a6ac-b2f92259d894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e29c9a90-f0ba-4849-a45c-0a582f23ac2d",
   "metadata": {},
   "source": [
    "### 2.2. Readability Scores Quality Annotator\n",
    "##### This transform calculates the McAlpine-EFLAW readability score for each document in the output parquet file from the previous step and adds McAlpine-EFLAW readability score column to the data.\n",
    "\n",
    "##### McAlpine-EFLAW readability score of a document is a numerical score computed as a function of the number of words in a document plus the number of mini-words (consisting of â‰¤ 3 characters) divided by the number of sentences. Lower score means the document is easier to understand for a reader with English as a foreign language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869513e5-2ee3-4a41-bb3a-d48e2e970d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210915e6-9813-43d1-a61c-da50d7a3fdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e09597a-d8d9-4e88-b1ef-4502f54447de",
   "metadata": {},
   "source": [
    "### 2.3. Extreme-Tokenized Annotator\n",
    "##### This annotator retrieves the tokens generated for a set of documents. Then, it calculates, for each document, the size and the total number of characters. The number of tokens is divided by the size and by the number of characters, and the resulting values are stored in two columns ( tokens_per_doc_size and tokens_per_doc_num_chars).\n",
    "\n",
    "##### The annotator transform annotates the input table with 5 columns:\n",
    "\n",
    "###### 1. doc_num_tokens - number of tokens for each document\n",
    "###### 2. doc_size_kbs - document size in kb\n",
    "###### 3. doc_num_chars - number of characters in the document\n",
    "###### 4. tokens_per_doc_size - ratio between number of tokens and document size\n",
    "###### 5. tokens_per_doc_num_chars - ratio between number of tokens and number of characters in document\n",
    "##### Documents with extremely high or low number of tokens per character (or tokens per byte) are identified as extreme-tokenized documents and can be excluded in the filtering step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008247a0-568e-46f0-911e-93f8587e3156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8d302-86b6-494f-85e9-1d9056bb2473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b3ad219-944d-4592-81b1-84a1d26ae99c",
   "metadata": {},
   "source": [
    "### 2.4. Document Category Classifiers\n",
    "##### This step annotates the documents using four FastText category classifiers:\n",
    "######   1. Science\n",
    "######   2. Education\n",
    "######   3. Technology & computing\n",
    "######   4. Medical health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737ffd4-48f2-4106-80a9-72ef1efb21e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91107fa-772d-4939-800b-c9e0a0fa7e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eadce7d7-1e39-4c1b-8f0b-39b115d8223a",
   "metadata": {},
   "source": [
    "### 3. Ensemble Quality Filter\n",
    "##### This filtering step filters out low-quality documents from the input data using multiple quality annotators and by leveraging the category information of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2030da-40a7-4eca-8841-3b5e59165694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
