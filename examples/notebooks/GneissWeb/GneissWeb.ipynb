{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40647db0-0509-4550-8b7b-bee114ca270e",
   "metadata": {},
   "source": [
    "# GneissWeb Recipe\n",
    "\n",
    "#### This notebook presents the GneissWeb recipe and applies the components in sequence to reproduce the GneissWeb processing pipeline using DPK transforms. \n",
    "#### ![](recipe3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd3d5a9-2c03-4945-ba17-30d8170bd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"data-prep-toolkit-transforms[rep_removal, readabilty, extreme_tokenized, filter]==1.0.1.dev1\"\n",
    "!pip install langcodes huggingface-hub fasttext-wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ee4b2-d3ca-4d4f-9a37-e0c029a241c2",
   "metadata": {},
   "source": [
    "### 0. Read the input parquet file\n",
    "##### Download a parquet file from HF using the HF download API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a74dff-faee-44ae-9f0a-d7dfd8be2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# import pandas as pd\n",
    "\n",
    "# REPO_ID = \"HuggingFaceFW/fineweb\"\n",
    "# FILENAME = \"data/CC-MAIN-2013-20/000_00000.parquet\"\n",
    "\n",
    "# hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78871738-46dc-4ce1-ae3c-8b7c281e8258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input/test1.parquet', <http.client.HTTPMessage at 0x1074e89a0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "shutil.os.makedirs(\"input\", exist_ok=True)\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/universal/rep_removal/test-data/input/test1.parquet\", \"input/test1.parquet\")\n",
    "# urllib.request.urlretrieve(\"https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/language/extreme_tokenized/test-data/input/arrow/test1.arrow\", \"tmp/input/test1.arrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed3dab-30af-41c6-a5c3-f665a7c49243",
   "metadata": {},
   "source": [
    "#### Pip installations\n",
    "\n",
    "##### These pip installs need to be adapted to use the appropriate release level. Alternatively, The venv running the jupyter lab could be pre-configured with a requirement file that includes the right release.\n",
    "\n",
    "##### Example for transform developers working from git clone:\n",
    "\n",
    "##### make venv\n",
    "\n",
    "##### source venv/bin/activate\n",
    "\n",
    "##### pip install jupyterlab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ab090-f34d-4707-8a16-f91bfd7e36c9",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Repetition Removal\n",
    "##### This component applies exact substring deduplication to remove any substring of predetermined length that repeats more than once within a single parquet file level by adapting the implementation from [deduplicate-text-datasets](https://github.com/google-research/deduplicate-text-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00652330-69bd-4652-81d1-5ffcca913742",
   "metadata": {},
   "source": [
    "\n",
    "#### Prerequisites\n",
    "\n",
    "##### To run the repetition removal transform, Rust is required to be installed on the machine. You can install rust following instructions [here](https://www.rust-lang.org/tools/install).\n",
    "\n",
    "##### Add Rust to $PATH\n",
    "\n",
    "##### If Rust is not added to your $PATH, run the below steps to add the rust installation location for proper execution.\n",
    "\n",
    "##### You can use the !whereis cargo command to find where rust is installed in your machine, and set the path there up to the /bin\n",
    "\n",
    "##### ex: whereis cargo produces: cargo: /Users/USERNAME/.cargo/bin/cargo\n",
    "\n",
    "##### set the $PATH to include /Users/USERNAME/.cargo/bin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab0fd19-dba5-4934-b510-c8c964b6ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "16:54:37 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "16:54:37 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "16:54:37 INFO - data factory data_ is using local data access: input_folder - input output_folder - tmp/repRemoval\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ is using local data access: input_folder - input output_folder - tmp/repRemoval\n",
      "16:54:37 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ max_files -1, n_sample -1\n",
      "16:54:37 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "16:54:37 INFO - orchestrator rep_removal started at 2025-02-06 16:54:37\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:orchestrator rep_removal started at 2025-02-06 16:54:37\n",
      "16:54:37 INFO - Number of files is 1, source profile {'max_file_size': 0.04273414611816406, 'min_file_size': 0.04273414611816406, 'total_file_size': 0.04273414611816406}\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Number of files is 1, source profile {'max_file_size': 0.04273414611816406, 'min_file_size': 0.04273414611816406, 'total_file_size': 0.04273414611816406}\n",
      "INFO:root:timeout is: 60.616790582403965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:running the merge\n",
      "INFO:root:merging complete\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.09s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpe_ti_ok9/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpe_ti_ok9/cache --num-threads 1 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 19909 \n",
      "Duplicates found: 7250\n",
      "Total time taken: 2ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:54:40 INFO - Completed 1 files (100.0%) in 0.036 min\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Completed 1 files (100.0%) in 0.036 min\n",
      "16:54:40 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Done processing 1 files, waiting for flush() completion.\n",
      "16:54:40 INFO - done flushing in 0.0 sec\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:done flushing in 0.0 sec\n",
      "16:54:40 INFO - Completed execution in 0.036 min, execution result 0\n",
      "INFO:data_processing.runtime.pure_python.transform_launcher:Completed execution in 0.036 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.2 s, sys: 2.17 s, total: 3.36 s\n",
      "Wall time: 8.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from dpk_rep_removal.runtime import RepRemoval\n",
    "\n",
    "RepRemoval(input_folder= \"input\",\n",
    "            output_folder= \"tmp/repRemoval\",\n",
    "            rep_removal_contents_column_name='text', \n",
    "            rep_removal_num_threads='1',\n",
    "            ).transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ff67d-7ce9-4e7c-b283-bb75739ea876",
   "metadata": {},
   "source": [
    "### 2. Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1a2b7-bc06-4dba-ae7c-764444f79da4",
   "metadata": {},
   "source": [
    "### 2.1. Fasttext Quality Annotator\n",
    "##### This step annotates the documents using two FastText quality classifiers: (i) the fastText classifier from [DCLM](https://arxiv.org/pdf/2406.11794) and (ii) our own fastText classifier trained on a mix of high-quality synthetic data and data annotated by an LLM for high educational value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f27e62-a2fc-4c7a-bc6f-f6f54985c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# credential= "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df8ef14a-6970-4eb1-a6ac-b2f92259d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:14:55 INFO - parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'fasttext_gneissweb_quality_annotator.bin', 'gcls_model_url': 'ibm-research/GneissWeb.Quality_annotator', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'lang', 'gcls_output_score_column_name': 'score'}\n",
      "INFO:dpk_gneissweb_classification.transform:parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'fasttext_gneissweb_quality_annotator.bin', 'gcls_model_url': 'ibm-research/GneissWeb.Quality_annotator', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'lang', 'gcls_output_score_column_name': 'score'}\n",
      "17:14:55 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "17:14:55 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "17:14:55 INFO - data factory data_ is using local data access: input_folder - tmp/repRemoval output_folder - tmp/fasttext/quality\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ is using local data access: input_folder - tmp/repRemoval output_folder - tmp/fasttext/quality\n",
      "17:14:55 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ max_files -1, n_sample -1\n",
      "17:14:55 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "17:14:55 INFO - orchestrator gcls started at 2025-02-06 17:14:55\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:orchestrator gcls started at 2025-02-06 17:14:55\n",
      "17:14:55 INFO - Number of files is 1, source profile {'max_file_size': 0.03802490234375, 'min_file_size': 0.03802490234375, 'total_file_size': 0.03802490234375}\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Number of files is 1, source profile {'max_file_size': 0.03802490234375, 'min_file_size': 0.03802490234375, 'total_file_size': 0.03802490234375}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /ibm-research/GneissWeb.Quality_annotator/resolve/main/fasttext_gneissweb_quality_annotator.bin HTTP/1.1\" 302 0\n",
      "17:14:58 INFO - Completed 1 files (100.0%) in 0.0 min\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Completed 1 files (100.0%) in 0.0 min\n",
      "17:14:58 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Done processing 1 files, waiting for flush() completion.\n",
      "17:14:58 INFO - done flushing in 0.0 sec\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:done flushing in 0.0 sec\n",
      "17:14:58 INFO - Completed execution in 0.046 min, execution result 0\n",
      "INFO:data_processing.runtime.pure_python.transform_launcher:Completed execution in 0.046 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.18 s, sys: 985 ms, total: 2.17 s\n",
      "Wall time: 2.76 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "from dpk_gneissweb_classification.transform_python import Classification\n",
    "\n",
    "Classification(input_folder= \"tmp/repRemoval\",\n",
    "        output_folder= \"tmp/fasttext/quality\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_gneissweb_quality_annotator.bin\",\n",
    "        gcls_model_url= \"ibm-research/GneissWeb.Quality_annotator\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12962e6-44b4-45b8-9c63-0a33a4e5c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:24:43 INFO - parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train.bin', 'gcls_model_url': 'mlfoundations/fasttext-oh-eli5', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'lang', 'gcls_output_score_column_name': 'score'}\n",
      "INFO:dpk_gneissweb_classification.transform:parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train.bin', 'gcls_model_url': 'mlfoundations/fasttext-oh-eli5', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'lang', 'gcls_output_score_column_name': 'score'}\n",
      "17:24:43 INFO - pipeline id pipeline_id\n",
      "INFO:data_processing.runtime.execution_configuration:pipeline id pipeline_id\n",
      "17:24:43 INFO - code location None\n",
      "INFO:data_processing.runtime.execution_configuration:code location None\n",
      "17:24:43 INFO - data factory data_ is using local data access: input_folder - tmp/fasttext/quality output_folder - tmp/fasttext/DCLM\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ is using local data access: input_folder - tmp/fasttext/quality output_folder - tmp/fasttext/DCLM\n",
      "17:24:43 INFO - data factory data_ max_files -1, n_sample -1\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ max_files -1, n_sample -1\n",
      "17:24:43 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "INFO:data_processing.data_access.data_access_factory_base746a4404-d03c-4082-9f2e-68a9653021e0:data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "17:24:43 INFO - orchestrator gcls started at 2025-02-06 17:24:43\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:orchestrator gcls started at 2025-02-06 17:24:43\n",
      "17:24:43 INFO - Number of files is 1, source profile {'max_file_size': 0.03874492645263672, 'min_file_size': 0.03874492645263672, 'total_file_size': 0.03874492645263672}\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Number of files is 1, source profile {'max_file_size': 0.03874492645263672, 'min_file_size': 0.03874492645263672, 'total_file_size': 0.03874492645263672}\n",
      "DEBUG:urllib3.connectionpool:Resetting dropped connection: huggingface.co\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /mlfoundations/fasttext-oh-eli5/resolve/main/openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train.bin HTTP/1.1\" 302 0\n",
      "DEBUG:filelock:Attempting to acquire lock 5480019424 on /Users/hajaremami/.cache/huggingface/hub/.locks/models--mlfoundations--fasttext-oh-eli5/f8da53846636ce5ed2836f95e2fffada6aade035e2169af1d6c099bda2674915.lock\n",
      "DEBUG:filelock:Lock 5480019424 acquired on /Users/hajaremami/.cache/huggingface/hub/.locks/models--mlfoundations--fasttext-oh-eli5/f8da53846636ce5ed2836f95e2fffada6aade035e2169af1d6c099bda2674915.lock\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): cdn-lfs-us-1.hf.co:443\n",
      "DEBUG:urllib3.connectionpool:https://cdn-lfs-us-1.hf.co:443 \"GET /repos/36/e2/36e2aba1f693e8dd4600824042f5bca44e4ba09c064f37b119e614a8329506bc/f8da53846636ce5ed2836f95e2fffada6aade035e2169af1d6c099bda2674915?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train.bin%3B+filename%3D%22openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1738884284&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODg4NDI4NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzM2L2UyLzM2ZTJhYmExZjY5M2U4ZGQ0NjAwODI0MDQyZjViY2E0NGU0YmEwOWMwNjRmMzdiMTE5ZTYxNGE4MzI5NTA2YmMvZjhkYTUzODQ2NjM2Y2U1ZWQyODM2Zjk1ZTJmZmZhZGE2YWFkZTAzNWUyMTY5YWYxZDZjMDk5YmRhMjY3NDkxNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=J4C1rHzwmOBrxD9vQAh5nNbb4DSgZ-u3tn4ID7vIk~e52Z3ScN6tOLF-zVF5dhZl7VRPPOs2xz4IfZvkQ6~r877uNSrpK8NldOBKzORF2gJ4Hg3586Fj~IxTcFaoj6RNLMxG3Ur0g1HR1rEhH7-EcAkh38wh0FP9wx8td3AJnKDjeuUm8T9B6QKreotZ2dJMyHRCXXLgXPlh1vbhCahK5R9pjrhple95qk0uD8XtzwrdMscQdK3ccVtWm4vTstRIRPyYxZkXBoVtEm21Gjeay7y5f~Tqazuvp2HOciwFoEPHntZobbYjep8Cme9TZWRoqrdx-n-FHG4lqk6E9jtNAA__&Key-Pair-Id=K24J24Z295AEI9 HTTP/1.1\" 200 2387190433\n",
      "DEBUG:filelock:Attempting to release lock 5480019424 on /Users/hajaremami/.cache/huggingface/hub/.locks/models--mlfoundations--fasttext-oh-eli5/f8da53846636ce5ed2836f95e2fffada6aade035e2169af1d6c099bda2674915.lock\n",
      "DEBUG:filelock:Lock 5480019424 released on /Users/hajaremami/.cache/huggingface/hub/.locks/models--mlfoundations--fasttext-oh-eli5/f8da53846636ce5ed2836f95e2fffada6aade035e2169af1d6c099bda2674915.lock\n",
      "17:28:35 WARNING - Exception processing file /Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/fasttext/quality/test1.parquet: Traceback (most recent call last):\n",
      "  File \"/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/data_processing/runtime/transform_file_processor.py\", line 79, in process_file\n",
      "    out_files, stats = self.transform.transform_binary(file_name=f_name, byte_array=filedata)\n",
      "  File \"/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/data_processing/transform/table_transform.py\", line 59, in transform_binary\n",
      "    out_tables, stats = self.transform(table=table, file_name=file_name)\n",
      "  File \"/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/dpk_gneissweb_classification/transform.py\", line 95, in transform\n",
      "    raise Exception(f\"column to store label ({self.output_label_column_name}) already exist\")\n",
      "Exception: column to store label (lang) already exist\n",
      "\n",
      "WARNING:data_processing.runtime.transform_file_processor:Exception processing file /Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/fasttext/quality/test1.parquet: Traceback (most recent call last):\n",
      "  File \"/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/data_processing/runtime/transform_file_processor.py\", line 79, in process_file\n",
      "    out_files, stats = self.transform.transform_binary(file_name=f_name, byte_array=filedata)\n",
      "  File \"/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/data_processing/transform/table_transform.py\", line 59, in transform_binary\n",
      "    out_tables, stats = self.transform(table=table, file_name=file_name)\n",
      "  File \"/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/dpk_gneissweb_classification/transform.py\", line 95, in transform\n",
      "    raise Exception(f\"column to store label ({self.output_label_column_name}) already exist\")\n",
      "Exception: column to store label (lang) already exist\n",
      "\n",
      "17:28:35 INFO - Completed 1 files (100.0%) in 0.0 min\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Completed 1 files (100.0%) in 0.0 min\n",
      "17:28:35 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:Done processing 1 files, waiting for flush() completion.\n",
      "17:28:35 INFO - done flushing in 0.0 sec\n",
      "INFO:data_processing.runtime.pure_python.transform_orchestrator:done flushing in 0.0 sec\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/data_processing/runtime/pure_python/transform_orchestrator.py\", line 131, in orchestrate\n",
      "    stats[\"processing_time\"] = round(stats[\"processing_time\"], 3)\n",
      "KeyError: 'processing_time'\n",
      "17:28:35 ERROR - Exception during execution 'processing_time': None\n",
      "ERROR:data_processing.runtime.pure_python.transform_orchestrator:Exception during execution 'processing_time': None\n",
      "17:28:35 INFO - Completed execution in 3.862 min, execution result 1\n",
      "INFO:data_processing.runtime.pure_python.transform_launcher:Completed execution in 3.862 min, execution result 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.23 s, sys: 20.8 s, total: 29 s\n",
      "Wall time: 3min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fasttext/quality\",\n",
    "        output_folder= \"tmp/fasttext/DCLM\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train.bin\",\n",
    "        gcls_model_url= \"mlfoundations/fasttext-oh-eli5\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ad219-944d-4592-81b1-84a1d26ae99c",
   "metadata": {},
   "source": [
    "### 2.2. Document Category Classifiers\n",
    "##### This step annotates the documents using four FastText category classifiers:\n",
    "######   1. Science\n",
    "######   2. Education\n",
    "######   3. Technology & computing\n",
    "######   4. Medical health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737ffd4-48f2-4106-80a9-72ef1efb21e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fasttext/quality\",\n",
    "        output_folder= \"tmp/fasttext/medical\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_medical.bin\",\n",
    "        gcls_model_url= \"ibm-research/GneissWeb.Med_classifier\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91107fa-772d-4939-800b-c9e0a0fa7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fasttext/medical\",\n",
    "        output_folder= \"tmp/fasttext/education\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_education.bin\",\n",
    "        gcls_model_url= \"ibm-research/GneissWeb.Edu_classifier\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8d302-86b6-494f-85e9-1d9056bb2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fasttext/education\",\n",
    "        output_folder= \"tmp/fasttext/technology\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_technology_computing.bin\",\n",
    "        gcls_model_url= \"ibm-research/GneissWeb.Tech_classifier\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008247a0-568e-46f0-911e-93f8587e3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fasttext/technology\",\n",
    "        output_folder= \"tmp/fasttext/science\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_science.bin\",\n",
    "        gcls_model_url= \"ibm-research/GneissWeb.Sci_classifier\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c9a90-f0ba-4849-a45c-0a582f23ac2d",
   "metadata": {},
   "source": [
    "### 2.3. Readability Scores Quality Annotator\n",
    "##### This transform calculates the McAlpine-EFLAW readability score for each document in the output parquet file from the previous step and adds McAlpine-EFLAW readability score column to the data.\n",
    "\n",
    "##### McAlpine-EFLAW readability score of a document is a numerical score computed as a function of the number of words in a document plus the number of mini-words (consisting of ≤ 3 characters) divided by the number of sentences. Lower score means the document is easier to understand for a reader with English as a foreign language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a2e3cf-7c3d-4c5b-bb28-bde0bddb7390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpk_readability.runtime import Readability\n",
    "\n",
    "Readability(\n",
    "    input_folder=\"tmp/fasttext/science\",\n",
    "    output_folder=\"tmp/readabilty\",\n",
    "    readability_contents_column_name=\"contents\",\n",
    "    readability_curriculum=False,\n",
    ").transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09597a-d8d9-4e88-b1ef-4502f54447de",
   "metadata": {},
   "source": [
    "### 2.4. Extreme-Tokenized Annotator\n",
    "##### This annotator retrieves the tokens generated for a set of documents. Then, it calculates, for each document, the size and the total number of characters. The number of tokens is divided by the size and by the number of characters, and the resulting values are stored in two columns ( tokens_per_doc_size and tokens_per_doc_num_chars).\n",
    "\n",
    "##### The annotator transform annotates the input table with 5 columns:\n",
    "\n",
    "###### 1. doc_num_tokens - number of tokens for each document\n",
    "###### 2. doc_size_kbs - document size in kb\n",
    "###### 3. doc_num_chars - number of characters in the document\n",
    "###### 4. tokens_per_doc_size - ratio between number of tokens and document size\n",
    "###### 5. tokens_per_doc_num_chars - ratio between number of tokens and number of characters in document\n",
    "##### Documents with extremely high or low number of tokens per character (or tokens per byte) are identified as extreme-tokenized documents and can be excluded in the filtering step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e805d12-418d-4e00-beed-8b570b9f2801",
   "metadata": {},
   "source": [
    "#### 2.4.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797926be-09b3-4d39-9c8f-aba4f7bef4e9",
   "metadata": {},
   "source": [
    "#### 2.4.2 Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e23cb3-a184-409d-b6cf-5649f5e35d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpk_extreme_tokenized.runtime import ExtremeTokenized\n",
    "\n",
    "ExtremeTokenized(\n",
    "    input_folder=\"tmp/readabilty\",\n",
    "    output_folder=\"tmp/extreme_tokenized\",\n",
    "    et_contents_column_name=\"text\",\n",
    "    et_arrow_path=\"tmp/extreme_tokenized/arrow\",\n",
    ").transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadce7d7-1e39-4c1b-8f0b-39b115d8223a",
   "metadata": {},
   "source": [
    "### 5. Ensemble Quality Filter\n",
    "##### This filtering step filters out low-quality documents from the input data using multiple quality annotators and by leveraging the category information of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2030da-40a7-4eca-8841-3b5e59165694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpk_filter.transform_python import Filter\n",
    "\n",
    "Filter(input_folder= \"tmp/fasttext/extreme_tokenized\",\n",
    "        output_folder= \"output\",\n",
    "        filter_criteria_list= [\n",
    "            \n",
    "        ],\n",
    "        filter_logical_operator= \"AND\").transform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
