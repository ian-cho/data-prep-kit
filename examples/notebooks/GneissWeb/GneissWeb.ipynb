{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40647db0-0509-4550-8b7b-bee114ca270e",
   "metadata": {},
   "source": [
    "# GneissWeb Recipe\n",
    "\n",
    "#### In order to be able to reproduce GneissWeb, we provide here a notebook that presents the GneissWeb recipe and applies the components in sequence to reproduce the GneissWeb processing pipeline using DPK transforms. \n",
    "<br>\n",
    "\n",
    "#### Owner:  IBM Research\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46fe08-3f61-4ed4-95fc-afeafefd20e1",
   "metadata": {},
   "source": [
    "\n",
    "### **An Overview of the GneissWeb Recipe**\n",
    "#### The GneissWeb dataset was obtained by applying the following processing steps:\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Exact substring deduplication at line level (Sec. 1)\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Quality annotators (Sec. 2): \n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&emsp;&emsp;&nbsp; - Custom built fastText Quality Classifier (Sec. 2.1)\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&emsp;&emsp;&nbsp; - Custom built fastText Category Classifiers (Sec. 2.2)\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&emsp;&emsp;&nbsp; - Custom built Readability Score Quality Annotator (Sec. 2.3)\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&emsp;&emsp;&nbsp; - Custom built Extreme-Tokenized-Documents Quality Annotator (Sec. 2.4)\n",
    "#### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Category-aware Ensemble Quality Filter (Sec. 3)\n",
    "\n",
    "####  These were applied in the order shown in the Figure.\n",
    "#### &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; ![](GneissWeb_recipe_new.png)\n",
    "\n",
    "#### Please refer to the [GneissWeb](https://huggingface.co/datasets/ibm-granite/GneissWeb) dataset page for more details.\n",
    "<b>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed3dab-30af-41c6-a5c3-f665a7c49243",
   "metadata": {},
   "source": [
    "### pip installations\n",
    "\n",
    "##### These pip installs need to be adapted to use the appropriate release level. Alternatively, The venv running the jupyter lab could be pre-configured with a requirements.txt file that includes the right release.\n",
    "\n",
    "\n",
    "##### Example for transform developers working from data-prep-kit git clone:\n",
    "\n",
    "##### cd data-prep-kit/examples/notebooks/GneissWeb/\n",
    "\n",
    "##### python -m venv venv\n",
    "\n",
    "##### source venv/bin/activate\n",
    "\n",
    "##### pip install --no-cache \"data-prep-toolkit-transforms[rep_removal, readabilty, extreme_tokenized, filter, tokenization]==1.0.1.dev1\"\n",
    "\n",
    "##### pip install jupyterlab\n",
    "\n",
    "##### venv/bin/jupyter lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2ee4b2-d3ca-4d4f-9a37-e0c029a241c2",
   "metadata": {},
   "source": [
    "### 0. Read the input parquet file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a23bd14-cfb5-4f8a-8819-ce446eecd15b",
   "metadata": {},
   "source": [
    "### 0.1. Download a parquet file \n",
    "#### Download a parquet file from HF using the HF download API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a74dff-faee-44ae-9f0a-d7dfd8be2ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "REPO_ID = \"HuggingFaceFW/fineweb\"\n",
    "FILENAME = \"data/CC-MAIN-2013-20/000_00000.parquet\"\n",
    "\n",
    "file1 = hf_hub_download(repo_id=REPO_ID, filename=FILENAME, repo_type=\"dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a82609-58fc-41d9-b434-10954772851d",
   "metadata": {},
   "source": [
    "### 0.2. Resize the parquet file\n",
    "#### Resize the file to a smaller size for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a60cd92e-2931-472f-8ee1-491523bfdc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23:59:31 INFO - Split file parameters are : {'max_rows_per_table': -1, 'max_mbytes_per_table': 200.0, 'size_type': 'disk'}\n",
      "23:59:31 INFO - pipeline id pipeline_id\n",
      "23:59:31 INFO - code location None\n",
      "23:59:31 INFO - data factory data_ is using local data access: input_folder - /Users/hajaremami/.cache/huggingface/hub/datasets--HuggingFaceFW--fineweb/snapshots/0f039043b23fe1d4eed300b504aa4b4a68f1c7ba/data/CC-MAIN-2013-20 output_folder - input\n",
      "23:59:31 INFO - data factory data_ max_files -1, n_sample -1\n",
      "23:59:31 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "23:59:31 INFO - orchestrator resize started at 2025-02-15 23:59:31\n",
      "23:59:31 INFO - Number of files is 1, source profile {'max_file_size': 2048.0454998016357, 'min_file_size': 2048.0454998016357, 'total_file_size': 2048.0454998016357}\n",
      "00:00:08 INFO - Completed 1 files (100.0%) in 0.623 min\n",
      "00:00:08 INFO - Done processing 1 files, waiting for flush() completion.\n",
      "00:00:09 INFO - done flushing in 0.705 sec\n",
      "00:00:09 INFO - Completed execution in 0.634 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dpk_resize.runtime import Resize\n",
    "Resize(input_folder= os.path.dirname(file1),\n",
    "        output_folder= \"input\",\n",
    "        resize_max_mbytes_per_table= 200).transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ab090-f34d-4707-8a16-f91bfd7e36c9",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Exact substring deduplication\n",
    "##### This component applies exact substring deduplication to remove any substring of predetermined length that repeats more than once within a single parquet file level by adapting the implementation from [deduplicate-text-datasets](https://github.com/google-research/deduplicate-text-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00652330-69bd-4652-81d1-5ffcca913742",
   "metadata": {},
   "source": [
    "<!--  -->\n",
    "#### Prerequisites\n",
    "\n",
    "##### To run the repetition removal transform, Rust is required to be installed on the machine. You can install rust following instructions [here](https://www.rust-lang.org/tools/install).\n",
    "\n",
    "##### Add Rust to $PATH\n",
    "\n",
    "##### If Rust is not added to your $PATH, run the below steps to add the rust installation location for proper execution.\n",
    "\n",
    "##### You can use the !whereis cargo command to find where rust is installed in your machine, and set the path there up to the /bin\n",
    "\n",
    "##### ex: whereis cargo produces: cargo: /Users/USERNAME/.cargo/bin/cargo\n",
    "\n",
    "##### set the $PATH to include /Users/USERNAME/.cargo/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc63703-f3f7-4c6e-b325-86f2679a5bdb",
   "metadata": {},
   "source": [
    "#### Input Parameters\n",
    "\n",
    "The transform can be initialized with the following parameters:\n",
    "\n",
    "| Parameter                          | Default                            | Description                                       |\n",
    "|------------------------------------|------------------------------------|---------------------------------------------------|\n",
    "| `rep_removal_contents_column_name` | `contents`                         | Name of the column holding the document contents  |\n",
    "| `rep_remova_length_thresh`         | `50`                               | Length threshold for processing                   |\n",
    "| `rep_removal_frequency_threshold`  | `1`                                | Frequency threshold for processing                |\n",
    "| `rep_removal_retain_first_copy`    | `True`                             | Boolean value for whether to retain first copy    |\n",
    "| `rep_removal_num_threads`          | `psutils.cpu_count(logical=False)` | Value for number of threads to use for processing |\n",
    "\n",
    "\n",
    "#### Output Format\n",
    "\n",
    "The output format will be a new parquet file with the repeated sequence(s) removed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab0fd19-dba5-4934-b510-c8c964b6ca00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:00:10 INFO - pipeline id pipeline_id\n",
      "00:00:10 INFO - code location None\n",
      "00:00:10 INFO - data factory data_ is using local data access: input_folder - input output_folder - tmp/repRemoval\n",
      "00:00:10 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:00:10 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:00:10 INFO - orchestrator rep_removal started at 2025-02-16 00:00:10\n",
      "00:00:10 INFO - Number of files is 9, source profile {'max_file_size': 166.99776935577393, 'min_file_size': 104.17425632476807, 'total_file_size': 1437.520227432251}\n",
      "00:00:21 INFO - encoding parquet\n",
      "00:01:00 INFO - making suffix array\n",
      "00:01:00 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmptwrkr2l1/save_dir/parquet\n",
      "00:01:01 INFO - timeout is: 5332.767657992565\n",
      "00:01:01 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:01:04 INFO - Creating part: 127653705-170204940\n",
      "00:01:04 INFO - Creating part: 42551235-85202470\n",
      "00:01:04 INFO - Creating part: 0-42651235\n",
      "00:01:04 INFO - Creating part: 85102470-127753705\n",
      "00:01:13 INFO - Checking file integrity...\n",
      "00:01:13 INFO - Merging suffix trees...\n",
      "00:01:38 INFO - Merge successful.\n",
      "00:01:38 INFO - Final cleanup and verification...\n",
      "00:01:39 INFO - Deduplication process completed successfully.\n",
      "00:01:39 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Updating\u001b[0m crates.io index\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m libc v0.2.169\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m version_check v0.9.5\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro2 v1.0.93\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m shlex v1.3.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m either v1.13.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m unicode-ident v1.0.16\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m glob v0.3.2\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m syn v1.0.109\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m autocfg v1.4.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd-safe v2.0.6+zstd.1.4.7\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m itertools v0.9.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m heck v0.4.1\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m hashbrown v0.12.3\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro-error-attr v1.0.4\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m proc-macro-error v1.0.4\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m os_str_bytes v6.6.1\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m once_cell v1.20.3\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m indexmap v1.9.3\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_lex v0.2.4\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m bitflags v1.3.2\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m textwrap v0.16.1\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m strsim v0.10.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m termcolor v1.4.1\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m crossbeam v0.3.2\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m quote v1.0.38\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m jobserver v0.1.32\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m atty v0.2.14\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m filebuffer v0.4.0\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m cc v1.2.14\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd-sys v1.4.18+zstd.1.4.7\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m clap_derive v3.2.25\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m clap v3.2.25\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m zstd v0.5.4+zstd.1.4.7\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m dedup_dataset v1.0.0 (/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/venv/lib/python3.10/site-packages/dpk_rep_removal/rust)\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 11.52s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmptwrkr2l1/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmptwrkr2l1/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 17020493 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:01:51 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 1401249\n",
      "Total time taken: 818ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:02:00 INFO - Num Duplicate Rows: 21906\n",
      "00:02:02 INFO - Completed 1 files (11.11%) in 1.7 min\n",
      "00:02:03 INFO - encoding parquet\n",
      "00:02:43 INFO - making suffix array\n",
      "00:02:43 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmprwwyvbph/save_dir/parquet\n",
      "00:02:43 INFO - timeout is: 5347.551239157373\n",
      "00:02:43 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:02:47 INFO - Creating part: 85341076-128111614\n",
      "00:02:47 INFO - Creating part: 42670538-85441076\n",
      "00:02:47 INFO - Creating part: 128011614-170682154\n",
      "00:02:47 INFO - Creating part: 0-42770538\n",
      "00:02:56 INFO - Checking file integrity...\n",
      "00:02:56 INFO - Merging suffix trees...\n",
      "00:03:22 INFO - Merge successful.\n",
      "00:03:22 INFO - Final cleanup and verification...\n",
      "00:03:22 INFO - Deduplication process completed successfully.\n",
      "00:03:22 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.06s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmprwwyvbph/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmprwwyvbph/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 17068214 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:03:24 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 1297013\n",
      "Total time taken: 890ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:03:33 INFO - Num Duplicate Rows: 21418\n",
      "00:03:35 INFO - Completed 2 files (22.22%) in 3.25 min\n",
      "00:03:36 INFO - encoding parquet\n",
      "00:04:16 INFO - making suffix array\n",
      "00:04:16 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpibb04qoy/save_dir/parquet\n",
      "00:04:16 INFO - timeout is: 5346.1872366790585\n",
      "00:04:16 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:04:20 INFO - Creating part: 127978593-170638124\n",
      "00:04:20 INFO - Creating part: 0-42759531\n",
      "00:04:20 INFO - Creating part: 85319062-128078593\n",
      "00:04:20 INFO - Creating part: 42659531-85419062\n",
      "00:04:28 INFO - Checking file integrity...\n",
      "00:04:28 INFO - Merging suffix trees...\n",
      "00:04:54 INFO - Merge successful.\n",
      "00:04:54 INFO - Final cleanup and verification...\n",
      "00:04:54 INFO - Deduplication process completed successfully.\n",
      "00:04:54 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.06s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpibb04qoy/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpibb04qoy/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 17063811 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:04:55 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 1328924\n",
      "Total time taken: 858ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:05:04 INFO - Num Duplicate Rows: 20805\n",
      "00:05:06 INFO - Completed 3 files (33.33%) in 4.764 min\n",
      "00:05:07 INFO - encoding parquet\n",
      "00:05:47 INFO - making suffix array\n",
      "00:05:47 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmp0elat747/save_dir/parquet\n",
      "00:05:47 INFO - timeout is: 5359.475712515489\n",
      "00:05:47 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:05:51 INFO - Creating part: 42766769-85633538\n",
      "00:05:51 INFO - Creating part: 0-42866769\n",
      "00:05:51 INFO - Creating part: 85533538-128400307\n",
      "00:05:51 INFO - Creating part: 128300307-171067076\n",
      "00:05:59 INFO - Checking file integrity...\n",
      "00:05:59 INFO - Merging suffix trees...\n",
      "00:06:25 INFO - Merge successful.\n",
      "00:06:25 INFO - Final cleanup and verification...\n",
      "00:06:26 INFO - Deduplication process completed successfully.\n",
      "00:06:26 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.02s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmp0elat747/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmp0elat747/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 17106706 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:06:27 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 1256571\n",
      "Total time taken: 864ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:06:36 INFO - Num Duplicate Rows: 19782\n",
      "00:06:38 INFO - Completed 4 files (44.44%) in 6.298 min\n",
      "00:06:39 INFO - encoding parquet\n",
      "00:07:18 INFO - making suffix array\n",
      "00:07:18 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpikdhq87w/save_dir/parquet\n",
      "00:07:18 INFO - timeout is: 5357.358921933085\n",
      "00:07:18 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:07:22 INFO - Creating part: 85499372-128349058\n",
      "00:07:22 INFO - Creating part: 128249058-170998746\n",
      "00:07:22 INFO - Creating part: 0-42849686\n",
      "00:07:22 INFO - Creating part: 42749686-85599372\n",
      "00:07:31 INFO - Checking file integrity...\n",
      "00:07:31 INFO - Merging suffix trees...\n",
      "00:07:57 INFO - Merge successful.\n",
      "00:07:57 INFO - Final cleanup and verification...\n",
      "00:07:57 INFO - Deduplication process completed successfully.\n",
      "00:07:57 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.02s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpikdhq87w/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpikdhq87w/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 17099873 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:07:59 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 1166360\n",
      "Total time taken: 876ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:08:08 INFO - Num Duplicate Rows: 19066\n",
      "00:08:10 INFO - Completed 5 files (55.56%) in 7.824 min\n",
      "00:08:10 INFO - encoding parquet\n",
      "00:08:51 INFO - making suffix array\n",
      "00:08:51 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpj_btrb32/save_dir/parquet\n",
      "00:08:51 INFO - timeout is: 5372.160037174721\n",
      "00:08:51 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:08:54 INFO - Creating part: 42869131-85838262\n",
      "00:08:54 INFO - Creating part: 0-42969131\n",
      "00:08:54 INFO - Creating part: 85738262-128707393\n",
      "00:08:54 INFO - Creating part: 128607393-171476526\n",
      "00:09:03 INFO - Checking file integrity...\n",
      "00:09:03 INFO - Merging suffix trees...\n",
      "00:09:29 INFO - Merge successful.\n",
      "00:09:29 INFO - Final cleanup and verification...\n",
      "00:09:30 INFO - Deduplication process completed successfully.\n",
      "00:09:30 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.05s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpj_btrb32/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpj_btrb32/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 17147651 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:09:31 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 1204603\n",
      "Total time taken: 886ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:09:40 INFO - Num Duplicate Rows: 19049\n",
      "00:09:42 INFO - Completed 6 files (66.67%) in 9.363 min\n",
      "00:09:43 INFO - encoding parquet\n",
      "00:10:23 INFO - making suffix array\n",
      "00:10:23 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpptikhbme/save_dir/parquet\n",
      "00:10:23 INFO - timeout is: 5364.340644361834\n",
      "00:10:23 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:10:27 INFO - Creating part: 0-42906029\n",
      "00:10:27 INFO - Creating part: 128418087-171224116\n",
      "00:10:27 INFO - Creating part: 42806029-85712058\n",
      "00:10:27 INFO - Creating part: 85612058-128518087\n",
      "00:10:36 INFO - Checking file integrity...\n",
      "00:10:36 INFO - Merging suffix trees...\n",
      "00:11:02 INFO - Merge successful.\n",
      "00:11:02 INFO - Final cleanup and verification...\n",
      "00:11:02 INFO - Deduplication process completed successfully.\n",
      "00:11:02 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.02s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpptikhbme/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpptikhbme/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 17122410 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:11:03 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 1220217\n",
      "Total time taken: 822ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:11:12 INFO - Num Duplicate Rows: 19826\n",
      "00:11:14 INFO - Completed 7 files (77.78%) in 10.901 min\n",
      "00:11:15 INFO - encoding parquet\n",
      "00:11:55 INFO - making suffix array\n",
      "00:11:55 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpmh1aghjl/save_dir/parquet\n",
      "00:11:55 INFO - timeout is: 5368.35092936803\n",
      "00:11:55 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:11:59 INFO - Creating part: 0-42938392\n",
      "00:11:59 INFO - Creating part: 128515176-171353568\n",
      "00:11:59 INFO - Creating part: 42838392-85776784\n",
      "00:11:59 INFO - Creating part: 85676784-128615176\n",
      "00:12:08 INFO - Checking file integrity...\n",
      "00:12:08 INFO - Merging suffix trees...\n",
      "00:12:34 INFO - Merge successful.\n",
      "00:12:34 INFO - Final cleanup and verification...\n",
      "00:12:34 INFO - Deduplication process completed successfully.\n",
      "00:12:34 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.02s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpmh1aghjl/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpmh1aghjl/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 17135355 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:12:36 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 1193033\n",
      "Total time taken: 899ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:12:44 INFO - Num Duplicate Rows: 19341\n",
      "00:12:47 INFO - Completed 8 files (88.89%) in 12.437 min\n",
      "00:12:47 INFO - encoding parquet\n",
      "00:13:13 INFO - making suffix array\n",
      "00:13:13 INFO - Starting the deduplication process for file: /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpa7znwhth/save_dir/parquet\n",
      "00:13:13 INFO - timeout is: 3375.1241635687734\n",
      "00:13:13 INFO - Scheduling 4 jobs to create dataset parts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu speed: 3228 MHz, Cores: 10\n",
      "gpu_usage: 0.00%, GPU speed: 0 MHz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "00:13:17 INFO - Creating part: 80259156-107012208\n",
      "00:13:17 INFO - Creating part: 26753052-53606104\n",
      "00:13:17 INFO - Creating part: 53506104-80359156\n",
      "00:13:17 INFO - Creating part: 0-26853052\n",
      "00:13:22 INFO - Checking file integrity...\n",
      "00:13:22 INFO - Merging suffix trees...\n",
      "00:13:37 INFO - Merge successful.\n",
      "00:13:37 INFO - Final cleanup and verification...\n",
      "00:13:37 INFO - Deduplication process completed successfully.\n",
      "00:13:37 INFO - finding repeated substrings\n",
      "\u001b[1m\u001b[32m    Finished\u001b[0m dev [optimized + debuginfo] target(s) in 0.02s\n",
      "\u001b[1m\u001b[32m     Running\u001b[0m `venv/lib/python3.10/site-packages/dpk_rep_removal/rust/target/debug/dedup_dataset self-similar --data-file /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpa7znwhth/save_dir/parquet --length-threshold 50 --cache-dir /var/folders/f3/5zmfvg4j539bhmnsxzqmbc2h0000gn/T/tmpa7znwhth/cache --num-threads 10 --frequency-threshold 1 --retain-first-copy`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start load!\n",
      "0 / 10701219 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:13:38 INFO - collecting duplicates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates found: 686798\n",
      "Total time taken: 557ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:13:44 INFO - Num Duplicate Rows: 11030\n",
      "00:13:45 INFO - Completed 9 files (100.0%) in 13.414 min\n",
      "00:13:45 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "00:13:45 INFO - done flushing in 0.0 sec\n",
      "00:13:45 INFO - Completed execution in 13.588 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 36.9 s, total: 1min 16s\n",
      "Wall time: 13min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from dpk_rep_removal.runtime import RepRemoval\n",
    "\n",
    "RepRemoval(input_folder= \"input\",\n",
    "            output_folder= \"tmp/repRemoval\",\n",
    "            rep_removal_contents_column_name='text', \n",
    "            rep_removal_num_threads='10',\n",
    "            ).transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ff67d-7ce9-4e7c-b283-bb75739ea876",
   "metadata": {},
   "source": [
    "## 2. Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1a2b7-bc06-4dba-ae7c-764444f79da4",
   "metadata": {},
   "source": [
    "### 2.1. FastText Quality Annotator\n",
    "##### This transform annotates each document with two fastText quality classifiers: \n",
    "##### (i) [GneissWeb.Quality_annotator](https://huggingface.co/ibm-granite/GneissWeb.Quality_annotator) classifier trained on a mix of high-quality synthetic data and data annotated by an LLM for high educational value \n",
    "##### (ii) the fastText classifier from [DCLM](https://arxiv.org/pdf/2406.11794)\n",
    "\n",
    "These fastText models are used as part of the ensemble filter in GneissWeb to detect and remove low-quality documents. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81af0ca-3afa-46f1-ad72-ea913c842ea9",
   "metadata": {},
   "source": [
    "#### Input Parameters\n",
    "\n",
    "The transform can be initialized with the following parameters:\n",
    "\n",
    "| Parameter                             | Default                            | Description                                       |\n",
    "|------------------------------------   |------------------------------------|---------------------------------------------------|\n",
    "| `gcls_model_credential`               | unset                              | Credential you use to get model. This is huggingface token. [Guide to get huggingface token](https://huggingface.co/docs/hub/security-tokens)  |\n",
    "| `gcls_model_file_name`                | unset                              | specifies what filename of model you use to get model, like fasttext_gneissweb_quality_annotator.bin|\n",
    "| `gcls_model_url`                      | unset                              | specifies url that model locates. For fasttext, this will be repo name of the model, like ibm-granite/GneissWeb.Quality_annotator                |\n",
    "| `gcls_content_column_name`            | contents                           | Name of the column containing documents   |\n",
    "| `gcls_output_lablel_column_name`      | label                              | Name of the output column to hold predicted classes |\n",
    "| `gcls_output_score_column_name`       | score                              | Name of the output column to hold score of prediction |\n",
    "\n",
    "\n",
    "#### Output Format\n",
    "\n",
    "The output format will be a new parquet file with the label and score columns added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f27e62-a2fc-4c7a-bc6f-f6f54985c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "credential= \"HUGGINGFACE CREDENTIAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df8ef14a-6970-4eb1-a6ac-b2f92259d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:16:45 INFO - parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'fasttext_gneissweb_quality_annotator.bin', 'gcls_model_url': 'ibm-granite/GneissWeb.Quality_annotator', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'cosmo_fastText_label', 'gcls_output_score_column_name': 'cosmo_fastText_score'}\n",
      "00:16:45 INFO - pipeline id pipeline_id\n",
      "00:16:45 INFO - code location None\n",
      "00:16:45 INFO - data factory data_ is using local data access: input_folder - tmp/repRemoval output_folder - tmp/fastText/quality\n",
      "00:16:45 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:16:45 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:16:45 INFO - orchestrator gcls started at 2025-02-16 00:16:45\n",
      "00:16:45 INFO - Number of files is 9, source profile {'max_file_size': 165.65634059906006, 'min_file_size': 103.46352863311768, 'total_file_size': 1425.8930778503418}\n",
      "00:17:06 INFO - Completed 1 files (11.11%) in 0.307 min\n",
      "00:17:25 INFO - Completed 2 files (22.22%) in 0.62 min\n",
      "00:17:44 INFO - Completed 3 files (33.33%) in 0.937 min\n",
      "00:18:03 INFO - Completed 4 files (44.44%) in 1.255 min\n",
      "00:18:22 INFO - Completed 5 files (55.56%) in 1.569 min\n",
      "00:18:41 INFO - Completed 6 files (66.67%) in 1.888 min\n",
      "00:19:00 INFO - Completed 7 files (77.78%) in 2.2 min\n",
      "00:19:18 INFO - Completed 8 files (88.89%) in 2.514 min\n",
      "00:19:31 INFO - Completed 9 files (100.0%) in 2.715 min\n",
      "00:19:31 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "00:19:31 INFO - done flushing in 0.0 sec\n",
      "00:19:31 INFO - Completed execution in 2.759 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 39s, sys: 5.9 s, total: 2min 44s\n",
      "Wall time: 2min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "from dpk_gneissweb_classification.transform_python import Classification\n",
    "\n",
    "Classification(input_folder= \"tmp/repRemoval\",\n",
    "        output_folder= \"tmp/fastText/quality\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_gneissweb_quality_annotator.bin\",\n",
    "        gcls_model_url= \"ibm-granite/GneissWeb.Quality_annotator\",\n",
    "        gcls_output_label_column_name= \"cosmo_fastText_label\",\n",
    "        gcls_output_score_column_name= \"cosmo_fastText_score\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b12962e6-44b4-45b8-9c63-0a33a4e5c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:20:06 INFO - parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train.bin', 'gcls_model_url': 'mlfoundations/fasttext-oh-eli5', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'dclm_fastText_label', 'gcls_output_score_column_name': 'dclm_fastText_score'}\n",
      "00:20:06 INFO - pipeline id pipeline_id\n",
      "00:20:06 INFO - code location None\n",
      "00:20:06 INFO - data factory data_ is using local data access: input_folder - tmp/fastText/quality output_folder - tmp/fastText/DCLM\n",
      "00:20:06 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:20:06 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:20:06 INFO - orchestrator gcls started at 2025-02-16 00:20:06\n",
      "00:20:06 INFO - Number of files is 9, source profile {'max_file_size': 165.78384017944336, 'min_file_size': 103.54288864135742, 'total_file_size': 1426.9458637237549}\n",
      "00:20:27 INFO - Completed 1 files (11.11%) in 0.318 min\n",
      "00:20:46 INFO - Completed 2 files (22.22%) in 0.636 min\n",
      "00:21:05 INFO - Completed 3 files (33.33%) in 0.953 min\n",
      "00:21:25 INFO - Completed 4 files (44.44%) in 1.273 min\n",
      "00:21:44 INFO - Completed 5 files (55.56%) in 1.591 min\n",
      "00:22:03 INFO - Completed 6 files (66.67%) in 1.909 min\n",
      "00:22:22 INFO - Completed 7 files (77.78%) in 2.233 min\n",
      "00:22:42 INFO - Completed 8 files (88.89%) in 2.56 min\n",
      "00:22:54 INFO - Completed 9 files (100.0%) in 2.762 min\n",
      "00:22:54 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "00:22:54 INFO - done flushing in 0.0 sec\n",
      "00:22:54 INFO - Completed execution in 2.796 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 42s, sys: 5.3 s, total: 2min 47s\n",
      "Wall time: 2min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fastText/quality\",\n",
    "        output_folder= \"tmp/fastText/DCLM\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train.bin\",\n",
    "        gcls_model_url= \"mlfoundations/fasttext-oh-eli5\",\n",
    "        gcls_output_label_column_name= \"dclm_fastText_label\",\n",
    "        gcls_output_score_column_name= \"dclm_fastText_score\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ad219-944d-4592-81b1-84a1d26ae99c",
   "metadata": {},
   "source": [
    "### 2.2. Document Category Classifiers\n",
    "##### This step annotates each document using four fastText category classifiers:\n",
    "##### &emsp;&emsp;  - [GneissWeb.Med_classifier](https://huggingface.co/ibm-granite/GneissWeb.Med_classifier)\n",
    "##### &emsp;&emsp;  - [GneissWeb.Edu_classifier](https://huggingface.co/ibm-granite/GneissWeb.Edu_classifier)\n",
    "##### &emsp;&emsp;  - [GneissWeb.Tech_classifier](https://huggingface.co/ibm-granite/GneissWeb.Tech_classifier)\n",
    "##### &emsp;&emsp;  - [GneissWeb.Sci_classifier](https://huggingface.co/ibm-granite/GneissWeb.Sci_classifier)\n",
    "\n",
    "These fastText models are used as part of the ensemble filter in GneissWeb to leverage the category annotations in category-aware readability score quality filtering and extreme-tokenized quality filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4737ffd4-48f2-4106-80a9-72ef1efb21e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:24:16 INFO - parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'fasttext_medical.bin', 'gcls_model_url': 'ibm-granite/GneissWeb.Med_classifier', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'medical_label', 'gcls_output_score_column_name': 'medical_score'}\n",
      "00:24:16 INFO - pipeline id pipeline_id\n",
      "00:24:16 INFO - code location None\n",
      "00:24:16 INFO - data factory data_ is using local data access: input_folder - tmp/fastText/DCLM output_folder - tmp/fastText/medical\n",
      "00:24:16 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:24:16 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:24:16 INFO - orchestrator gcls started at 2025-02-16 00:24:16\n",
      "00:24:16 INFO - Number of files is 9, source profile {'max_file_size': 166.23744297027588, 'min_file_size': 103.83616161346436, 'total_file_size': 1430.8555393218994}\n",
      "00:24:39 INFO - Completed 1 files (11.11%) in 0.319 min\n",
      "00:24:58 INFO - Completed 2 files (22.22%) in 0.634 min\n",
      "00:25:16 INFO - Completed 3 files (33.33%) in 0.946 min\n",
      "00:25:35 INFO - Completed 4 files (44.44%) in 1.258 min\n",
      "00:25:53 INFO - Completed 5 files (55.56%) in 1.565 min\n",
      "00:26:12 INFO - Completed 6 files (66.67%) in 1.878 min\n",
      "00:26:32 INFO - Completed 7 files (77.78%) in 2.207 min\n",
      "00:26:51 INFO - Completed 8 files (88.89%) in 2.526 min\n",
      "00:27:03 INFO - Completed 9 files (100.0%) in 2.724 min\n",
      "00:27:03 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "00:27:03 INFO - done flushing in 0.0 sec\n",
      "00:27:03 INFO - Completed execution in 2.785 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 7.08 s, total: 2min 45s\n",
      "Wall time: 2min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fastText/DCLM\",\n",
    "        output_folder= \"tmp/fastText/medical\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_medical.bin\",\n",
    "        gcls_model_url= \"ibm-granite/GneissWeb.Med_classifier\",\n",
    "        gcls_output_label_column_name= \"medical_label\",\n",
    "        gcls_output_score_column_name= \"medical_score\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d91107fa-772d-4939-800b-c9e0a0fa7e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:27:03 INFO - parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'fasttext_education.bin', 'gcls_model_url': 'ibm-granite/GneissWeb.Edu_classifier', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'education_label', 'gcls_output_score_column_name': 'education_score'}\n",
      "00:27:03 INFO - pipeline id pipeline_id\n",
      "00:27:03 INFO - code location None\n",
      "00:27:03 INFO - data factory data_ is using local data access: input_folder - tmp/fastText/medical output_folder - tmp/fastText/education\n",
      "00:27:03 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:27:03 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:27:03 INFO - orchestrator gcls started at 2025-02-16 00:27:03\n",
      "00:27:03 INFO - Number of files is 9, source profile {'max_file_size': 166.36492156982422, 'min_file_size': 103.91450691223145, 'total_file_size': 1431.9192390441895}\n",
      "00:27:25 INFO - Completed 1 files (11.11%) in 0.313 min\n",
      "00:27:45 INFO - Completed 2 files (22.22%) in 0.643 min\n",
      "00:28:04 INFO - Completed 3 files (33.33%) in 0.964 min\n",
      "00:28:24 INFO - Completed 4 files (44.44%) in 1.289 min\n",
      "00:28:44 INFO - Completed 5 files (55.56%) in 1.627 min\n",
      "00:29:03 INFO - Completed 6 files (66.67%) in 1.944 min\n",
      "00:29:21 INFO - Completed 7 files (77.78%) in 2.247 min\n",
      "00:29:40 INFO - Completed 8 files (88.89%) in 2.558 min\n",
      "00:29:51 INFO - Completed 9 files (100.0%) in 2.753 min\n",
      "00:29:51 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "00:29:51 INFO - done flushing in 0.0 sec\n",
      "00:29:51 INFO - Completed execution in 2.806 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 7.37 s, total: 2min 46s\n",
      "Wall time: 2min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fastText/medical\",\n",
    "        output_folder= \"tmp/fastText/education\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_education.bin\",\n",
    "        gcls_model_url= \"ibm-granite/GneissWeb.Edu_classifier\",\n",
    "        gcls_output_label_column_name= \"education_label\",\n",
    "        gcls_output_score_column_name= \"education_score\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95f8d302-86b6-494f-85e9-1d9056bb2473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:29:51 INFO - parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'fasttext_technology_computing.bin', 'gcls_model_url': 'ibm-granite/GneissWeb.Tech_classifier', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'technology_computing_label', 'gcls_output_score_column_name': 'technology_computing_score'}\n",
      "00:29:51 INFO - pipeline id pipeline_id\n",
      "00:29:51 INFO - code location None\n",
      "00:29:51 INFO - data factory data_ is using local data access: input_folder - tmp/fastText/education output_folder - tmp/fastText/technology\n",
      "00:29:51 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:29:51 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:29:51 INFO - orchestrator gcls started at 2025-02-16 00:29:51\n",
      "00:29:51 INFO - Number of files is 9, source profile {'max_file_size': 166.49221515655518, 'min_file_size': 103.99302196502686, 'total_file_size': 1432.976061820984}\n",
      "00:30:13 INFO - Completed 1 files (11.11%) in 0.305 min\n",
      "00:30:31 INFO - Completed 2 files (22.22%) in 0.61 min\n",
      "00:30:49 INFO - Completed 3 files (33.33%) in 0.912 min\n",
      "00:31:08 INFO - Completed 4 files (44.44%) in 1.225 min\n",
      "00:31:27 INFO - Completed 5 files (55.56%) in 1.531 min\n",
      "00:31:45 INFO - Completed 6 files (66.67%) in 1.83 min\n",
      "00:32:02 INFO - Completed 7 files (77.78%) in 2.13 min\n",
      "00:32:21 INFO - Completed 8 files (88.89%) in 2.434 min\n",
      "00:32:32 INFO - Completed 9 files (100.0%) in 2.622 min\n",
      "00:32:32 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "00:32:32 INFO - done flushing in 0.0 sec\n",
      "00:32:32 INFO - Completed execution in 2.679 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 35s, sys: 5.11 s, total: 2min 40s\n",
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fastText/education\",\n",
    "        output_folder= \"tmp/fastText/technology\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_technology_computing.bin\",\n",
    "        gcls_model_url= \"ibm-granite/GneissWeb.Tech_classifier\",\n",
    "        gcls_output_label_column_name= \"technology_computing_label\",\n",
    "        gcls_output_score_column_name= \"technology_computing_score\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "008247a0-568e-46f0-911e-93f8587e3156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:32:32 INFO - parameters are : {'gcls_model_credential': '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', 'gcls_model_file_name': 'fasttext_science.bin', 'gcls_model_url': 'ibm-granite/GneissWeb.Sci_classifier', 'gcls_content_column_name': 'text', 'gcls_output_label_column_name': 'science_label', 'gcls_output_score_column_name': 'science_score'}\n",
      "00:32:32 INFO - pipeline id pipeline_id\n",
      "00:32:32 INFO - code location None\n",
      "00:32:32 INFO - data factory data_ is using local data access: input_folder - tmp/fastText/technology output_folder - tmp/fastText/science\n",
      "00:32:32 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:32:32 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:32:32 INFO - orchestrator gcls started at 2025-02-16 00:32:32\n",
      "00:32:32 INFO - Number of files is 9, source profile {'max_file_size': 166.6246156692505, 'min_file_size': 104.07626056671143, 'total_file_size': 1434.1196670532227}\n",
      "00:32:56 INFO - Completed 1 files (11.11%) in 0.311 min\n",
      "00:33:14 INFO - Completed 2 files (22.22%) in 0.623 min\n",
      "00:33:33 INFO - Completed 3 files (33.33%) in 0.937 min\n",
      "00:33:51 INFO - Completed 4 files (44.44%) in 1.233 min\n",
      "00:34:09 INFO - Completed 5 files (55.56%) in 1.524 min\n",
      "00:34:26 INFO - Completed 6 files (66.67%) in 1.822 min\n",
      "00:34:45 INFO - Completed 7 files (77.78%) in 2.128 min\n",
      "00:35:03 INFO - Completed 8 files (88.89%) in 2.434 min\n",
      "00:35:15 INFO - Completed 9 files (100.0%) in 2.626 min\n",
      "00:35:15 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "00:35:15 INFO - done flushing in 0.0 sec\n",
      "00:35:15 INFO - Completed execution in 2.711 min, execution result 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 35s, sys: 5.55 s, total: 2min 41s\n",
      "Wall time: 2min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "Classification(input_folder= \"tmp/fastText/technology\",\n",
    "        output_folder= \"tmp/fastText/science\",\n",
    "        gcls_model_credential= credential,\n",
    "        gcls_model_file_name= \"fasttext_science.bin\",\n",
    "        gcls_model_url= \"ibm-granite/GneissWeb.Sci_classifier\",\n",
    "        gcls_output_label_column_name= \"science_label\",\n",
    "        gcls_output_score_column_name= \"science_score\",\n",
    "        gcls_content_column_name= \"text\").transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29c9a90-f0ba-4849-a45c-0a582f23ac2d",
   "metadata": {},
   "source": [
    "### 2.3. Readability Scores Quality Annotator\n",
    "\n",
    "Readability scores are formulas based on text statistics (such as sentence length, average number of words, number of syllables etc.) designed to assess how easily the text can be read and understood.\n",
    "\n",
    "This transform calculates the McAlpine-EFLAW readability score for each document in the output parquet file from the previous step and adds [McAlpine-EFLAW](https://www.angelfire.com/nd/nirmaldasan/journalismonline/fpetge.html) readability score column to the data.\n",
    "\n",
    "McAlpine-EFLAW readability score of a document is a numerical score computed as a function of the number of words in a document plus the number of mini-words (consisting of  3 characters) divided by the number of sentences. Lower score means the document is easier to understand for a reader with English as a foreign language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782a201-8adb-4a79-acb7-98b573af130d",
   "metadata": {},
   "source": [
    "#### Input Parameters\n",
    "\n",
    "The transform can be initialized with the following parameters:\n",
    "\n",
    "| Parameter                          | Default                            | Description                                       |\n",
    "|------------------------------------|------------------------------------|---------------------------------------------------|\n",
    "| `readability_contents_column_name` | `text`                             | specifies the name of the column holding the document text.  |\n",
    "| `readability_score_list`           | `mcalpine_eflaw_textstat`          | list of readability scores to be computed by the transform   |\n",
    "\n",
    "\n",
    "\n",
    "#### Output Format\n",
    "\n",
    "The output format will be a new parquet file with the Readability scores added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96a2e3cf-7c3d-4c5b-bb28-bde0bddb7390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:48:44 INFO - Readability parameters are : {'readability_contents_column_name': 'text', 'readability_score_list': 'mcalpine_eflaw_textstat'}\n",
      "00:48:44 INFO - pipeline id pipeline_id\n",
      "00:48:44 INFO - code location None\n",
      "00:48:44 INFO - data factory data_ is using local data access: input_folder - tmp/fastText/science output_folder - tmp/readabilty\n",
      "00:48:44 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:48:44 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:48:44 INFO - orchestrator readability started at 2025-02-16 00:48:44\n",
      "00:48:44 INFO - Number of files is 9, source profile {'max_file_size': 166.7492561340332, 'min_file_size': 104.1508150100708, 'total_file_size': 1435.1349716186523}\n",
      "00:49:09 INFO - Completed 1 files (11.11%) in 0.426 min\n",
      "00:49:36 INFO - Completed 2 files (22.22%) in 0.871 min\n",
      "00:50:03 INFO - Completed 3 files (33.33%) in 1.315 min\n",
      "00:50:30 INFO - Completed 4 files (44.44%) in 1.76 min\n",
      "00:50:55 INFO - Completed 5 files (55.56%) in 2.192 min\n",
      "00:51:22 INFO - Completed 6 files (66.67%) in 2.63 min\n",
      "00:51:48 INFO - Completed 7 files (77.78%) in 3.072 min\n",
      "00:52:15 INFO - Completed 8 files (88.89%) in 3.51 min\n",
      "00:52:31 INFO - Completed 9 files (100.0%) in 3.777 min\n",
      "00:52:31 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "00:52:31 INFO - done flushing in 0.0 sec\n",
      "00:52:31 INFO - Completed execution in 3.778 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install textstat\n",
    "from dpk_readability.runtime import Readability\n",
    "\n",
    "Readability(\n",
    "    input_folder=\"tmp/fastText/science\",\n",
    "    output_folder=\"tmp/readabilty\",\n",
    "    readability_contents_column_name=\"text\",\n",
    ").transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e09597a-d8d9-4e88-b1ef-4502f54447de",
   "metadata": {},
   "source": [
    "### 2.4. Extreme-Tokenized-Documents Quality Annotator\n",
    "##### This annotator retrieves the tokens generated for a set of documents. Then, it calculates, for each document, the size and the total number of characters. The number of tokens is divided by the size and by the number of characters, and the resulting values are stored in two columns ( tokens_per_doc_size and tokens_per_doc_num_chars).\n",
    "\n",
    "##### The annotator transform annotates the input table with 5 columns:\n",
    "\n",
    "##### &emsp;&emsp;1. doc_num_tokens - number of tokens for each document\n",
    "##### &emsp;&emsp;2. doc_size_kbs - document size in kb\n",
    "##### &emsp;&emsp;3. doc_num_chars - number of characters in the document\n",
    "##### &emsp;&emsp;4. tokens_per_doc_size - ratio between number of tokens and document size\n",
    "##### &emsp;&emsp;5. tokens_per_doc_num_chars - ratio between number of tokens and number of characters in document\n",
    "##### Documents with extremely high or low number of tokens per character (or tokens per byte) are identified as extreme-tokenized documents and can be excluded in the filtering step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e805d12-418d-4e00-beed-8b570b9f2801",
   "metadata": {},
   "source": [
    "#### 2.4.1 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92501596-1c2c-4a8f-80f2-155afd10ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages needed for the tokenization\n",
    "# !pip install \"transformers>=4.38.2\" \"torch\" \"python-dotenv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "734018bf-8330-4fe3-870f-b14c87da8b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:58:18 INFO - pipeline id pipeline_id\n",
      "00:58:18 INFO - code location None\n",
      "00:58:18 INFO - data factory data_ is using local data access: input_folder - tmp/readabilty output_folder - tmp/arrows\n",
      "00:58:18 INFO - data factory data_ max_files -1, n_sample -1\n",
      "00:58:18 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "00:58:18 INFO - orchestrator Tokenization2Arrow started at 2025-02-16 00:58:18\n",
      "00:58:18 INFO - Number of files is 9, source profile {'max_file_size': 166.8946590423584, 'min_file_size': 104.24081420898438, 'total_file_size': 1436.3791055679321}\n",
      "00:58:18 INFO - Tokenizer config['tokenizer'] = 'bigcode/starcoder' loaded.\n",
      "00:58:19 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_0.parquet'\n",
      "01:02:27 INFO - Completed 1 files (11.11%) in 4.146 min\n",
      "01:02:27 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_1.parquet'\n",
      "01:06:35 INFO - Completed 2 files (22.22%) in 8.281 min\n",
      "01:06:35 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_2.parquet'\n",
      "01:11:00 INFO - Completed 3 files (33.33%) in 12.689 min\n",
      "01:11:00 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_3.parquet'\n",
      "01:15:18 INFO - Completed 4 files (44.44%) in 16.999 min\n",
      "01:15:18 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_4.parquet'\n",
      "01:19:37 INFO - Completed 5 files (55.56%) in 21.305 min\n",
      "01:19:37 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_5.parquet'\n",
      "01:23:43 INFO - Completed 6 files (66.67%) in 25.416 min\n",
      "01:23:43 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_6.parquet'\n",
      "01:27:49 INFO - Completed 7 files (77.78%) in 29.507 min\n",
      "01:27:49 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_7.parquet'\n",
      "01:31:53 INFO - Completed 8 files (88.89%) in 33.576 min\n",
      "01:31:53 INFO - Tokenization2ArrowTransform.transform_binary file_name = '/Users/hajaremami/Desktop/DPK_notebook/data-prep-kit/examples/notebooks/GneissWeb/tmp/readabilty/000_00000_8.parquet'\n",
      "01:34:27 INFO - Completed 9 files (100.0%) in 36.139 min\n",
      "01:34:27 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "01:34:27 INFO - done flushing in 0.0 sec\n",
      "01:34:27 INFO - Completed execution in 36.145 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpk_tokenization2arrow.transform_python import Tokenization2Arrow\n",
    "\n",
    "Tokenization2Arrow(\n",
    "        input_folder= \"tmp/readabilty\",\n",
    "        output_folder= \"tmp/arrows\",\n",
    "        tkn_tokenizer=  \"bigcode/starcoder\",\n",
    "        tkn_doc_id_column= \"id\",\n",
    "        tkn_doc_content_column= \"text\",\n",
    "        tkn_chunk_size= 20_000).transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797926be-09b3-4d39-9c8f-aba4f7bef4e9",
   "metadata": {},
   "source": [
    "#### 2.4.2 Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c6694-8143-4f68-986a-8f47dc2949c1",
   "metadata": {},
   "source": [
    "#### Input Parameters\n",
    "\n",
    "The transform can be initialized with the following parameters:\n",
    "\n",
    "| Parameter                          | Default                            | Description                                       |\n",
    "|------------------------------------|------------------------------------|---------------------------------------------------|\n",
    "| `et_contents_column_name`          | `text`                             | specifies the name of the column holding the document text.  |\n",
    "| `et_arrow_path`                    | `unset`                            | location of the folder containing the arrow (tokenization) files.   |\n",
    "\n",
    "\n",
    "\n",
    "#### Output Format\n",
    "\n",
    "The output format will be a new parquet file with 5 columns added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02e23cb3-a184-409d-b6cf-5649f5e35d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:58:41 INFO - data factory et_ is using local configuration without input/output path\n",
      "09:58:41 INFO - data factory et_ max_files -1, n_sample -1\n",
      "09:58:41 INFO - data factory et_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "09:58:41 INFO - pipeline id pipeline_id\n",
      "09:58:41 INFO - code location None\n",
      "09:58:41 INFO - data factory data_ is using local data access: input_folder - tmp/readabilty output_folder - tmp/extreme_tokenized\n",
      "09:58:41 INFO - data factory data_ max_files -1, n_sample -1\n",
      "09:58:41 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "09:58:41 INFO - orchestrator et started at 2025-02-16 09:58:41\n",
      "09:58:41 INFO - Number of files is 9, source profile {'max_file_size': 166.8946590423584, 'min_file_size': 104.24081420898438, 'total_file_size': 1436.3791055679321}\n",
      "09:58:59 INFO - Completed 1 files (11.11%) in 0.309 min\n",
      "09:59:18 INFO - Completed 2 files (22.22%) in 0.62 min\n",
      "09:59:37 INFO - Completed 3 files (33.33%) in 0.941 min\n",
      "09:59:56 INFO - Completed 4 files (44.44%) in 1.26 min\n",
      "10:00:15 INFO - Completed 5 files (55.56%) in 1.568 min\n",
      "10:00:33 INFO - Completed 6 files (66.67%) in 1.875 min\n",
      "10:00:53 INFO - Completed 7 files (77.78%) in 2.199 min\n",
      "10:01:12 INFO - Completed 8 files (88.89%) in 2.516 min\n",
      "10:01:24 INFO - Completed 9 files (100.0%) in 2.712 min\n",
      "10:01:24 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "10:01:24 INFO - done flushing in 0.0 sec\n",
      "10:01:24 INFO - Completed execution in 2.712 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpk_extreme_tokenized.runtime import ExtremeTokenized\n",
    "\n",
    "ExtremeTokenized(\n",
    "    input_folder=\"tmp/readabilty\",\n",
    "    output_folder=\"tmp/extreme_tokenized\",\n",
    "    et_contents_column_name=\"text\",\n",
    "    et_arrow_path=\"tmp/arrows\",\n",
    ").transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadce7d7-1e39-4c1b-8f0b-39b115d8223a",
   "metadata": {},
   "source": [
    "### 3. Category-aware Ensemble Quality Filter\n",
    "\n",
    "##### GneissWeb ensemble filtering rule: A document is retained if either the fastText combination and category-aware readability score filter agree, or the fastText combination and category-aware extreme-toeknized filter agree. Here the fastText combination is logical OR of the fastText classifiers, i.e., either of the fastText classifiers agrees. Please refer to the [GneissWeb](https://huggingface.co/datasets/ibm-granite/GneissWeb) dataset page and GneissWeb paper for more details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefffcae-2bcd-4342-817b-b8b34fd5c9be",
   "metadata": {},
   "source": [
    "\n",
    "##### This filtering step filters out low-quality documents from the input data using multiple quality annotators and by leveraging the category information of documents. \n",
    "\n",
    "#### Input Parameters\n",
    "\n",
    "The transform can be initialized with the following parameters:\n",
    "\n",
    "| Parameter                          | Default                            | Description                                       |\n",
    "|------------------------------------|------------------------------------|---------------------------------------------------|\n",
    "| `filter_criteria_list`             | `[]`                               | specifies the list of row filter criteria (in SQL WHERE clause format). Each filter criterion is a string. The default value of this parameter is [] (an empty list, meaning that all the rows in the input table will be kept).  |\n",
    "| `filter_logical_operator`          | `AND`                              | specifies the logical operator that joins filter criteria (AND or OR).   |\n",
    "| `filter_columns_to_drop`           | `[]`                              | the list with the names of the columns to drop after row filtering is complete. The default value of this parameter is [] (an empty list, meaning that all the columns in the input table will be kept).   |\n",
    "\n",
    "\n",
    "\n",
    "#### Output Format\n",
    "\n",
    "The output format will be a new parquet file with the rows that do not meet a specific set of criteria removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c2030da-40a7-4eca-8841-3b5e59165694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:02:50 INFO - pipeline id pipeline_id\n",
      "10:02:50 INFO - code location None\n",
      "10:02:50 INFO - data factory data_ is using local data access: input_folder - tmp/extreme_tokenized output_folder - output\n",
      "10:02:50 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:02:50 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:02:50 INFO - orchestrator filter started at 2025-02-16 10:02:50\n",
      "10:02:50 INFO - Number of files is 9, source profile {'max_file_size': 169.34099292755127, 'min_file_size': 105.82319831848145, 'total_file_size': 1457.4556217193604}\n",
      "10:02:52 INFO - Completed 1 files (11.11%) in 0.03 min\n",
      "10:02:53 INFO - Completed 2 files (22.22%) in 0.061 min\n",
      "10:02:55 INFO - Completed 3 files (33.33%) in 0.091 min\n",
      "10:02:57 INFO - Completed 4 files (44.44%) in 0.122 min\n",
      "10:02:59 INFO - Completed 5 files (55.56%) in 0.154 min\n",
      "10:03:01 INFO - Completed 6 files (66.67%) in 0.184 min\n",
      "10:03:03 INFO - Completed 7 files (77.78%) in 0.215 min\n",
      "10:03:04 INFO - Completed 8 files (88.89%) in 0.246 min\n",
      "10:03:06 INFO - Completed 9 files (100.0%) in 0.266 min\n",
      "10:03:06 INFO - Done processing 9 files, waiting for flush() completion.\n",
      "10:03:06 INFO - done flushing in 0.0 sec\n",
      "10:03:06 INFO - Completed execution in 0.266 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dpk_filter.transform_python import Filter\n",
    "\n",
    "Filter(input_folder= \"tmp/extreme_tokenized\",\n",
    "        output_folder= \"output\",\n",
    "        filter_criteria_list= \n",
    "        ['((\"dclm_fastText_score\" > 0.002 OR \"cosmo_fastText_score\" > 0.03)) AND (((mcalpine_eflaw_textstat < 70) AND (technology_computing_label IN (\\'technology\\') OR medical_label IN (\\'medical\\') OR education_label IN (\\'education\\') OR science_label IN (\\'science\\'))) OR ((mcalpine_eflaw_textstat < 30) AND (technology_computing_label IN (\\'cc\\') AND medical_label IN (\\'cc\\') AND education_label IN (\\'cc\\') AND science_label IN (\\'cc\\'))))',\n",
    "         '((\"dclm_fastText_score\" > 0.002 OR \"cosmo_fastText_score\" > 0.03)) AND (((tokens_per_doc_num_chars BETWEEN 0.1 AND 0.5) AND (technology_computing_label IN (\\'technology\\') OR medical_label IN (\\'medical\\') OR education_label IN (\\'education\\') OR science_label IN (\\'science\\'))) OR ((tokens_per_doc_num_chars BETWEEN 0.22 AND 0.28) AND (technology_computing_label IN (\\'cc\\') AND medical_label IN (\\'cc\\') AND education_label IN (\\'cc\\') AND science_label IN (\\'cc\\'))))'],\n",
    "        filter_logical_operator= \"OR\").transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e1b57-dc5b-47ed-9ba1-4555927025e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
