{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data using LLM\n",
    "\n",
    "Here is the overall RAG pipeline.   In this notebook, we will do steps (6), (7), (8), (9) and (10)\n",
    "- Importing data is already done in this notebook [rag_2_load_data_into_milvus.ipynb](rag_2_load_data_into_milvus.ipynb)\n",
    "- üëâ Step 6: Calculate embedding for user query\n",
    "- üëâ Step 7 & 8: Send the query to vector db to retrieve relevant documents\n",
    "- üëâ Step 9 & 10: Send the query and relevant documents (returned above step) to LLM and get answers to our query\n",
    "\n",
    "![image missing](media/rag-overview-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Load .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ config REPLICATE_API_TOKEN found\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# debug\n",
    "# print (config)\n",
    "\n",
    "MY_CONFIG.REPLICATE_API_TOKEN = config.get('REPLICATE_API_TOKEN')\n",
    "\n",
    "if  MY_CONFIG.REPLICATE_API_TOKEN:\n",
    "    print (\"‚úÖ config REPLICATE_API_TOKEN found\")\n",
    "else:\n",
    "    raise Exception (\"'‚ùå REPLICATE_API_TOKEN' is not set.  Please set it above to continue...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n",
    "\n",
    "<span style=\"color:blue;\">Note: If you encounter an error about unable to load database, try this: </span>\n",
    "\n",
    "- <span style=\"color:blue;\">In **vscode** : **restart the kernel** of previous notebook. This will release the db.lock </span>\n",
    "- <span style=\"color:blue;\">In **Jupyter**: Do `File --> Close and Shutdown Notebook` of previous notebook. This will release the db.lock</span>\n",
    "- <span style=\"color:blue;\">Re-run this cell again</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Milvus instance: ./rag_1_dpk.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"‚úÖ Connected to Milvus instance:\", MY_CONFIG.DB_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Setup Embeddings\n",
    "\n",
    "Use the same embeddings we used to index our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(MY_CONFIG.EMBEDDING_MODEL)\n",
    "\n",
    "def get_embeddings (str):\n",
    "    embeddings = model.encode(str, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings len = 384\n",
      "embeddings[:5] =  [-0.05225765  0.0459774   0.04810527 -0.00142914 -0.0265453 ]\n"
     ]
    }
   ],
   "source": [
    "# Test embeddings\n",
    "embeddings = get_embeddings('Paris 2024 Olympics')\n",
    "print ('embeddings len =', len(embeddings))\n",
    "print ('embeddings[:5] = ', embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Vector Search and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant documents using vector / sementic search\n",
    "\n",
    "def fetch_relevant_documents (query : str) :\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,\n",
    "        data = [get_embeddings(query)], # Use the `emb_text` function to convert the question to an embedding vector\n",
    "        limit=3,  # Return top 3 results\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "        output_fields=[\"text\"],  # Return the text field\n",
    "    )\n",
    "    # print (search_res)\n",
    "\n",
    "    retrieved_docs_with_distances = [\n",
    "        {'text': res[\"entity\"][\"text\"], 'distance' : res[\"distance\"]} for res in search_res[0]\n",
    "    ]\n",
    "    return retrieved_docs_with_distances\n",
    "## --- end ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'distance': 0.8260288238525391,\n",
      "        'text': '## 4.3 Optimization\\n'\n",
      "                '\\n'\n",
      "                'We use AdamW optimizer (Kingma & Ba, 2017) with Œ≤ 1 = 0.9, Œ≤ '\n",
      "                '2 = 0.95 and weight decay of 0.1 for training all our Granite '\n",
      "                'code models. For the phase-1 pretraining, the learning rate '\n",
      "                'follows a cosine schedule starting from 3 √ó 10 - 4 which '\n",
      "                'decays to 3 √ó 10 - 5 with an initial linear warmup step of 2k '\n",
      "                'iterations. For phase-2 pretraining, we start from 3 √ó 10 - 4 '\n",
      "                '(1.5 √ó 10 - 4 for 20B and 34B models) and adopt an '\n",
      "                'exponential decay schedule to anneal it to 10% of the initial '\n",
      "                'learning rate. We use a batch size of 4M-5M tokens depending '\n",
      "                'on the model size during both phases of pretraining.\\n'\n",
      "                '\\n'\n",
      "                'To accelerate training, we use FlashAttention 2 (Dao et al., '\n",
      "                '2022; Dao, 2023), the persistent layernorm kernel, Fused '\n",
      "                'RMSNorm kernel (depending on the model) and the Fused Adam '\n",
      "                \"kernel available in NVIDIA's Apex library. We use a custom \"\n",
      "                \"fork of NVIDIA's MegatronLM (Shoeybi et al., 2019; Narayanan \"\n",
      "                'et al., 2021) for distributed training of all our models. We '\n",
      "                'train with a mix of 3D parallelism: tensor parallel, pipeline '\n",
      "                'parallel and data parallel. We also use sequence parallelism '\n",
      "                '(Korthikanti et al., 2023) for reducing the activation memory '\n",
      "                'consumption of large context length during training. We use '\n",
      "                \"Megatron's distributed optimizer with mixed precision \"\n",
      "                'training (Micikevicius et al., 2018) in BF16 (Kalamkar et '\n",
      "                'al., 2019) with gradient all-reduce and gradient accumulation '\n",
      "                'in FP32 for training stability.'},\n",
      "    {   'distance': 0.7880659699440002,\n",
      "        'text': '## 4.1 Two Phase Training\\n'\n",
      "                '\\n'\n",
      "                'Granite Code models are trained on 3.5T to 4.5T tokens of '\n",
      "                'code data and natural language datasets related to code. Data '\n",
      "                'is tokenized via byte pair encoding (BPE, (Sennrich et al., '\n",
      "                '2015)), employing the same tokenizer as StarCoder (Li et al., '\n",
      "                '2023a). Following (Shen et al., 2024; Hu et al., 2024), we '\n",
      "                'utilize high-quality data with two phases of training as '\n",
      "                'follows.\\n'\n",
      "                '\\n'\n",
      "                '- ¬∑ Phase 1 (code only training) : During phase 1, both 3B '\n",
      "                'and 8B models are trained for 4 trillion tokens of code data '\n",
      "                'comprising 116 languages. The 20B parameter model is trained '\n",
      "                'on 3 trillion tokens of code. The 34B model is trained on '\n",
      "                '1.4T tokens after the depth upscaling which is done on the '\n",
      "                '1.6T checkpoint of 20B model.\\n'\n",
      "                '- ¬∑ Phase 2 (code + language training) : In phase 2, we '\n",
      "                'include additional high-quality publicly available data from '\n",
      "                'various domains, including technical, mathematics, and web '\n",
      "                \"documents, to further improve the model's performance in \"\n",
      "                'reasoning and problem solving skills, which are essential for '\n",
      "                'code generation. We train all our models for 500B tokens (80% '\n",
      "                'code and 20% language data) in phase 2 training.'},\n",
      "    {   'distance': 0.7878236770629883,\n",
      "        'text': '## 3 Model Architecture\\n'\n",
      "                '\\n'\n",
      "                'We train a series of code models of varying sizes based on '\n",
      "                'the transformer decoder architecture (Vaswani et al., 2017). '\n",
      "                'The model hyperparameters for these models are given in Table '\n",
      "                '1. For all model architectures, we use pre-normalization '\n",
      "                '(Xiong et al., 2020): normalization applied to the input of '\n",
      "                'attention and MLP blocks.\\n'\n",
      "                '\\n'\n",
      "                'Table 1: Model conÔ¨Ågurations for Granite Code models.\\n'\n",
      "                '\\n'\n",
      "                '| Model           | 3B       | 8B      | 20B       | '\n",
      "                '34B       |\\n'\n",
      "                '|-----------------|----------|---------|-----------|-----------|\\n'\n",
      "                '| Batch size      | 2048     | 1024    | 576       | '\n",
      "                '532       |\\n'\n",
      "                '| Context length  | 2048     | 4096    | 8192      | '\n",
      "                '8192      |\\n'\n",
      "                '| Hidden size     | 2560     | 4096    | 6144      | '\n",
      "                '6144      |\\n'\n",
      "                '| FFN hidden size | 10240    | 14336   | 24576     | '\n",
      "                '24576     |\\n'\n",
      "                '| Attention heads | 32       | 32      | 48        | '\n",
      "                '48        |\\n'\n",
      "                '| Key-Value heads | 32 (MHA) | 8 (GQA) | 1 (MQA)   | 1 '\n",
      "                '(MQA)   |\\n'\n",
      "                '| Layers          | 32       | 36      | 52        | '\n",
      "                '88        |\\n'\n",
      "                '| Normalization   | RMSNorm  | RMSNorm | LayerNorm | '\n",
      "                'LayerNorm |\\n'\n",
      "                '| Activation      | swiglu   | swiglu  | gelu      | '\n",
      "                'gelu      |\\n'\n",
      "                '| Vocab size      | 49152    | 49152   | 49152     | '\n",
      "                '49152     |\\n'\n",
      "                '\\n'\n",
      "                '3B : The smallest model in the Granite-code model family is '\n",
      "                'trained with RoPE embedding (Su et al., 2023) and Multi-Head '\n",
      "                'Attention (Vaswani et al., 2017). This model use the swish '\n",
      "                'activation function (Ramachandran et al., 2017) with GLU '\n",
      "                '(Shazeer, 2020) for the MLP, also commonly referred to as '\n",
      "                'swiglu. For normalization, we use RMSNorm (Zhang & Sennrich, '\n",
      "                \"2019) since it's computationally more efÔ¨Åcient than LayerNorm \"\n",
      "                '(Ba et al., 2016). The 3B model is trained with a context '\n",
      "                'length of 2048 tokens.\\n'\n",
      "                '\\n'\n",
      "                '- 8B : The 8B model has a similar architecture as the 3B '\n",
      "                'model with the exception of using Grouped-Query Attention '\n",
      "                '(GQA) (Ainslie et al., 2023). Using GQA offers a better '\n",
      "                'tradeoff between model performance and inference efÔ¨Åciency at '\n",
      "                'this scale. We train the 8B model with a context length of '\n",
      "                '4096 tokens.\\n'\n",
      "                '\\n'\n",
      "                '20B : The 20B code model is trained with learned absolute '\n",
      "                'position embeddings. We use Multi-Query Attention (Shazeer, '\n",
      "                '2019) during training for efÔ¨Åcient downstream inference. For '\n",
      "                'the MLP block, we use the GELU activation function (Hendrycks '\n",
      "                '& Gimpel, 2023). For normalizing the activations, we use '\n",
      "                'LayerNorm (Ba et al., 2016). This model is trained with a '\n",
      "                'context length of 8192 tokens.\\n'\n",
      "                '\\n'\n",
      "                '34B : To train the 34B model, we follow the approach by Kim '\n",
      "                'et al. for depth upscaling of the 20B model. SpeciÔ¨Åcally, we '\n",
      "                'Ô¨Årst duplicate the 20B code model with 52 layers and then\\n'\n",
      "                '\\n'\n",
      "                'Figure 2: An overview of depth upscaling (Kim et al., 2024) '\n",
      "                'for efÔ¨Åcient training of Granite34B-Code. We utilize the 20B '\n",
      "                'model after 1.6T tokens to start training of 34B model with '\n",
      "                'the same code pretraining data without any changes to the '\n",
      "                'training and inference framework.\\n'\n",
      "                '\\n'\n",
      "                '<!-- image -->\\n'\n",
      "                '\\n'\n",
      "                'remove Ô¨Ånal 8 layers from the original model and initial 8 '\n",
      "                'layers from its duplicate to form two models. Finally, we '\n",
      "                'concatenate both models to form Granite-34B-Code model with '\n",
      "                '88 layers (see Figure 2 for an illustration). After the depth '\n",
      "                'upscaling, we observe that the drop in performance compared '\n",
      "                'to 20B model is pretty small contrary to what is observed by '\n",
      "                'Kim et al.. This performance is recovered pretty quickly '\n",
      "                'after we continue pretraining of the upscaled 34B model. '\n",
      "                'Similar, to 20B, we use a 8192 token context during '\n",
      "                'pretraining.'}]\n"
     ]
    }
   ],
   "source": [
    "# test relevant vector search\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "question = \"What was the training data used to train Granite models?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "pprint.pprint(relevant_docs, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Initialize LLM\n",
    "\n",
    "### LLM Choices at Replicate\n",
    "\n",
    "\n",
    "| Model                               | Publisher | Params | Description                                          |\n",
    "|-------------------------------------|-----------|--------|------------------------------------------------------|\n",
    "| ibm-granite/granite-3.0-8b-instruct | IBM       | 8 B    | IBM's newest Granite Model v3.0  (default)           |\n",
    "| ibm-granite/granite-3.0-2b-instruct | IBM       | 2 B    | IBM's newest Granite Model v3.0                      |\n",
    "| meta/meta-llama-3.1-405b-instruct   | Meta      | 405 B  | Meta's flagship 405 billion parameter language model |\n",
    "| meta/meta-llama-3-8b-instruct       | Meta      | 8 B    | Meta's 8 billion parameter language model            |\n",
    "| meta/meta-llama-3-70b-instruct      | Meta      | 70 B   | Meta's 70 billion parameter language model           |\n",
    "\n",
    "References \n",
    "\n",
    "- https://www.ibm.com/granite\n",
    "- https://www.llama.com/\n",
    "- https://replicate.com/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: ibm-granite/granite-3.1-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = MY_CONFIG.REPLICATE_API_TOKEN\n",
    "\n",
    "print ('Using model:', MY_CONFIG.LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "def ask_LLM (question, relevant_docs):\n",
    "    context = \"\\n\".join(\n",
    "        [doc['text'] for doc in relevant_docs]\n",
    "    )\n",
    "    \n",
    "    max_new_tokens = 1024\n",
    "    \n",
    "    ## Truncate context, so we don't over shoot context window\n",
    "    context = context[:(MY_CONFIG.MAX_CONTEXT_WINDOW - max_new_tokens - 100)]\n",
    "    # print (\"context length:\", len(context))\n",
    "    # print ('============ context (this is the context supplied to LLM) ============')\n",
    "    # print (context)\n",
    "    # print ('============ end  context ============', flush=True)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "    # print (\"user_prompt length:\", len(user_prompt))\n",
    "\n",
    "    print ('============ here is the answer from LLM =====')\n",
    "    # The meta/meta-llama-3-8b-instruct model can stream output as it's running.\n",
    "    for event in replicate.stream(\n",
    "        MY_CONFIG.LLM_MODEL,\n",
    "        input={\n",
    "            \"top_k\": 1,\n",
    "            \"top_p\": 0.95,\n",
    "            \"prompt\": user_prompt,\n",
    "            #\"max_tokens\": MY_CONFIG.MAX_CONTEXT_WINDOW,\n",
    "            \"temperature\": 0.1,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"length_penalty\": 1,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"presence_penalty\": 0,\n",
    "            \"log_performance_metrics\": False\n",
    "        },\n",
    "    ):\n",
    "        print(str(event), end=\"\")\n",
    "    ## ---\n",
    "    print ('\\n======  end LLM answer ======\\n', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-7: Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ here is the answer from LLM =====\n",
      "Granite Code models were trained on 3.5T to 4.5T tokens of code data and natural language datasets related to code. The code data comprised 116 languages, and the natural language datasets included high-quality data from various domains such as technical, mathematics, and web documents. During phase 1, the models were trained solely on code data, while in phase 2, additional high-quality language data was included to improve the model's performance in reasoning and problem-solving skills.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 77.3 ms, sys: 17.6 ms, total: 94.9 ms\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"What was the training data used to train Granite models?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ here is the answer from LLM =====\n",
      "The attention mechanism is a function that maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. This mechanism is used in the Transformer model for tasks such as encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 28.9 ms, sys: 7.26 ms, total: 36.2 ms\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"What is attention mechanism?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ here is the answer from LLM =====\n",
      "The provided context does not contain information about the moon landing. Therefore, I cannot answer the question.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 24 ms, sys: 6.31 ms, total: 30.3 ms\n",
      "Wall time: 652 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"When was the moon landing?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpk-2-rag-pdf-r1.0.0-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
