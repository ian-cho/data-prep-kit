{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Data using LLM\n",
    "\n",
    "Here is the overall RAG pipeline.   In this notebook, we will do steps (6), (7), (8), (9) and (10)\n",
    "- Importing data is already done in this notebook [rag_2_load_data_into_milvus.ipynb](rag_2_load_data_into_milvus.ipynb)\n",
    "- üëâ Step 6: Calculate embedding for user query\n",
    "- üëâ Step 7 & 8: Send the query to vector db to retrieve relevant documents\n",
    "- üëâ Step 9 & 10: Send the query and relevant documents (returned above step) to LLM and get answers to our query\n",
    "\n",
    "![image missing](media/rag-overview-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Load .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ config REPLICATE_API_TOKEN found\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "## Load Settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "# debug\n",
    "# print (config)\n",
    "\n",
    "MY_CONFIG.REPLICATE_API_TOKEN = config.get('REPLICATE_API_TOKEN')\n",
    "\n",
    "if  MY_CONFIG.REPLICATE_API_TOKEN:\n",
    "    print (\"‚úÖ config REPLICATE_API_TOKEN found\")\n",
    "else:\n",
    "    raise Exception (\"'‚ùå REPLICATE_API_TOKEN' is not set.  Please set it above to continue...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n",
    "\n",
    "<span style=\"color:blue;\">Note: If you encounter an error about unable to load database, try this: </span>\n",
    "\n",
    "- <span style=\"color:blue;\">In **vscode** : **restart the kernel** of previous notebook. This will release the db.lock </span>\n",
    "- <span style=\"color:blue;\">In **Jupyter**: Do `File --> Close and Shutdown Notebook` of previous notebook. This will release the db.lock</span>\n",
    "- <span style=\"color:blue;\">Re-run this cell again</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to Milvus instance: ./rag_1_dpk.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"‚úÖ Connected to Milvus instance:\", MY_CONFIG.DB_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Setup Embeddings\n",
    "\n",
    "Use the same embeddings we used to index our documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(MY_CONFIG.EMBEDDING_MODEL)\n",
    "\n",
    "def get_embeddings (str):\n",
    "    embeddings = model.encode(str, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings len = 384\n",
      "embeddings[:5] =  [ 0.02468892  0.10352131  0.0275264  -0.08551715 -0.01412829]\n"
     ]
    }
   ],
   "source": [
    "# Test embeddings\n",
    "embeddings = get_embeddings('Paris 2024 Olympics')\n",
    "print ('embeddings len =', len(embeddings))\n",
    "print ('embeddings[:5] = ', embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Vector Search and RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant documents using vector / sementic search\n",
    "\n",
    "def fetch_relevant_documents (query : str) :\n",
    "    search_res = milvus_client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,\n",
    "        data = [get_embeddings(query)], # Use the `emb_text` function to convert the question to an embedding vector\n",
    "        limit=3,  # Return top 3 results\n",
    "        search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "        output_fields=[\"text\"],  # Return the text field\n",
    "    )\n",
    "    # print (search_res)\n",
    "\n",
    "    retrieved_docs_with_distances = [\n",
    "        {'text': res[\"entity\"][\"text\"], 'distance' : res[\"distance\"]} for res in search_res[0]\n",
    "    ]\n",
    "    return retrieved_docs_with_distances\n",
    "## --- end ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'distance': 0.5530709028244019,\n",
      "        'text': '## 5 Instruction Tuning\\n'\n",
      "                '\\n'\n",
      "                'Finetuning code LLMs on a variety of tasks explained via '\n",
      "                'instructions has been shown to improve model usability and '\n",
      "                'general performance. While there has been much progress in '\n",
      "                'code instruction tuning, most of them adopt synthetically '\n",
      "                'generated data from OpenAI models, which limits the model use '\n",
      "                'in many enterprise applications. Thus, following OctoCoder '\n",
      "                '(Muennighoff et al., 2023), we use only a combination of '\n",
      "                'permissively licensed data, with an aim to enhance '\n",
      "                'instruction following capabilities of our models, including '\n",
      "                'logical reasoning and problem-solving skills. SpeciÔ¨Åcally, '\n",
      "                'Granite Code Instruct models are trained on the following '\n",
      "                'types of data.\\n'\n",
      "                '\\n'\n",
      "                '- ¬∑ Code Commits Dataset : CommitPackFT (Muennighoff et al., '\n",
      "                '2023), a Ô¨Åltered version of full CommitPack dataset across 92 '\n",
      "                'programming languages 6 ;\\n'\n",
      "                '\\n'\n",
      "                'Table 2: Summary of evaluation tasks.\\n'\n",
      "                '\\n'\n",
      "                '| Task                               | Benchmark           | '\n",
      "                'Reference                 |\\n'\n",
      "                '|------------------------------------|---------------------|---------------------------|\\n'\n",
      "                '| Multilingual code generation       | HumanEvalSynthesize | '\n",
      "                'Muennighoff et al. (2023) |\\n'\n",
      "                '| Multilingual code generation       | MultiPL-E           | '\n",
      "                'Cassano et al. (2023)     |\\n'\n",
      "                '| Python code generation             | MBPP                | '\n",
      "                'Austin et al. (2021)      |\\n'\n",
      "                '| Python code generation             | MBPP+               | '\n",
      "                'Liu et al. (2023a)        |\\n'\n",
      "                '| Data science code generation       | DS1000              | '\n",
      "                'Lai et al. (2023)         |\\n'\n",
      "                '| Repository-level code generation   | RepoBench           | '\n",
      "                'Liu et al. (2023b)        |\\n'\n",
      "                '| Repository-level code generation   | CrossCodeEval       | '\n",
      "                'Ding et al. (2023)        |\\n'\n",
      "                '| Fill-in-the-middle code completion | SantaCoder-FIM      | '\n",
      "                'Allal et al. (2023)       |\\n'\n",
      "                '| Multilingual code explanation      | HumanEvalExplain    | '\n",
      "                'Muennighoff et al. (2023) |\\n'\n",
      "                '| Multilingual code Ô¨Åxing            | HumanEvalFix        | '\n",
      "                'Muennighoff et al. (2023) |\\n'\n",
      "                '| Code editing                       | CanItEdit           | '\n",
      "                'Cassano et al. (2024)     |\\n'\n",
      "                '| Code translation                   | CodeLingua          | '\n",
      "                'Pan et al. (2024)         |\\n'\n",
      "                '| Code execution                     | CruxEval            | '\n",
      "                'Gu et al. (2024)          |\\n'\n",
      "                '| Math reasoning                     | MATH                | '\n",
      "                'Hendrycks et al. (2021)   |\\n'\n",
      "                '| Math reasoning                     | GSM8K               | '\n",
      "                'Cobbe et al. (2021)       |\\n'\n",
      "                '| Math reasoning                     | SAT                 | '\n",
      "                'Azerbayev et al. (2023)   |\\n'\n",
      "                '| Math reasoning                     | OCW                 | '\n",
      "                'Lewkowycz et al. (2022)   |\\n'\n",
      "                '| Function calling                   | BFCL                | '\n",
      "                'Yan et al. (2024)         |\\n'\n",
      "                '| Model robustness                   | ReCode              | '\n",
      "                'Wang et al. (2022)        |\\n'\n",
      "                '\\n'\n",
      "                '- ¬∑ Math Datasets : MathInstruct 7 (Yue et al., 2023) and '\n",
      "                'MetaMathQA (Yu et al., 2023);\\n'\n",
      "                '- ¬∑ Code Instruction Datasets : Glaive-Code-Assistant-v3 8 , '\n",
      "                'Self-OSS-Instruct-SC2 9 , Glaive-Function-Calling-v2 10 , '\n",
      "                'NL2SQL 11 and few synthetically generated API calling '\n",
      "                'datasets (Basu et al., 2024);\\n'\n",
      "                '- ¬∑ Language Instruction Datasets : High-quality datasets '\n",
      "                'like HelpSteer (Wang et al., 2023), an open license-Ô¨Åltered '\n",
      "                'version of Platypus 12 (Lee et al., 2023) including a '\n",
      "                'collection of hardcoded prompts to ensure model generates '\n",
      "                'correct outputs given inquiries about its name or '\n",
      "                'developers.\\n'\n",
      "                '\\n'\n",
      "                'For training, we use a cosine scheduler with 250 warmup '\n",
      "                'steps, an initial learning rate 10 - 5 , and train for three '\n",
      "                'epochs. Further, we add random, uniform noise with a '\n",
      "                'magnitude of 5 ‚àö Nh , where N is the sequence length and h is '\n",
      "                'the embedding dimension, to the embedding vector, as proposed '\n",
      "                'by Jain et al.. The additional noise improved overall answer '\n",
      "                'quality of the instruction model. We use FlashAttention 2 '\n",
      "                '(Dao, 2023; Dao et al., 2022) with a Padding-Free Transformer '\n",
      "                '13 implementation to reduce GPU memory usage and redundant '\n",
      "                'FLOPs during Ô¨Ånetuning. We also use full activation '\n",
      "                'checkpointing (Korthikanti et al., 2023), which allows us to '\n",
      "                'Ô¨Ånetune our Granite-20B-Code models with 8K context length '\n",
      "                'within a single node within a few hours on 8 √ó A100 GPUs.'},\n",
      "    {   'distance': 0.477556437253952,\n",
      "        'text': '## Granite Code Models: A Family of Open Foundation Models '\n",
      "                'for Code Intelligence\\n'\n",
      "                '\\n'\n",
      "                'Mayank Mishra ‚ãÜ Matt Stallone ‚ãÜ Gaoyuan Zhang ‚ãÜ Yikang Shen '\n",
      "                'Aditya Prasad Adriana Meza Soria Michele Merler Parameswaran '\n",
      "                'Selvam Saptha Surendran Shivdeep Singh Manish Sethi Xuan-Hong '\n",
      "                'Dang Pengyuan Li Kun-Lung Wu Syed Zawad Andrew Coleman '\n",
      "                'Matthew White Mark Lewis Raju Pavuluri Yan Koyfman Boris '\n",
      "                'Lublinsky Maximilien de Bayser Ibrahim Abdelaziz Kinjal Basu '\n",
      "                'Mayank Agarwal Yi Zhou Chris Johnson Aanchal Goyal Hima Patel '\n",
      "                'Yousaf Shah Petros Zerfos Heiko Ludwig Asim Munawar Maxwell '\n",
      "                'Crouse Pavan Kapanipathi Shweta Salaria Bob Calio Sophia Wen '\n",
      "                'Seetharami Seelam Brian Belgodere Carlos Fonseca Amith '\n",
      "                'Singhee Nirmit Desai David D. Cox Ruchir Puri ‚Ä† Rameswar '\n",
      "                'Panda ‚Ä†\\n'\n",
      "                '\\n'\n",
      "                'IBM Research ‚ãÜ Equal Contribution ‚Ä† Corresponding Authors '\n",
      "                'ruchir@us.ibm.com, rpanda@ibm.com'},\n",
      "    {   'distance': 0.45931386947631836,\n",
      "        'text': '## 4.1 Two Phase Training\\n'\n",
      "                '\\n'\n",
      "                'Granite Code models are trained on 3.5T to 4.5T tokens of '\n",
      "                'code data and natural language datasets related to code. Data '\n",
      "                'is tokenized via byte pair encoding (BPE, (Sennrich et al., '\n",
      "                '2015)), employing the same tokenizer as StarCoder (Li et al., '\n",
      "                '2023a). Following (Shen et al., 2024; Hu et al., 2024), we '\n",
      "                'utilize high-quality data with two phases of training as '\n",
      "                'follows.\\n'\n",
      "                '\\n'\n",
      "                '- ¬∑ Phase 1 (code only training) : During phase 1, both 3B '\n",
      "                'and 8B models are trained for 4 trillion tokens of code data '\n",
      "                'comprising 116 languages. The 20B parameter model is trained '\n",
      "                'on 3 trillion tokens of code. The 34B model is trained on '\n",
      "                '1.4T tokens after the depth upscaling which is done on the '\n",
      "                '1.6T checkpoint of 20B model.\\n'\n",
      "                '- ¬∑ Phase 2 (code + language training) : In phase 2, we '\n",
      "                'include additional high-quality publicly available data from '\n",
      "                'various domains, including technical, mathematics, and web '\n",
      "                \"documents, to further improve the model's performance in \"\n",
      "                'reasoning and problem solving skills, which are essential for '\n",
      "                'code generation. We train all our models for 500B tokens (80% '\n",
      "                'code and 20% language data) in phase 2 training.'}]\n"
     ]
    }
   ],
   "source": [
    "# test relevant vector search\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "question = \"What was the training data used to train Granite models?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "pprint.pprint(relevant_docs, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Initialize LLM\n",
    "\n",
    "### LLM Choices at Replicate\n",
    "\n",
    "\n",
    "| Model                               | Publisher | Params | Description                                          |\n",
    "|-------------------------------------|-----------|--------|------------------------------------------------------|\n",
    "| ibm-granite/granite-3.0-8b-instruct | IBM       | 8 B    | IBM's newest Granite Model v3.0  (default)           |\n",
    "| ibm-granite/granite-3.0-2b-instruct | IBM       | 2 B    | IBM's newest Granite Model v3.0                      |\n",
    "| meta/meta-llama-3.1-405b-instruct   | Meta      | 405 B  | Meta's flagship 405 billion parameter language model |\n",
    "| meta/meta-llama-3-8b-instruct       | Meta      | 8 B    | Meta's 8 billion parameter language model            |\n",
    "| meta/meta-llama-3-70b-instruct      | Meta      | 70 B   | Meta's 70 billion parameter language model           |\n",
    "\n",
    "References \n",
    "\n",
    "- https://www.ibm.com/granite\n",
    "- https://www.llama.com/\n",
    "- https://replicate.com/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: ibm-granite/granite-3.0-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = MY_CONFIG.REPLICATE_API_TOKEN\n",
    "\n",
    "print ('Using model:', MY_CONFIG.LLM_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "def ask_LLM (question, relevant_docs):\n",
    "    context = \"\\n\".join(\n",
    "        [doc['text'] for doc in relevant_docs]\n",
    "    )\n",
    "    \n",
    "    max_new_tokens = 1024\n",
    "    \n",
    "    ## Truncate context, so we don't over shoot context window\n",
    "    context = context[:(MY_CONFIG.MAX_CONTEXT_WINDOW - max_new_tokens - 100)]\n",
    "    # print (\"context length:\", len(context))\n",
    "    # print ('============ context (this is the context supplied to LLM) ============')\n",
    "    # print (context)\n",
    "    # print ('============ end  context ============', flush=True)\n",
    "\n",
    "    system_prompt = \"\"\"\n",
    "    Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    <question>\n",
    "    {question}\n",
    "    </question>\n",
    "    \"\"\"\n",
    "    # print (\"user_prompt length:\", len(user_prompt))\n",
    "\n",
    "    print ('============ here is the answer from LLM =====')\n",
    "    # The meta/meta-llama-3-8b-instruct model can stream output as it's running.\n",
    "    for event in replicate.stream(\n",
    "        MY_CONFIG.LLM_MODEL,\n",
    "        input={\n",
    "            \"top_k\": 1,\n",
    "            \"top_p\": 0.95,\n",
    "            \"prompt\": user_prompt,\n",
    "            #\"max_tokens\": MY_CONFIG.MAX_CONTEXT_WINDOW,\n",
    "            \"temperature\": 0.1,\n",
    "            \"system_prompt\": system_prompt,\n",
    "            \"length_penalty\": 1,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"stop_sequences\": \"<|end_of_text|>,<|eot_id|>\",\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "            \"presence_penalty\": 0,\n",
    "            \"log_performance_metrics\": False\n",
    "        },\n",
    "    ):\n",
    "        print(str(event), end=\"\")\n",
    "    ## ---\n",
    "    print ('\\n======  end LLM answer ======\\n', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-7: Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ here is the answer from LLM =====\n",
      "The Granite Code Instruct models were trained on a combination of permissively licensed data, including the Code Commits Dataset (CommitPackFT) and Math Datasets (MathInstruct and MetaMathQA). Additionally, they were trained on Code Instruction Datasets such as Glaive-Code-Assistant-v3, Self-OSS-Instruct-SC2, Glaive-Function-Calling-v2, and NL2SQL.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 78.4 ms, sys: 12.3 ms, total: 90.6 ms\n",
      "Wall time: 3.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"What was the training data used to train Granite models?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ here is the answer from LLM =====\n",
      "The attention mechanism is a method that allows a model to focus on specific parts of the input when producing an output. It maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values, and the weight assigned to each value is determined by a compatibility function of the query with the corresponding key. In the context of the Transformer model, attention is used in three ways: encoder-decoder attention layers, self-attention layers in the encoder, and self-attention layers in the decoder.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 43 ms, sys: 13.7 ms, total: 56.7 ms\n",
      "Wall time: 1.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"What is attention mechanism?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ here is the answer from LLM =====\n",
      "I'm sorry, the provided context does not contain information about the moon landing.\n",
      "======  end LLM answer ======\n",
      "\n",
      "CPU times: user 29 ms, sys: 7.71 ms, total: 36.7 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"When was the moon landing?\"\n",
    "relevant_docs = fetch_relevant_documents(question)\n",
    "ask_LLM(question=question, relevant_docs=relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpk-1-rag-pdf-r1.0.0.a4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
