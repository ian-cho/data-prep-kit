{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handy Utils to do Vector Search on Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Connect to Vector Database\n",
    "\n",
    "Milvus can be embedded and easy to use.\n",
    "\n",
    "<span style=\"color:blue;\">Note: If you encounter an error about unable to load database, try this: </span>\n",
    "\n",
    "- <span style=\"color:blue;\">In **vscode** : **restart the kernel** of previous notebook. This will release the db.lock </span>\n",
    "- <span style=\"color:blue;\">In **Jupyter**: Do `File --> Close and Shutdown Notebook` of previous notebook. This will release the db.lock</span>\n",
    "- <span style=\"color:blue;\">Re-run this cell again</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance: ./rag_1_dpk.db\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(MY_CONFIG.DB_URI)\n",
    "\n",
    "print (\"✅ Connected to Milvus instance:\", MY_CONFIG.DB_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Setup Embeddings\n",
    "\n",
    "Two choices here. \n",
    "\n",
    "1. use sentence transformers directly\n",
    "2. use Milvus model wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 1 - use sentence transformers directly\n",
    "\n",
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(MY_CONFIG.EMBEDDING_MODEL)\n",
    "\n",
    "def get_embeddings (str):\n",
    "    embeddings = embedding_model.encode(str, normalize_embeddings=True)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 2 - Milvus model\n",
    "from pymilvus import model\n",
    "\n",
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "\n",
    "# embedding_fn = model.DefaultEmbeddingFunction()\n",
    "\n",
    "## initialize the SentenceTransformerEmbeddingFunction\n",
    "embedding_fn = model.dense.SentenceTransformerEmbeddingFunction(\n",
    "    model_name = MY_CONFIG.EMBEDDING_MODEL,\n",
    "    device='cpu' # this will work on all devices (KIS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence transformer : embeddings len = 384\n",
      "sentence transformer : embeddings[:5] =  [ 0.02468892  0.10352131  0.0275264  -0.08551715 -0.01412829]\n",
      "milvus model wrapper : embeddings len = 384\n",
      "milvus model wrapper  : embeddings[:5] =  [ 0.02468898  0.10352129  0.02752643 -0.08551721 -0.01412823]\n"
     ]
    }
   ],
   "source": [
    "# Test Embeddings\n",
    "text = 'Paris 2024 Olympics'\n",
    "embeddings = get_embeddings(text)\n",
    "print ('sentence transformer : embeddings len =', len(embeddings))\n",
    "print ('sentence transformer : embeddings[:5] = ', embeddings[:5])\n",
    "\n",
    "embeddings = embedding_fn([text])\n",
    "print ('milvus model wrapper : embeddings len =', len(embeddings[0]))\n",
    "print ('milvus model wrapper  : embeddings[:5] = ', embeddings[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Do A  Vector Search\n",
    "\n",
    "We will do this to verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "## helper function to perform vector search\n",
    "def  do_vector_search (query):\n",
    "    query_vectors = [get_embeddings(query)]  # Option 1 - using sentence transformers\n",
    "    # query_vectors = embedding_fn([query])  # using Milvus model \n",
    "\n",
    "    results = milvus_client.search(\n",
    "        collection_name=MY_CONFIG.COLLECTION_NAME,  # target collection\n",
    "        data=query_vectors,  # query vectors\n",
    "        limit=5,  # number of returned entities\n",
    "        output_fields=[\"filename\", \"page_number\", \"text\"],  # specifies fields to be returned\n",
    "    )\n",
    "    return results\n",
    "## ----\n",
    "\n",
    "def  print_search_results (results):\n",
    "    # pprint (results)\n",
    "    print ('num results : ', len(results[0]))\n",
    "\n",
    "    for i, r in enumerate (results[0]):\n",
    "        #pprint(r, indent=4)\n",
    "        print (f'------ result {i+1} --------')\n",
    "        print ('search score:', r['distance'])\n",
    "        print ('filename:', r['entity']['filename'])\n",
    "        if 'page_number' in r['entity']:\n",
    "            print ('page number:', r['entity']['page_number'])\n",
    "        print ('text:\\n', r['entity']['text'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results :  5\n",
      "------ result 1 --------\n",
      "search score: 0.5530709028244019\n",
      "filename: granite.pdf\n",
      "text:\n",
      " ## 5 Instruction Tuning\n",
      "\n",
      "Finetuning code LLMs on a variety of tasks explained via instructions has been shown to improve model usability and general performance. While there has been much progress in code instruction tuning, most of them adopt synthetically generated data from OpenAI models, which limits the model use in many enterprise applications. Thus, following OctoCoder (Muennighoff et al., 2023), we use only a combination of permissively licensed data, with an aim to enhance instruction following capabilities of our models, including logical reasoning and problem-solving skills. Speciﬁcally, Granite Code Instruct models are trained on the following types of data.\n",
      "\n",
      "- · Code Commits Dataset : CommitPackFT (Muennighoff et al., 2023), a ﬁltered version of full CommitPack dataset across 92 programming languages 6 ;\n",
      "\n",
      "Table 2: Summary of evaluation tasks.\n",
      "\n",
      "| Task                               | Benchmark           | Reference                 |\n",
      "|------------------------------------|---------------------|---------------------------|\n",
      "| Multilingual code generation       | HumanEvalSynthesize | Muennighoff et al. (2023) |\n",
      "| Multilingual code generation       | MultiPL-E           | Cassano et al. (2023)     |\n",
      "| Python code generation             | MBPP                | Austin et al. (2021)      |\n",
      "| Python code generation             | MBPP+               | Liu et al. (2023a)        |\n",
      "| Data science code generation       | DS1000              | Lai et al. (2023)         |\n",
      "| Repository-level code generation   | RepoBench           | Liu et al. (2023b)        |\n",
      "| Repository-level code generation   | CrossCodeEval       | Ding et al. (2023)        |\n",
      "| Fill-in-the-middle code completion | SantaCoder-FIM      | Allal et al. (2023)       |\n",
      "| Multilingual code explanation      | HumanEvalExplain    | Muennighoff et al. (2023) |\n",
      "| Multilingual code ﬁxing            | HumanEvalFix        | Muennighoff et al. (2023) |\n",
      "| Code editing                       | CanItEdit           | Cassano et al. (2024)     |\n",
      "| Code translation                   | CodeLingua          | Pan et al. (2024)         |\n",
      "| Code execution                     | CruxEval            | Gu et al. (2024)          |\n",
      "| Math reasoning                     | MATH                | Hendrycks et al. (2021)   |\n",
      "| Math reasoning                     | GSM8K               | Cobbe et al. (2021)       |\n",
      "| Math reasoning                     | SAT                 | Azerbayev et al. (2023)   |\n",
      "| Math reasoning                     | OCW                 | Lewkowycz et al. (2022)   |\n",
      "| Function calling                   | BFCL                | Yan et al. (2024)         |\n",
      "| Model robustness                   | ReCode              | Wang et al. (2022)        |\n",
      "\n",
      "- · Math Datasets : MathInstruct 7 (Yue et al., 2023) and MetaMathQA (Yu et al., 2023);\n",
      "- · Code Instruction Datasets : Glaive-Code-Assistant-v3 8 , Self-OSS-Instruct-SC2 9 , Glaive-Function-Calling-v2 10 , NL2SQL 11 and few synthetically generated API calling datasets (Basu et al., 2024);\n",
      "- · Language Instruction Datasets : High-quality datasets like HelpSteer (Wang et al., 2023), an open license-ﬁltered version of Platypus 12 (Lee et al., 2023) including a collection of hardcoded prompts to ensure model generates correct outputs given inquiries about its name or developers.\n",
      "\n",
      "For training, we use a cosine scheduler with 250 warmup steps, an initial learning rate 10 - 5 , and train for three epochs. Further, we add random, uniform noise with a magnitude of 5 √ Nh , where N is the sequence length and h is the embedding dimension, to the embedding vector, as proposed by Jain et al.. The additional noise improved overall answer quality of the instruction model. We use FlashAttention 2 (Dao, 2023; Dao et al., 2022) with a Padding-Free Transformer 13 implementation to reduce GPU memory usage and redundant FLOPs during ﬁnetuning. We also use full activation checkpointing (Korthikanti et al., 2023), which allows us to ﬁnetune our Granite-20B-Code models with 8K context length within a single node within a few hours on 8 × A100 GPUs.\n",
      "\n",
      "------ result 2 --------\n",
      "search score: 0.477556437253952\n",
      "filename: granite.pdf\n",
      "text:\n",
      " ## Granite Code Models: A Family of Open Foundation Models for Code Intelligence\n",
      "\n",
      "Mayank Mishra ⋆ Matt Stallone ⋆ Gaoyuan Zhang ⋆ Yikang Shen Aditya Prasad Adriana Meza Soria Michele Merler Parameswaran Selvam Saptha Surendran Shivdeep Singh Manish Sethi Xuan-Hong Dang Pengyuan Li Kun-Lung Wu Syed Zawad Andrew Coleman Matthew White Mark Lewis Raju Pavuluri Yan Koyfman Boris Lublinsky Maximilien de Bayser Ibrahim Abdelaziz Kinjal Basu Mayank Agarwal Yi Zhou Chris Johnson Aanchal Goyal Hima Patel Yousaf Shah Petros Zerfos Heiko Ludwig Asim Munawar Maxwell Crouse Pavan Kapanipathi Shweta Salaria Bob Calio Sophia Wen Seetharami Seelam Brian Belgodere Carlos Fonseca Amith Singhee Nirmit Desai David D. Cox Ruchir Puri † Rameswar Panda †\n",
      "\n",
      "IBM Research ⋆ Equal Contribution † Corresponding Authors ruchir@us.ibm.com, rpanda@ibm.com\n",
      "\n",
      "------ result 3 --------\n",
      "search score: 0.45931386947631836\n",
      "filename: granite.pdf\n",
      "text:\n",
      " ## 4.1 Two Phase Training\n",
      "\n",
      "Granite Code models are trained on 3.5T to 4.5T tokens of code data and natural language datasets related to code. Data is tokenized via byte pair encoding (BPE, (Sennrich et al., 2015)), employing the same tokenizer as StarCoder (Li et al., 2023a). Following (Shen et al., 2024; Hu et al., 2024), we utilize high-quality data with two phases of training as follows.\n",
      "\n",
      "- · Phase 1 (code only training) : During phase 1, both 3B and 8B models are trained for 4 trillion tokens of code data comprising 116 languages. The 20B parameter model is trained on 3 trillion tokens of code. The 34B model is trained on 1.4T tokens after the depth upscaling which is done on the 1.6T checkpoint of 20B model.\n",
      "- · Phase 2 (code + language training) : In phase 2, we include additional high-quality publicly available data from various domains, including technical, mathematics, and web documents, to further improve the model's performance in reasoning and problem solving skills, which are essential for code generation. We train all our models for 500B tokens (80% code and 20% language data) in phase 2 training.\n",
      "\n",
      "------ result 4 --------\n",
      "search score: 0.43072062730789185\n",
      "filename: granite.pdf\n",
      "text:\n",
      " ## 6.1.6 FIM: Inﬁlling Evaluations\n",
      "\n",
      "Granite Code models are trained for code completion purposes using FIM objective, as described in Sec. 4.2. We use SantaCoder-FIM benchmark (Allal et al., 2023), for inﬁlling evaluations which tests the ability of models to ﬁll in a single line of code in Python, JavaScript, and Java solutions to HumanEval. We use greedy decoding and report the mean exact match for all the models. Table 9 shows that Granite Code models signiﬁcantly outperforms StarCoder and StarCoder2 across all model sizes, demonstrating it to be\n",
      "\n",
      "Figure 3: Performance of Granite-8B-Code-Instruct, Mistral-7B-Instruct-v0.2, Gemma-7B-IT, and Llama-3-8B-Instruct on HumanEvalPack. Best viewed in color.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "excellent well-rounded models for code completion use cases. Moreover, we observe no performance improvement in scaling the model sizes from 8B to 34B, indicating that smaller models are often more suitable for FIM code completion tasks.\n",
      "\n",
      "------ result 5 --------\n",
      "search score: 0.41963690519332886\n",
      "filename: attention.pdf\n",
      "text:\n",
      " ## 5 Training\n",
      "\n",
      "This section describes the training regime for our models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What was the training data used to train Granite models?\"\n",
    "\n",
    "results = do_vector_search (query)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num results :  5\n",
      "------ result 1 --------\n",
      "search score: 0.6020913124084473\n",
      "filename: attention.pdf\n",
      "text:\n",
      " ## 3.2 Attention\n",
      "\n",
      "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
      "\n",
      "Scaled Dot-Product Attention\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
      "\n",
      "------ result 2 --------\n",
      "search score: 0.5734226703643799\n",
      "filename: attention.pdf\n",
      "text:\n",
      " ## Attention Visualizations Input-Input Layer5\n",
      "\n",
      "Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of the verb 'making', completing the phrase 'making...more difﬁcult'. Attentions here shown only for the word 'making'. Different colors represent different heads. Best viewed in color.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Input-Input Layer5\n",
      "\n",
      "Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top: Full attentions for head 5. Bottom: Isolated attentions from just the word 'its' for attention heads 5 and 6. Note that the attentions are very sharp for this word.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Input-Input Layer5\n",
      "\n",
      "Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the sentence. We give two such examples above, from two different heads from the encoder self-attention at layer 5 of 6. The heads clearly learned to perform different tasks.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "------ result 3 --------\n",
      "search score: 0.4910251796245575\n",
      "filename: attention.pdf\n",
      "text:\n",
      " ## 3.2.3 Applications of Attention in our Model\n",
      "\n",
      "The Transformer uses multi-head attention in three different ways:\n",
      "\n",
      "- · In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n",
      "- · The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
      "- · Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to -∞ ) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n",
      "\n",
      "------ result 4 --------\n",
      "search score: 0.44283992052078247\n",
      "filename: attention.pdf\n",
      "text:\n",
      " ## 2 Background\n",
      "\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\n",
      "\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34].\n",
      "\n",
      "To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "\n",
      "------ result 5 --------\n",
      "search score: 0.42441824078559875\n",
      "filename: attention.pdf\n",
      "text:\n",
      " ## 3.2.2 Multi-Head Attention\n",
      "\n",
      "Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values h times with different, learned linear projections to d k , d k and d v dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding d v -dimensional\n",
      "\n",
      "output values. These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.\n",
      "\n",
      "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "MultiHead( Q,K,V ) = Concat(head 1 ,..., head h ) W O where head i = Attention( QW Q i ,KW K i ,VW V i )\n",
      "\n",
      "Where the projections are parameter matrices W Q i ∈ R d model × d k , W K i ∈ R d model × d k , W V i ∈ R d model × d v and W O ∈ R hd v × d model .\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use d k = d v = d model /h = 64 . Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the attention mechanism?\"\n",
    "\n",
    "results = do_vector_search (query)\n",
    "print_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# milvus_client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpk-1-rag-pdf-r1.0.0.a4-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
